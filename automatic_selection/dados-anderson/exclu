%-- Google Scholar

@Article{
id = {373}, 
title = {Studying Evolving Software Ecosystems based on Ecological Models},
abstract = {Research on software evolution is very active, but evolutionary principles, models and theories that properly explain why and how software systems evolve over time are still lacking. Similarly, more empirical research is needed to understand how different software projects co-exist and co-evolve, and how contributors collaborate within their encompassing software ecosystem.

In this chapter, we explore the differences and analogies between natural ecosystems and biological evolution on the one hand, and software ecosystems and software evolution on the other hand. The aim is to learn from research in ecology to advance the understanding of evolving software ecosystems. Ultimately, we wish to use such knowledge to derive diagnostic tools aiming to predict survival of software projects within their ecosystem, to analyse and optimise the fitness of software projects in their environment, and to help software project communities in managing their projects better.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {375}, 
	title = {Automated analysis of CSS rules to support style maintenance},
	abstract = {Abstract:
	CSS is a widely used language for describing the presentation semantics of HTML elements on the web. The language has a number of characteristics, such as inheritance and cascading order, which makes maintaining CSS code a challenging task for web developers. As a result, it is common for unused rules to be accumulated over time. Despite these challenges, CSS analysis has not received much attention from the research community. We propose an automated technique to support styling code maintenance, which (1) analyzes the runtime relationship between the CSS rules and DOM elements of a given web application (2) detects unmatched and ineffective selectors, overridden declaration properties, and undefined class values. Our technique, implemented in an open source tool called Cilla, has a high precision and recall rate. The results of our case study, conducted on fifteen open source and industrial web-based systems, show an average of 60\% unused CSS selectors in deployed applications, which points to the ubiquity of the problem.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {376}, 
	title = {Web application testing: A systematic literature review},
	abstract = {Context
	
	The web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013.
	
	Objective
	
	As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends and empirical evidence in this specialized field.
	
	Methods
	
	We systematically review the body of knowledge related to functional testing of web application through a systematic literature review (SLR) study. This SLR is a follow-up and complimentary study to a recent systematic mapping (SM) study that we conducted in this area. As part of this study, we pose three sets of research questions, define selection and exclusion criteria, and synthesize the empirical evidence in this area.
	
	Results
	
	Our pool of studies includes a set of 95 papers (from the 193 retrieved papers) published in the area of web application testing between 2000 and 2013. The data extracted during our SLR study is available through a publicly-accessible online repository. Among our results are the followings: (1) the list of test tools in this area and their capabilities, (2) the types of test models and fault models proposed in this domain, (3) the way the empirical studies in this area have been designed and reported, and (4) the state of empirical evidence and industrial relevance.
	
	Conclusion
	
	We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our SLR can help researchers to obtain an overview of existing web application testing approaches, fault models, tools, metrics and empirical evidence, and subsequently identify areas in the field that require more attention from the research community.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {381}, 
	title = {A Comprehensive Client-Side Behavior Model for Diagnosing Attacks in Ajax Applications},
	abstract = {Abstract:
	Behavior models of applications are widely used for diagnosing security incidents in complex web-based systems. However, Ajax techniques that enable better web experiences also make it fairly challenging to model Ajax application behaviors in the complex browser environment. In Ajax applications, server-side states are no longer synchronous with the views to end users at the client side. Therefore, to model the behaviors of Ajax applications, it is indispensable to incorporate client-side application states into the behavior models, as being explored by prior work. Unfortunately, how to leverage behavior models to perform security diagnosis in Ajax applications has yet been thoroughly examined. Existing models extracted from Ajax application behaviors are insufficient in a security context. In this paper, we propose a new behavior model for diagnosing attacks in Ajax applications, which abstracts both client-side state transitions as well as their communications to external servers. Our model articulates different states with the browser events or user actions that trigger state transitions. With a prototype implementation, we demonstrate that the proposed model is effective in attack diagnosis for real-world Ajax applications.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {382}, 
	title = {Clustering-Aided Page Object Generation for Web Testing},
	abstract = {Abstract
	To decouple test code from web page details, web testers adopt the Page Object design pattern. Page objects are facade classes abstracting the internals of web pages (e.g., form fields) into high-level business functions that can be invoked by test cases (e.g., user authentication). However, writing such page objects requires substantial effort, which is paid off only later, during software evolution. In this paper we propose a clustering-based approach for the identification of meaningful abstractions that are automatically turned into Java page objects. Our clustering approach to page object identification has been integrated into our tool for automated page object generation, Apogen. Experimental results indicate that the clustering approach provides clusters of web pages close to those manually produced by a human (with, on average, only three differences per web application). 75 \% of the code generated by Apogen can be used as-is by web testers, breaking down the manual effort for page object creation. Moreover, a large portion (84 \%) of the page object methods created automatically to support assertion definition corresponds to useful behavioural abstractions.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {383}, 
	title = {AdMotional: Towards Personalized Online Ads},
	abstract = {ABSTRACT
	AdMotional is a research project aiming at achieving a win-win
	situation for online advertisers and web users alike by optimizing
	the campaign selection process and creating personalized ads.
	This results in increased campaign performance for advertisers,
	and in more relevant and thus less annoying ads for consumers.
	We give a general overview and present the system architecture,
	before describing the main components in greater detail. We also
	introduce the learning and optimization component and strategies,
	before concluding with a summary and brief outlook into future
	developments. },
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {385}, 
	title = {Improving oracle quality by detecting brittle assertions and unused inputs in tests},
	abstract = {Writing oracles is challenging. As a result, developers often create oracles that check too little, resulting in tests that are unable to detect failures, or check too much, resulting in tests that are brittle and difficult to maintain. In this paper we present a new technique for automatically analyzing test oracles. The technique is based on dynamic tainting and detects both brittle assertions�assertions that depend on values that are derived from uncontrolled inputs�and unused inputs�inputs provided by the test that are not checked by an assertion. We also presented OraclePolish, an implementation of the technique that can analyze tests that are written in Java and use the JUnit testing framework. Using OraclePolish, we conducted an empirical evaluation of more than 4000 real test cases. The results of the evaluation show that OraclePolish is effective; it detected 164 tests that contain brittle assertions and 1618 tests that have unused inputs. In addition, the results also demonstrate that the costs associated with using the technique are reasonable.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {386}, 
	title = {Hidden-Web Induced by Client-Side Scripting: An Empirical Study},
	abstract = {Client-side JavaScript is increasingly used for enhancing web application functionality, interactivity, and responsiveness. Through the execution of JavaScript code in browsers, the DOM tree representing a webpage at runtime, can be incrementally updated without requiring a URL change. This dynamically updated content is hidden from general search engines. In this paper, we present the first empirical study on measuring and characterizing the hidden-web induced as a result of clientside JavaScript execution. Our study reveals that this type of hidden-web content is prevalent in online web applications today: from the 500 websites we analyzed, 95% contain client-side hidden-web content; On those websites that contain client-side hidden-web content, (1) on average, 62% of the web states are hidden, (2) per hidden state, there is an average of 19 kilobytes of data that is hidden from which 0.6 kilobytes contain textual content, (3) the DIV element is the most common clickable element used (61%) to initiate this type of hidden-web state transition, and (4) on average 25 minutes is required to dynamically crawl 50 DOM states. Further, our study indicates that there is a correlation between DOM tree size and hidden-web content, but no correlation exists between the amount of JavaScript code and client-side hidden-web.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {387}, 
	title = {Detection and Localization of HTML Presentation Failures Using Computer Vision-Based Techniques},
	abstract = {Abstract:
	An attractive and visually appealing appearance is important for the success of a website. Presentation failures in a site's web pages can negatively impact end users' perception of the quality of the site and the services it delivers. Debugging such failures is challenging because testers must visually inspect large web pages and analyze complex interactions among the HTML elements of a page. In this paper we propose a novel automated approach for debugging web page user interfaces. Our approach uses computer vision techniques to detect failures and can then identify HTML elements that are likely to be responsible for the failure. We evaluated our approach on a set of real-world web applications and found that the approach was able to accurately and quickly identify faulty HTML elements.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {393}, 
	title = {Evolution of Web Systems},
	abstract = {The World Wide Web has led to a new kind of software, web systems, which are based on web technologies. Just like software in other domains, web systems have evolution challenges. This chapter discusses evolution of web systems on three dimensions: architecture, (conceptual) design, and technology. For each of these dimensions we introduce the state-of-the-art in the techniques and tools that are currently available. In order to place current evolution techniques into context, we also provide a survey of the different kinds of web systems as they have emerged, tracing the most important achievements of web systems evolution research from static web sites over dynamic web applications and web services to Ajax-based Rich Internet Applications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {394}, 
	title = {Migrating cascading style sheets to preprocessors by introducing mixins},
	abstract = {Cascading Style Sheets (CSS) is the standard language for styling web documents and is extensively used in the industry. However, CSS lacks constructs that would allow code reuse (e.g., functions). Consequently, maintaining CSS code is often a cumbersome and error-prone task. Preprocessors (e.g., Less and Sass) have been introduced to fill this gap, by extending CSS with the missing constructs. Despite the clear maintainability benefits coming from the use of preprocessors, there is currently no support for migrating legacy CSS code to preprocessors. In this paper, we propose a technique for automatically detecting duplicated style declarations in CSS code that can be migrated to preprocessor functions (i.e., mixins). Our technique can parameterize differences in the style values of duplicated declarations, and ensure that the migration will not change the presentation semantics of the web documents. The evaluation has shown that our technique is able to detect 98% of the mixins that professional developers introduced in websites and Style Sheet libraries, and can safely migrate real CSS code.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {397}, 
	title = {Software Analysis for the Web: Achievements and Prospects},
	abstract = {Abstract:
	The web has had a significant impact on our lives. A technology that was initially created for sharing documents across the network has evolved into a strong medium for developing and distributing software applications. In this paper, we first provide a concise overview of the evolution of the web itself. We then focus on some of the main industrial and research achievements in software analysis and testing techniques geared toward web apps, in the past two decades. We discuss static, dynamic, and hybrid analyses approaches, software testing and test adequacy techniques, as well as techniques that help developers write, analyze and maintain their code. Finally, we present some of the current and future challenges and research opportunities ahead in this field.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {398}, 
	title = {Software services: a research roadmap},
	abstract = {Software services companies offer software development, testing and maintenance as a �service� to other organizations. As a thriv- ing industry in its own right, software services offers certain unique research problems as well as different takes on research problems typically considered in software engineering research. In this paper, we highlight some of these research problems, drawing heavily upon our involvement with IBM Global Business Services organization over the past several years. We focus on four selected topics: how to organize people and the flow of work through people, how to manage knowledge at an organizational level, how to estimate and manage risk in a services engagement, and finally, testing services. These topics by no means cover all areas pertinent to soft- ware services; rather, they reflect ones in which we have personal perspectives to offer. We also share our experience in deployment of research innovations in a large service delivery organization.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {399}, 
	title = {Chapter Five - Advances in Testing JavaScript-Based Web Applications},
	abstract = {Abstract
	JavaScript is a flexible and expressive prototype-based scripting language that is used by developers to create interactive web applications. The language is interpreted, dynamic, weakly typed, and has first-class functions. It also interacts extensively with other web languages such as CSS and HTML at runtime. All these characteristics make JavaScript code particularly error-prone and challenging to analyze and test. In this chapter, we explore recent advances made in analysis and testing techniques geared toward JavaScript-based web applications. In particular, we look at recent empirical studies, testing techniques, test oracle automation approaches, test adequacy assessment methods, fault localization and repair, and Integrated Development Environment support to help programmers write better JavaScript code.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {401}, 
	title = {Open Data: Reverse Engineering and Maintenance Perspective},
	abstract = {Open data is an emerging paradigm to share large and diverse datasets -- primarily from governmental agencies, but also from other organizations -- with the goal to enable the exploitation of the data for societal, academic, and commercial gains. There are now already many datasets available with diverse characteristics in terms of size, encoding and structure. These datasets are often created and maintained in an ad-hoc manner. Thus, open data poses many challenges and there is a need for effective tools and techniques to manage and maintain it. In this paper we argue that software maintenance and reverse engineering have an opportunity to contribute to open data and to shape its future development. From the perspective of reverse engineering research, open data is a new artifact that serves as input for reverse engineering techniques and processes. Specific challenges of open data are document scraping, image processing, and structure/schema recognition. From the perspective of maintenance research, maintenance has to accommodate changes of open data sources by third-party providers, traceability of data transformation pipelines, and quality assurance of data and transformations. We believe that the increasing importance of open data and the research challenges that it brings with it may possibly lead to the emergence of new research streams for reverse engineering as well as for maintenance.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {404}, 
	title = {APOGEN: automatic page object generator for web testing},
	abstract = {Abstract
	Modern web applications are characterized by ultra-rapid development cycles, and web testers tend to pay scant attention to the quality of their automated end-to-end test suites. Indeed, these quickly become hard to maintain, as the application under test evolves. As a result, end-to-end automated test suites are abandoned, despite their great potential for catching regressions. The use of the Page Object pattern has proven to be very effective in end-to-end web testing. Page objects are fa�ade classes abstracting the internals of web pages into high-level business functions that can be invoked by the test cases. By decoupling test code from web page details, web test cases are more readable and maintainable. However, the manual development of such page objects requires substantial coding effort, which is paid off only later, during software evolution. In this paper, we describe a novel approach for the automatic generation of page objects for web applications. Our approach is implemented in the tool Apogen, which automatically derives a testing model by reverse engineering the target web application. It combines clustering and static analysis to identify meaningful page abstractions that are automatically turned into Java page objects for Selenium WebDriver. Our evaluation on an open-source web application shows that our approach is highly promising: Automatically generated page object methods cover most of the application functionalities and result in readable and meaningful code, which can be very useful to support the creation of more maintainable web test suites.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {411}, 
	title = {Technique for coordinating the distributed, parallel crawling of interactive client-server applications},
	abstract = {RESUMO
	An electronic device includes a memory and a processor coupled to the memory. The memory contains a master state graph. The master state graph includes information regarding the operation of interactive client-server application. The processor is configured to send a first job to a first worker node, send a second job to a second worker node, receive results of crawling the interactive client-server application, and integrate results of crawling the interactive client-server application into the master state graph. The first job includes crawling instructions for crawling a first portion of an interactive client-server application. The second job includes crawling instructions for crawling a second portion of the interactive client-server application. The first worker node and second worker node crawl the interactive client-server application in parallel.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {414}, 
	title = {Webbasierte Re-Implementierung von LICARA nanoSCAN},
	abstract = {Abstract
	Thanks to the expensive research the number of nanoparticles and nanomaterials
	and their variations have signifantly increased during the last years.
	Even though great expectations are placed on nanoparticles there are still a
	lot of unanswered questions regarding the benets and risks towards human
	and environment.
	In September 2014, EMPA developed in cooperation with the companies
	TNO and NCB the LICARA guidelines to be used to conduct a Life Cycle,
	Risk and Benet Assessment of nanoparticles. The LICARA nanoSCAN Excel
	version is connected to these guidelines and is acting as a semi-quantitative
	decision support tool while answering questions.
	Within the scope of this bachelor thesis a web based version of the Excel
	tool has been developed to increase the usability of the software. The existing
	Excel tool has been analysed using the Design principles of Donald Norman.
	Based on these results the requirements were made. The developed tool has
	been tested by users and developers of the Excel tool and their feedback has
	been considered during the re-design.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {416}, 
	title = {Refactoring and migration of cascading style sheets: towards optimization and improved maintainability},
	abstract = {Cascading Style Sheets is the standard styling language, and is extensively used for defining the presentation of web, mobile and desktop applications. Despite its popularity, the language's design shortcomings have made CSS development and maintenance challenging. This thesis aims at developing techniques for safely transforming CSS code (through refactoring, or migration to a preprocessor language), with the goal of optimization and improved maintainability.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {417}, 
	title = {Evaluation of resizing capability of web browser},
	abstract = {RESUMO
	A new method and computer program for evaluating page resizing of a web browser. A web page with a test area having test elements is created. Then, the web page is displayed by the web browser and locations of at least two original edge points for each of the test elements are determined. After this, the test area within the web page is resized and displayed with the resized test area. Then, a further determination step is performed for each resized test element in the displayed web page, whereby locations of at least two respective resized edge points of the test element are determined. After obtaining these locations, comparisons between the locations of the at least two original edge points and the locations of the at least two respective resized edge points are made.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {418}, 
	title = {Portability of Process-Aware and Service-Oriented Software},
	abstract = {Abstract
	Modern software systems are becoming increasingly integrated and are required
	to operate over organizational boundaries through networks. The development
	of such distributed software systems has been shaped by the orthogonal trends
	of service-orientation and process-awareness. These trends put an emphasis
	on technological neutrality, loose coupling, independence from the execution
	platform, and location transparency. Execution platforms supporting these
	trends provide context and cross-cutting functionality to applications and are
	referred to as engines.
	Applications and engines interface via language standards. The engine implements
	a standard. If an application is implemented in conformance to this
	standard, it can be executed on the engine. A primary motivation for the usage
	of standards is the portability of applications. Portability, the ability to move
	software among different execution platforms without the necessity for full or partial
	reengineering, protects from vendor lock-in and enables application migration
	to newer engines.
	The arrival of cloud computing has made it easy to provision new and scalable
	execution platforms. To enable easy platform changes, existing international
	standards for implementing service-oriented and process-aware software name
	the portability of standardized artifacts as an important goal. Moreover, they
	provide platform-independent serialization formats that enable the portable
	implementation of applications. Nevertheless, practice shows that service-oriented
	and process-aware applications today are limited with respect to their portability.
	The reason for this is that engines rarely implement a complete standard, but
	leave out parts or differ in the interpretation of the standard. As a consequence,
	even applications that claim to be portable by conforming to a standard might
	not be so.
	This thesis contributes to the development of portable service-oriented and
	process-aware software in two ways: Firstly, it provides evidence for the existence
	of portability issues and the insufficiency of standards for guaranteeing software
	portability. Secondly, it derives and validates a novel measurement framework
	for quantifying portability. We present a methodology for benchmarking the
	conformance of engines to a language standard and implement it in a fully
	automated benchmarking tool. Several test suites of conformance tests for two
	different languages, the Web Services Business Process Execution Language 2.0
	and the Business Process Model and Notation 2.0, allow to uncover a variety of
	standard conformance issues in existing engines. This provides evidence that the
	standard-based portability of applications is a real issue. Based on these results,
	this thesis derives a measurement framework for portability. The framework
	v
	is aligned to the ISO/IEC Systems and software Quality Requirements and
	Evaluation method, the recent revision of the renowned ISO/IEC software quality
	model and measurement methodology. This quality model separates the software
	quality characteristic of portability into the subcharacteristics of installability,
	adaptability, and replaceability. Each of these characteristics forms one part of the
	measurement framework. This thesis targets each characteristic with a separate
	analysis, metrics derivation, evaluation, and validation. We discuss existing
	metrics from the body of literature and derive new extensions specifically tailored
	to the evaluation of service-oriented and process-aware software. Proposed metrics
	are defined formally and validated theoretically using an informal and a formal
	validation framework. Furthermore, the computation of the metrics has been
	prototypically implemented. This implementation is used to evaluate metrics
	performance in experiments based on large scale software libraries obtained from
	public open source software repositories.
	In summary, this thesis provides evidence that contemporary standards and
	their implementations are not sufficient for enabling the portability of processaware
	and service-oriented applications. Furthermore, it proposes, validates, and
	practically evaluates a framework for measuring portability.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {421}, 
	title = {Architecture for distributed, parallel crawling of interactive client-server applications},
	abstract = {RESUMO
	In one embodiment, a distributed computing system includes a first worker node configured to execute a first job, a second worker node configured to execute a second job, and a master node including a processor coupled to a memory. The first job indicates a first portion of an interactive client-server application to be crawled. The second job indicates a second portion of an interactive client-server application to be crawled. The second worker node and the first worker node are configured to execute their respective jobs in parallel. The second job indicates a second portion of an interactive client-server application to be crawled. The master node is configured to assign the first job to the first worker node, assign the second job to the second worker node, and integrate the results from the first worker node and the second worker node into a record of operation of the application.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {424}, 
	title = {Multiple-implementation testing of supervised learning software},
	abstract = {Machine Learning (ML) software, used to implement an ML algorithm, is widely used in many application domains such as financial, business, and engineering domains. Faults in ML software can cause substantial losses in these application domains. Thus, it is very critical to conduct effective testing of ML software to detect and eliminate its faults. However, testing ML software is difficult, especially on producing test oracles used for checking behavior correctness (such as using expected properties or expected test outputs). To tackle the test-oracle issue, this thesis presents a novel black-box approach of multiple-implementation testing for supervised learning software. The insight underlying the approach is that there can be multiple implementations (independently written) for a supervised learning algorithm, and majority of them may produce the expected output for a test input (even if none of these implementations are fault-free). In particular, the proposed approach derives a pseudo oracle for a test input by running the test input on n implementations of the supervised learning algorithm, and then using the common test output produced by a majority (determined by a percentage threshold) of these n implementations. The proposed approach includes techniques to address challenges in multiple-implementation testing (or generally testing) of supervised learning software: the definition of test cases in testing supervised learning software, along with resolution of inconsistent algorithm configurations across implementations. In addition, to improve dependability of supervised learning software during in-field usage while incurring low runtime overhead, The approach includes a multiple-implementation monitoring technique. The evaluations on the proposed approach show that multiple-implementation testing is effective in detecting real faults in real-world ML software (even popularly used ones), including 5 faults from 10 NaiveBayes implementations and 4 faults from 20 k-nearest neighbor implementations, and the proposed technique of multiple-implementation monitoring substantially reduces the need of running multiple implementations with high prediction accuracy.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {427}, 
	title = {Automated layout failure detection for responsive web pages without an explicit oracle},
	abstract = {As the number and variety of devices being used to access the World Wide Web grows exponentially, ensuring the correct presentation of a web page, regardless of the device used to browse it, is an important and challenging task. When developers adopt responsive web design (RWD) techniques, web pages modify their appearance to accommodate a device�s display constraints. However, a current lack of automated support means that presentation failures may go undetected in a page�s layout when rendered for different viewport sizes. A central problem is the difficulty in providing an automated �oracle� to validate RWD layouts against, meaning that checking for failures is largely a manual process in practice, which results in layout failures in many live responsive web sites. This paper presents an automated failure detection technique that checks the consistency of a responsive page�s layout across a range of viewport widths, obviating the need for an explicit oracle. In an empirical study, this method found failures in 16 of 26 real-world production pages studied, detecting 33 distinct failures in total.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {428}, 
	title = {Online shopping cart},
	abstract = {This thesis is about online shopping cart systems. After researching the various shopping cart systems, a user friendly shopping cart system has been developed and a mail delivery system has been incorporated into this shopping cart system. A mail delivery system is a system where the user can enter the email ID of a friend and suggest a product. Moreover, users can have a record of the items bought.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {430}, 
	title = {An Empirical Study of Internationalization Failures in the Web},
	abstract = {Abstract:
	Web application internationalization frameworks allow businesses to more easily market and sell their products and services around the world. However, internationalization can lead to problems. Text expansion and contraction after translation may result in a distortion of the layout of the translated versions of a webpage, which can reduce their usability and aesthetics. In this paper, we investigate and report on the frequency and severity of different types of failures in webpages' user interfaces that are due to internationalization. In our study, we analyzed 449 real world internationalized webpages. Our results showed that internationalization failures occur frequently and they range significantly in terms of severity and impact on the web applications. These findings motivate and guide future work in this area.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {431}, 
	title = {A Crowdsourcing framework for Detecting Cross-Browser Issues in Web Application},
	abstract = {With the advent of Web 2.0 application, and the increasing number of browsers and platforms on which the applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious problem for organizations to develop web-based software with good user experience. Although some techniques and tools have been proposed to identify XBIs, some XBIs are still missed as only partial state space is explored (by the crawler) in the testing environment. To address this limitation, based on record/replay technique, this paper proposed a crowdsourcing framework to detect cross-browser issues for Web application deployed in the field. Our empirical evaluation shows that the proposed technique is effective and efficient, improves on the state of the art.},
	duplicado = {false},
	inserir = {}
}
@Article{
	id = {433}, 
	title = {Engenharia reversa de padr�es de intera��o},
	abstract = {Graphical user interfaces (GUIs) are populated with recurring behaviors that vary only slightly. For example, authentication (login / password) is a behavior common to many software applications. However, there are different behaviors between different implementations of this behavior. Sometimes a message appears when the user does not enter the correct data, sometimes, the application software only erases entered data and shows no indication to the user. These recurring behaviors (UI patterns) are well identified in the literature.The goal of this dissertation is to continue the work already done on an existing tool called PARADIGM-RE, a dynamic reverse engineering approach to extract User Interface (UI) Patterns from existent Web applications. As such, we will develop a data analysis module with the goal of improving and substantiate the existing identifying heuristics set, and we will extend the current set of identifiable patterns.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {434}, 
	title = {Detecting and Localizing Visual Inconsistencies in Web Applications},
	abstract = {Abstract:
	Failures in the presentation layer of a web application can negatively impact its usability and end users' perception of the application's quality. The problem of verifying the consistency of a web application's user interface across its different pages is one of the many challenges that software development teams face in testing the presentation layer. In this paper we propose a novel automated approach to detect and localize visual inconsistencies in web applications. To detect visual inconsistencies, our approach uses computer vision techniques to compare a test web page with its reference. Then to localize, our approach analyzes the structure and style of the underlying HTML elements to find the faulty elements responsible for the observed inconsistencies.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {438}, 
	title = {Detection of dead widgets in software applications},
	abstract = {In one embodiment, a user interface includes at least one instance of each of at least one widget. Recording a plurality of widget interaction instances (WIIs) for the user interface, each WII resulting from a user interaction applied to a particular instance of a particular widget. Clustering the plurality of WIIs based on a text value and a path value of each WII, such that each cluster of WIIs is associated with a particular widget. Determining, for each of at least one cluster of WIIs, whether the particular widget associated with the cluster of WIIs is erroneous based on whether user interactions corresponding to the WIIs in the cluster have produced responses from a software application that includes the user interface.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {440}, 
	title = {Technique for stateless distributed parallel crawling of interactive client-server applications},
	abstract = {A distributed computing system includes worker nodes and a master node including a processor coupled to a memory. Each worker node crawls a portion of an interactive client-server application. The memory includes a master state graph, including the results of crawling. The master node is configured to examine the master state graph to determine a number of reconverging traces, receive a result from a job from a worker node if the number of reconverging traces is below a threshold, and add the result to the master state graph without attempting to remove duplicate states or transitions. A trace includes states and transitions representing valid. A reconvergent trace includes a trace including a reconvergent state, which is a state that can be reached through two or more distinct traces. The result containing states and transitions is associated with crawling a first portion of the interactive client-server application.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {441}, 
	title = {CSSDev: refactoring duplication in cascading style sheets},
	abstract = {Cascading Style Sheets (CSS) is a widely-used language for defining the presentation of structured documents and user interfaces. Despite its popularity, CSS still lacks adequate tool support for everyday maintenance tasks, such as debugging and refactoring. In this paper, we present CSSDev, a tool suite for analyzing CSS code to detect refactoring opportunities. (https://youtu.be/lu3oITi1XrQ)},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {320}, 
	title = {A Mapping Study on Software Product Lines Testing Tools},
	abstract = {Abstract�The benefits of using a software testing tool in order
	to achieve significant reductions in cost and time to market,
	and, at the same time, increasing the quality has encouraged
	the adoption of testing tools both in single systems and product
	lines. In this context, this study focuses on the following goals:
	analyze how the available tools are supporting the Software
	Product Lines (SPL) Testing Process, investigate the state-of-theart
	on single system and SPL testing tools, synthesize available
	evidence, and identify gaps among the tools, available in the
	literature. A mapping study was undertaken to analyze important
	aspects that should be considered when adopting testing tools. A
	set of four research questions were defined in which 33 studies,
	dated from 1999 to 2011, were evaluated. From the total of 33
	studies considered, 24 of them described single system testing
	tools and the other 9 described SPL testing tools. However, there
	is insufficient information about publications describing tools
	used in the industry. There is no tool suitable to all testing levels
	of a SPL, researchers need to consider the feasibility of adapting
	existing tools or constructing new tools.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {321}, 
	title = {A Comparative Study of Web Application Testing and Mobile Application Testing},
	abstract = {Abstract:
	Web application have gained increased acceptance over the years in companies and organization as the world move to a global village. Software developers have also grown interest in developing web applications compared to stand-alone application because of the immense benefits it offers such as ubiquity, platform dependence, low cost of support and maintenance, better speed and performance, piracy proof etc. As mobile application emerged in the last decade, attention has been focused on mobile applications by organizations and businesses in order to maximize their profits as much as possible. There has been a rapid increase of software release in the mobile applications store. As the growth of both web application and mobile application increase, the question of quality assurance remains a concern. A comparative study of software testing techniques can be performed to improve the standard of testing of both web and mobile application. This paper therefore reviews the similarity and difference in the testing mechanism.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {323}, 
	title = {An Oracle based on Image Comparison for Regression Testing of Web Applications},
	abstract = {Abstract�Much work has been done on automating regression
	testing for Web applications, but most of them focus on test
	data generation or test execution. Little work has been done
	on automatically determining if a test passed or failed; testers
	would need to visually confirm the result which can be a tedious
	task. The difficulty is compounded by the fact that parts of a
	Web page (such as advertisements) may change each time the
	Web application is executed even though it has no bearing on the
	Web application function itself. We thus propose a test oracle for
	automatically determining the result of regression testing a Web
	application. The key point of our approach is the identification
	of parts that may change, which we call variable region. We
	first generate the expected result, by executing the original (premodification)
	Web application multiple times so that variable
	regions can be identified. Then, after the Web application is
	modified, regression testing is conducted by comparing the output
	of the modified Web application against the expected output. An
	evaluation confirmed the usefulness of our approach.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {325}, 
	title = {Automated Techniques to Detect Faults Early in Large Software Applications},
	abstract = {Abstract Modern software applications are very complex and they need fre-quent changes 
	as per the changes in user requirements. These applications are developed using the 
	combination of various different programming languages. They consist of a multi-tiered 
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {326}, 
	title = {Graphical-user-interface-based method and system for designing and configuring web-site testing and analysis},
	abstract = {The current application is directed to methods and systems for designing and configuring web-site testing and analysis. In certain implementations, a testing service collects customer page-access and conversion information on behalf of a web site. The testing service is straightforwardly accessed and configured, through a web-site-based graphical user interface, and is virtually incorporated into the web site.
	},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {328}, 
	title = {A Survey of Dynamic Analysis and Test Generation for JavaScript},
	abstract = {JavaScript has become one of the most prevalent programming languages. Unfortunately, some of the unique
	properties that contribute to this popularity also make JavaScript programs prone to errors and difficult for
	program analyses to reason about. These properties include the highly dynamic nature of the language, a set
	of unusual language features, a lack of encapsulation mechanisms, and the �no crash� philosophy. This paper
	surveys dynamic program analysis and test generation techniques for JavaScript targeted at improving the
	correctness, reliability, performance, security, and privacy of JavaScript-based software.
	
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {330}, 
	title = {Detecting latent cross-platform API violations},
	abstract = {Abstract:
	Many APIs enable cross-platform system development by abstracting over the details of a platform, allowing application developers to write one implementation that will run on a wide variety of platforms. Unfortunately, subtle differences in the behavior of the underlying platforms make cross-platform behavior difficult to achieve. As a result, applications using these APIs can be plagued by bugs difficult to observe before deployment. These portability bugs can be particularly difficult to diagnose and fix because they arise from the API implementation, the operating system, or hardware, rather than application code. This paper describes CheckAPI, a technique for detecting violations of cross-platform portability. CheckAPI compares an application's interactions with the API implementation to its interactions with a partial specification-based API implementation, and does so efficiently enough to be used in real production systems and at runtime. CheckAPI finds latent errors that escape pre-release testing. This paper discusses the subtleties of different kinds of API calls and strategies for effectively producing the partial implementations. Validating CheckAPI on JavaScript, the Seattle project's Repy VM, and POSIX detects dozens of violations that are confirmed bugs in widely-used software.
	},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {332}, 
	title = {Measuring and Improving Website User Experience using UX Methodologies: A Case Study on Cross Browser Compatibility Heuristic},
	abstract = {Abstract�The growing diversity of web-client platform
	configurations causes websites to vary unpredictably, creating a
	myriad of challenges during software development life cycle
	(SDLC). This eventually affects websites user experience (UX).
	Exploratory heuristic evaluation (EHE) and lab-based usability
	testing (LBUT) are popular usability evaluation methods (UEM)
	that could be used to measure and improve the websites UX.
	Hence, the objective of this study is to derive and validate the
	EHE and LBUT process flow for website cross browser
	compatibility. In addition to finding compatibility defects using
	EHE, it is important to determine the reliability of these defects
	using LBUT. When it comes to improving websites UX, root
	cause analysis (RCA) is performed and recommendation are
	provided to the design and development team. From the results,
	it can be concluded that the compatibility guideline and process
	flow developed can improve the productivity and reliability of the
	EHE and LBUT methodologies.
	
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {447}, 
	title = {Crawling Ajax-Based Web Applications through Dynamic Analysis of User Interface State Changes},
	abstract = {Using JavaScript and dynamic DOM manipulation on the client side of Web applications is becoming a widespread approach for achieving rich interactivity and responsiveness in modern Web applications. At the same time, such techniques---collectively known as Ajax---shatter the concept of webpages with unique URLs, on which traditional Web crawlers are based. This article describes a novel technique for crawling Ajax-based applications through automatic dynamic analysis of user-interface-state changes in Web browsers. Our algorithm scans the DOM tree, spots candidate elements that are capable of changing the state, fires events on those candidate elements, and incrementally infers a state machine that models the various navigational paths and states within an Ajax application. This inferred model can be used in program comprehension and in analysis and testing of dynamic Web states, for instance, or for generating a static version of the application. In this article, we discuss our sequential and concurrent Ajax crawling algorithms. We present our open source tool called Crawljax, which implements the concepts and algorithms discussed in this article. Additionally, we report a number of empirical studies in which we apply our approach to a number of open-source and industrial Web applications and elaborate on the obtained results.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {448}, 
	title = {Real Challenges in Mobile App Development},
	abstract = {Abstract:
	Context: Mobile app development is a relatively new phenomenon that is increasing rapidly due to the ubiquity and popularity of smartphones among end-users. Objective: The goal of our study is to gain an understanding of the main challenges developers face in practice when they build apps for different mobile devices. Method: We conducted a qualitative study, following a Grounded Theory approach, in which we interviewed 12 senior mobile developers from 9 different companies, followed by a semi-structured survey, with 188 respondents from the mobile development community. Results: The outcome is an overview of the current challenges faced by mobile developers in practice, such as developing apps across multiple platforms, lack of robust monitoring, analysis, and testing tools, and emulators that are slow or miss many features of mobile devices. Conclusion: Based on our findings of the current practices and challenges, we highlight areas that require more attention from the research and development community.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {449}, 
	title = {Invariant-Based Automatic Testing of Modern Web Applications},
	abstract = {Abstract:
	Ajax-based Web 2.0 applications rely on stateful asynchronous client/server communication, and client-side runtime manipulation of the DOM tree. This not only makes them fundamentally different from traditional web applications, but also more error-prone and harder to test. We propose a method for testing Ajax applications automatically, based on a crawler to infer a state-flow graph for all (client-side) user interface states. We identify Ajax-specific faults that can occur in such states (related to, e.g., DOM validity, error messages, discoverability, back-button compatibility) as well as DOM-tree invariants that can serve as oracles to detect such faults. Our approach, called Atusa, is implemented in a tool offering generic invariant checking components, a plugin-mechanism to add application-specific state validators, and generation of a test suite covering the paths obtained during crawling. We describe three case studies, consisting of six subjects, evaluating the type of invariants that can be obtained for Ajax applications as well as the fault revealing capabilities, scalability, required manual effort, and level of automation of our testing approach.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {451}, 
	title = {A systematic mapping study of web application testing},
	abstract = {Context
	
	The Web has had a significant impact on all aspects of our society. As our society relies more and more on the Web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 147 papers in the area of web application testing, which have appeared between 2000 and 2011.
	
	Objective
	
	As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends in this specialized field.
	
	Method
	
	We review and structure the body of knowledge related to web application testing through a systematic mapping (SM) study. As part of this study, we pose two sets of research questions, define selection and exclusion criteria, and systematically develop and refine a classification schema. In addition, we conduct a bibliometrics analysis of the papers included in our study.
	
	Results
	
	Our study includes a set of 79 papers (from the 147 retrieved papers) published in the area of web application testing between 2000 and 2011. We present the results of our systematic mapping study. Our mapping data is available through a publicly-accessible repository. We derive the observed trends, for instance, in terms of types of papers, sources of information to derive test cases, and types of evaluations used in papers. We also report the demographics and bibliometrics trends in this domain, including top-cited papers, active countries and researchers, and top venues in this research area.
	
	Conclusion
	
	We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing web application testing approaches and indentify areas in the field that require more attention from the research community.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {454}, 
	title = {Reverse Engineering iOS Mobile Applications},
	abstract = {Abstract:
	As a result of the ubiquity and popularity of smart phones, the number of third party mobile applications is explosively growing. With the increasing demands of users for new dependable applications, novel software engineering techniques and tools geared towards the mobile platform are required to support developers in their program comprehension and analysis tasks. In this paper, we propose a reverse engineering technique that automatically (1) hooks into, dynamically runs, and analyzes a given iOS mobile application, (2) exercises its user interface to cover the interaction state space and extracts information about the runtime behaviour, and (3) generates a state model of the given application, capturing the user interface states and transitions between them. Our technique is implemented in a tool called iCrawler. To evaluate our technique, we have conducted a case study using six open-source iPhone applications. The results indicate that iCrawler is capable of automatically detecting the unique states and generating a correct model of a given mobile application.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {457}, 
	title = {Mining questions asked by web developers},
	abstract = {Modern web applications consist of a significant amount of client- side code, written in JavaScript, HTML, and CSS. In this paper, we present a study of common challenges and misconceptions among web developers, by mining related questions asked on Stack Over- flow. We use unsupervised learning to categorize the mined questions and define a ranking algorithm to rank all the Stack Overflow questions based on their importance. We analyze the top 50 questions qualitatively. The results indicate that (1) the overall share of web development related discussions is increasing among developers, (2) browser related discussions are prevalent; however, this share is decreasing with time, (3) form validation and other DOM related discussions have been discussed consistently over time, (4) web related discussions are becoming more prevalent in mobile development, and (5) developers face implementation issues with new HTML5 features such as Canvas. We examine the implications of the results on the development, research, and standardization communities.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {458}, 
	title = {The first decade of GUI ripping: Extensions, applications, and broader impacts},
	abstract = {Abstract:
	This paper provides a retrospective examination of GUI Ripping - reverse engineering a workflow model of the graphical user interface of a software application - born a decade ago out of recognition of the severe need for improving the then largely manual state-of-the-practice of functional GUI testing. In these last 10 years, GUI ripping has turned out to be an enabler for much research, both within our group at Maryland and other groups. Researchers have found new and unique applications of GUI ripping, ranging from measuring human performance to re-engineering legacy user interfaces. GUI ripping has also enabled large-scale experimentation involving millions of test cases, thereby helping to understand the nature of GUI faults and characteristics of test cases to detect them. It has resulted in large multi-institutional Government-sponsored research projects on test automation and benchmarking. GUI ripping tools have been ported to many platforms, including Java AWT and Swing, iOS, Android, UNO, Microsoft Windows, and web. In essence, the technology has transformed the way researchers and practitioners think about the nature of GUI testing, no longer considered a manual activity; rather, thanks largely to GUI Ripping, automation has become the primary focus of current GUI testing techniques.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {459}, 
	title = {Feedback-Directed Exploration of Web Applications to Derive Test Models},
	abstract = {Abstract�Dynamic exploration techniques play a significant
	role in automated web application testing and analysis. However,
	a general web application crawler that exhaustively explores
	the states can become mired in limited specific regions of
	the web application, yielding poor functionality coverage. In
	this paper, we propose a feedback-directed web application
	exploration technique to derive test models. While exploring,
	our approach dynamically measures and applies a combination
	of code coverage impact, navigational diversity, and structural
	diversity, to decide a-priori (1) which state should be expanded,
	and (2) which event should be exercised next to maximize the
	overall coverage, while minimizing the size of the test model.
	Our approach is implemented in a tool called FEEDEX. We
	have empirically evaluated the efficacy of FEEDEX using six web
	applications. The results show that our technique is successful in
	yielding higher coverage while reducing the size of the test model,
	compared to classical exhaustive techniques such as depth-first,
	breadth-first, and random exploration.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {461}, 
	title = {DLint: dynamically checking bad coding practices in JavaScript},
	abstract = {JavaScript has become one of the most popular programming languages, yet it is known for its suboptimal design. To effectively use JavaScript despite its design flaws, developers try to follow informal code quality rules that help avoid correctness, maintainability, performance, and security problems. Lightweight static analyses, implemented in "lint-like" tools, are widely used to find violations of these rules, but are of limited use because of the language's dynamic nature. This paper presents DLint, a dynamic analysis approach to check code quality rules in JavaScript. DLint consists of a generic framework and an extensible set of checkers that each addresses a particular rule. We formally describe and implement 28 checkers that address problems missed by state-of-the-art static approaches. Applying the approach in a comprehensive empirical study on over 200 popular web sites shows that static and dynamic checking complement each other. On average per web site, DLint detects 49 problems that are missed statically, including visible bugs on the web sites of IKEA, Hilton, eBay, and CNBC.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {462}, 
	title = {Software engineering for the web: the state of the practice},
	abstract = {Today�s web applications increasingly rely on client-side code execution. HTML is not just created on the server, but manipulated extensively within the browser through JavaScript code. In this paper, we seek to understand the software engineering implications of this. We look at deviations from many known best practices in such areas of performance, accessibility, and correct structuring of HTML documents. Furthermore, we assess to what extent such deviations manifest themselves through client-side code manipulation only. To answer these questions, we conducted a large scale experiment, involving automated client-enabled crawling of over 4000 web applications, resulting in over 100,000,000 pages analyzed, and close to 1,000,000 unique client-side user interface states. Our findings show that the majority of sites contain a substantial number of problems, making sites unnecessarily slow, inaccessible for the visually impaired, and with layout that is unpredictable due to errors in the dynamically modified DOM trees.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {464}, 
	title = {Extended Subtree: A New Similarity Function for Tree Structured Data},
	abstract = {Abstract:
	Although several distance or similarity functions for trees have been introduced, their performance is not always satisfactory in many applications, ranging from document clustering to natural language processing. This research proposes a new similarity function for trees, namely Extended Subtree (EST), where a new subtree mapping is proposed. EST generalizes the edit base distances by providing new rules for subtree mapping. Further, the new approach seeks to resolve the problems and limitations of previous approaches. Extensive evaluation frameworks are developed to evaluate the performance of the new approach against previous proposals. Clustering and classification case studies utilizing three real-world and one synthetic labeled data sets are performed to provide an unbiased evaluation where different distance functions are investigated. The experimental results demonstrate the superior performance of the proposed distance function. In addition, an empirical runtime analysis demonstrates that the new approach is one of the best tree distance functions in terms of runtime efficiency.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {465}, 
	title = {Extracting Interaction-Based Stateful Behavior in Rich Internet Applications},
	abstract = {Abstract:
	Although asynchronous technologies such as Ajax make Rich Internet Applications (RIAs) responsive, they can result in unexpected behavior due to nondeterministic client-side processing and asynchronous communication. One difficulty in understanding such erroneous behavior lies in the unpredictable contexts of the running system. Dynamic behavior analysis techniques do not help to verify the correctness of certain "blind spots" in the execution path. In this work, we present a static approach for extracting all possible state transitions described in source code from the RIAs. Our approach is based on the assumption that user, server and self interactions with the RIAs can change the states of the application. Our method consists of three steps: (i) annotating interactions and extracting their controls in source code (ii) abstracting a call graph to extract relationships among the interactions and (iii) refining the relationships with interaction controls By extracting the state machines of test scenarios of the correct and wrong behavior, it can help developers to pinpoint the statements in the source code that lead to the erroneous behavior. Our approach has been evaluated against a few experimental cases and we conclude that it can extract comprehensible state machines in a reasonable time.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {468}, 
	title = {Detecting inconsistencies in multi-platform mobile apps},
	abstract = {Abstract:
	Due to the increasing popularity and diversity of mobile devices, developers write the same mobile app for different platforms. Since each platform requires its own unique environment in terms of programming languages and tools, the teams building these multi-platform mobile apps are usually separate. This in turn can result in inconsistencies in the apps developed. In this paper, we propose an automated technique for detecting inconsistencies in the same native app implemented for iOS and Android platforms. Our technique (1) automatically instruments and traces the app on each platform for given execution scenarios, (2) infers abstract models from each platform execution trace, (3) compares the models using a set of code-based and GUI-based criteria to expose any discrepancies, and finally (4) generates a visualization of the models, highlighting any detected inconsistencies. We have implemented our approach in a tool called CheckCAMP. CheckCAMP can help mobile developers in testing their apps across multiple platforms. An evaluation of our approach with a set of 14 industrial and open-source multi-platform native mobile app-pairs indicates that CheckCAMP can correctly extract and abstract the models of mobile apps from multiple platforms, infer likely mappings between the generated models based on different comparison criteria, and detect inconsistencies at multiple levels of granularity.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {469}, 
	title = {Crawl-based analysis of web applications: Prospects and challenges},
	abstract = {Abstract
	In this paper we review five years of research in the field of automated crawling and testing of web applications. We describe the open source Crawljax tool, and the various extensions that have been proposed in order to address such issues as cross-browser compatibility testing, web application regression testing, and style sheet usage analysis.
	Based on that we identify the main challenges and future directions of crawl-based testing of web applications. In particular, we explore ways to reduce the exponential growth of the state space, as well as ways to involve the human tester in the loop, thus reconciling manual exploratory testing and automated test input generation. Finally, we sketch the future of crawl-based testing in the light of upcoming developments, such as the pervasive use of touch devices and mobile computing, and the increasing importance of cyber-security.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {470}, 
	title = {Automated Acceptance Testing of JavaScript Web Applications},
	abstract = {Abstract:
	Acceptance testing is an important part of software development and it is performed to ensure that a system delivers its required functionalities. Today, most modern interactive web applications are designed using Web 2.0 technologies, many among them relying on JavaScript. JavaScript enables the development of client-side functionality through the dynamic modification of the web-page's content and structure without calls to the server. This implies that server-side testing frameworks will necessarily fail to test the complete application behaviors. In this paper we present a method for automated acceptance testing of JavaScript web applications to ensure that required functionalities have been implemented. Using an intuitive, human-readable scripting language our method allows users to describe user stories in high level declarative test scripts and to then execute these test scripts on a web application using an automated website crawler. We also describe a case study that evaluates our approach in terms of capabilities to translate user stories in automated acceptance test scripts.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {472}, 
	title = {Visual testing of Graphical User Interfaces: An exploratory study towards systematic definitions and approaches},
	abstract = {Abstract:
	Graphical User Interface (GUI) testing literature emphasizes testing a system's functionality through its GUI, rather than testing visual aspects of the GUI itself. In this paper we introduce the notion of visual testing as a subset of GUI testing. To explore visual testing, we have conducted a study of defects in four open source systems. We found that visual defects represent between 16% and 33% of reported defects in those systems. Two categories of visual defects are identified with six subcategories within each of them. Other findings are also reported that are aimed at motivating the importance and the need for systematically conducting visual testing among researchers and practitioners.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {478}, 
	title = {Native and Multiple Targeted Mobile Applications},
	abstract = {Abstract
	Together with the expansion of the WWW we are seeing the expansion of mobile devices that are becoming more and more pervasive. Mobile application development is becoming more and more complex as users of mobile applications are demanding more high quality software. Our contribution is to frame the positive and negative aspects of native and multiple targeted mobile applications that should be considered by the involved stakeholders more particularly the software organization decision-makers.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {480}, 
	title = {Automated verification of pattern-based interaction invariants in Ajax applications},
	abstract = {When developing asynchronous JavaScript and XML (Ajax) applications, developers implement Ajax design patterns for increasing the usability of the applications. However, unpredictable contexts of running applications might conceal faults that will break the design patterns, which decreases usability. We propose a support tool called JSVerifier that automatically verifies interaction invariants; the applications handle their interactions in invariant occurrence and order. We also present a selective set of interaction invariants derived from Ajax design patterns, as input. If the application behavior breaks the design patterns, JSVerifier automatically outputs faulty execution paths for debugging. The results of our case studies show that JSVerifier can verify the interaction invariants in a feasible amount of time, and we conclude that it can help developers increase the usability of Ajax applications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {483}, 
	title = {Validating ajax applications using a delay-based mutation technique},
	abstract = {The challenge of validating Asynchronous JavaScript and XML (Ajax) applications lies in actual errors exposed in a user environment. Several studies have proposed effective and efficient testing techniques to identify executable faults. However, the applications might have faults that are not executed during testing, but might cause actual errors in a user environment. Although we have investigated static methods for finding ``potential faults'' that seem to cause actual errors if executed, developers need to confirm whether or not the potential faults are actually executable. Herein, we propose a mutation-based testing method implemented in a tool called JSPreventer. Even if the potential faults are not easily executable in a given environment, our method mutates the applications until they are executable using two delay-based mutation operators to manipulate the timing of the applications handling interactions. Thus, JSPreventer provides executable evidences of the not-easily-executable faults for developers, if it reveals actual errors by testing the mutated applications. We applied our method to real-world applications and found actual errors that developers could debug to improve their reliability. Therefore, JSPreventer can help developers validate reliable real-world Ajax applications.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {490}, 
	title = {How to Train Your Browser: Preventing XSS Attacks Using Contextual Script Fingerprints},
	abstract = {Cross-Site Scripting (XSS) is one of the most common web application vulnerabilities. It is therefore sometimes referred to as the �buffer overflow of the web.� Drawing a parallel from the current state of practice in preventing unauthorized native code execution (the typical goal in a code injection), we propose a script whitelisting approach to tame JavaScript-driven XSS attacks. Our scheme involves a transparent script interception layer placed in the browser�s JavaScript engine. This layer is designed to detect every script that reaches the browser, from every possible route, and compare it to a list of valid scripts for the site or page being accessed; scripts not on the list are prevented from executing. To avoid the false positives caused by minor syntactic changes (e.g., due to dynamic code generation), our layer uses the concept of contextual fingerprints when comparing scripts.
	
	Contextual fingerprints are identifiers that represent specific elements of a script and its execution context. Fingerprints can be easily enriched with new elements, if needed, to enhance the proposed method�s robustness. The list can be populated by the website�s administrators or a trusted third party. To verify our approach, we have developed a prototype and tested it successfully against an extensive array of attacks that were performed on more than 50 real-world vulnerable web applications. We measured the browsing performance overhead of the proposed solution on eight websites that make heavy use of JavaScript. Our mechanism imposed an average overhead of 11.1% on the execution time of the JavaScript engine. When measured as part of a full browsing session, and for all tested websites, the overhead introduced by our layer was less than 0.05%. When script elements are altered or new scripts are added on the server side, a new fingerprint generation phase is required. To examine the temporal aspect of contextual fingerprints, we performed a short-term and a long-term experiment based on the same websites. The former, showed that in a short period of time (10 days), for seven of eight websites, the majority of valid fingerprints stay the same (more than 92% on average). The latter, though, indicated that, in the long run, the number of fingerprints that do not change is reduced. Both experiments can be seen as one of the first attempts to study the feasibility of a whitelisting approach for the web.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {492}, 
	title = {Analyzing web applications: An empirical study},
	abstract = {Abstract
	Due to the increased usage of JavaScript in web applications and the speed at which web technologies and browsers are evolving, web applications are be- coming ever more complex. Our hypothesis is that these applications contain severe errors, take unnecessary performance penalties, and violate accessibility standards. This study analyzes such errors and tries to quantify the need for a tool that can help developers make web applications with less errors. The research is conducted by first showing how much of the DOM is modified after the initial page load. This could indicate that static analysis does not suffice anymore. After that we quantify the amount of faults in the web application. The research is done on 3,422 sites randomly selected from the internet. They were automatically analyzed using a crawler. We conclude that the use of static analysis tools to prevent these faults does not suffice anymore. The errors and accessibility standard violations happen in dynamically generated DOM, which are not detectable by static analysis. The performance penalties are only visible through dynamic analysis. We propose to develop a random testing tool based on a crawler that checks for these errors. Our main contributions are the design of such a tool, the large dataset that we have gathered during this research and the quantification of both the level of dynamism of modern web applications and the fault-proneness of these applications due to this dynamism.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {493}, 
	title = {cross browser compatibility as design issue in various websites.},
	abstract = {Abstract  -  In this current era of information technology websites are very important means of communication. Lot of efforts is required by different institutions / organizations to portray complete information on beautifully designed websites.  Websites act as an online agent through which a user can get his work done without physically visiting the organizations.  Website design is given with a very critical look by the designer so that it can provide users with all the facilities of the concerned institutions / organizations online. To make websites behavior similar in all the different browsers employed by the different categories of the users, the responsibility of the designer and the concerned institutions / organizations increases manifold. In this research paper author developed an online tool using .NET Framework using C# to study cross browser compatibility as Design issue in various categories of the websites like Job portals, Government, educational,  Commercial and Social networking. The automated tool developed by author function on the basis of the different standards prescribed in W3C guidelines document UAAG 2.0 [7] and act like a parser and renders the complete code of the website and produces result on basis of the behavior of the websites in five most popular and widely used Browsers like parameters like Internet Explorer[7,8,9], Chrome, Safari, Fire fox. Each Browser is tested on the basis of the five parameters which are included in the parser are Blinking, Active X control, Website Resolution; image Formats, HTML Tag errors. The results  obtained after testing five different categories of websites shows that educational and social networking sites shows least compatibility in multiple browsers where as job portals, commercial and government websites shows 100% compliance to the website design standards recommended by W3C w.r.t browser compatibility of different websites on different browsing platform.    },
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {495}, 
	title = {???XaaS????????????????},
	abstract = {???????Xaa S????????????????????,????????????????????????????,?????????????????????????????????????Xaa S????????????????,????????????????},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {504}, 
	title = {Diversity-Based Automated Test Case Generation},
	abstract = {Software testing is an expensive task that consumes around half of a project�s effort. To reduce the cost of testing and improve the software quality, test cases can be produced automatically. Random Testing (RT) is a low cost and straightforward automated test generation approach. However, its effectiveness is not satisfactory. To increase the effectiveness of RT, researchers have developed more effective test generation approaches such as Adaptive Random Testing (ART) which improves the testing by increasing the test case coverage of the input domain. This research proposes new test case generation methods that improve the effectiveness of the test cases by increasing the diversity of the test cases. Numerical, string, and tree test case structures are investigated. For numerical test generation, the use of Centroidal Voronoi Tessellations (CVT) is proposed. Accordingly, a test case generation method, namely Random Border CVT (RBCVT), is introduced which can enhance the previous RT methods to improve their coverage of the input space. The generated numerical test cases by the other methods act as the input to the RBCVT algorithm and the output is an improved set of test cases. An extensive simulation study and a mutant based software testing investigation have been performed demonstrating that RBCVT outperforms previous methods. For string test cases, two objective functions are introduced to produce effective test cases. The diversity of the test cases is the first objective, where it can be measured through string distance functions. The second objective is guiding the string length distribution into a Benford distribution which implies shorter strings have, in general, a higher chance of failure detection. When both objectives are enforced via a multi-objective optimization algorithm, superior string test sets are produced. An empirical study is performed with several real-world programs indicating that the generated string test cases outperform test cases generated by other methods. Prior to tree test generation study, a new tree distance function is proposed. Although several distance or similarity functions for trees have been introduced, their failure detection performance is not always satisfactory. This research proposes a new similarity function for trees, namely Extended Subtree (EST), where a new subtree mapping is proposed. EST generalizes the edit base distances by providing new rules for subtree mapping. Further, the new approach seeks to resolve the problems and limitations of previous approaches. Extensive evaluation frameworks are developed to evaluate the performance of the new approach against previous methods. Clustering and classification case studies are performed to provide an evaluation against different tree distance functions. The experimental results demonstrate the superior performance of the proposed distance function. In addition, an empirical runtime analysis demonstrates that the new approach is one of the best tree distance functions in terms of runtime efficiency. Finally, the study on the string test case generation is extended to tree test case generation. An abstract tree model is defined by a user based on a program under the test. Then, tree test cases are produced according to the model where diversity is maximized through an evolutionary optimization technique. Real world programs are used to investigate the performance of generated test cases where superior performance of the introduced method is demonstrated compared to the previous methods. Further, the proposed tree distance function is compared against the previous functions in the tree test case generation context. The proposed tree distance function outperforms other functions in tree test generation. },
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {507}, 
	title = {On the Use of Smelly Examples to Detect Code Smells in JavaScript},
	abstract = {Abstract
	JavaScript has become one of the widely-used languages. However, as the size of JavaScript-based applications grows, the number of defects grows as well. Recent studies have produced a set of manually defined rules to identify these defects. We propose, in this work, the automation of deriving these rules to ensure scalability and potentially the detection of a wider set of defects without requiring any extensive knowledge on rules tuning. To this end, we rely on a base of existing code smells that is used to train the detection rules using Genetic Programming and find the best threshold of metrics composing the rules. The evaluation of our work on 9 JavaScript web projects has shown promising results in terms of detection precision of 92% and recall of 85%, with no threshold tuning required.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {512}, 
	title = {Search-based testing tools for Ajax- A systematic literature review},
	abstract = {Search-based testing seeks to solve many novel problems including testing Ajax applications, and there have been a number of tools created to accomplish this purpose. Objective: This thesis aims to identify search-based software testing tools for Ajax web applications and how they have been evaluated. Method: Systematic literature review is used as the research methodology. Result: There are six different tools identified in scientific literature of which three are variants of Crawljax. Also, searchbased testing tools for Ajax are primarily evaluated through the use of case studies. Conclusion: The evaluation of the identified tools should be conducted using an experimental design in order to make them comparable and repeatable as well as follow benchmarking frameworks proposed in scientific literature.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {513}, 
	title = {Mining Stack Overflow for questions asked by web developers : an empirical study},
	abstract = {Modern web applications consist of a significant amount of client-side code,
	written in JavaScript, HTML, and CSS. In this thesis, we present a study
	of common challenges and misconceptions among web developers, by mining
	related questions asked on Stack Overflow. We use unsupervised learning to
	categorize the mined questions and define a ranking algorithm to rank all the
	Stack Overflow questions based on their importance. We analyze the top 50
	questions qualitatively. The results indicate that (1) the overall share of web
	development related discussions is increasing among developers, (2) browser
	related discussions are prevalent; however, this share is decreasing with time,
	(3) form validation and other DOM related discussions have been discussed
	consistently over time, (4) web related discussions are becoming more prevalent
	in mobile development, and (5) developers face implementation issues
	with new HTML5 features such as Canvas. We examine the implications
	of the results on the development, research, and standardization communities.
	Our results show that there is a consistent knowledge gap between the
	options available and options known to developers. Given the presence of
	knowledge gap among developers, we need better tools customized to assist
	developers in building web applications.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {521}, 
	title = {Browser Compatibility: the State of the Art from a Historical Perspective},
	abstract = {ABSTRACT
	Browser compatibility is an aspect of web development
	that reaches back to the beginning of the internet. Because
	browsers have been marketed by various vendors
	in a competitive landscape, fragmentation was inevitable.
	This creates technical challenges for web developers whom
	have incentive to create websites that perform predictably
	on different browsers, browser versions and rendering engines.
	Over the years the browser landscape has changed
	and so has mutual compatibility. Until now, no research
	has been conducted that provides insight in the developments
	of this area. Little is therefore known about the
	state of the art relative to the historical context. This paper
	provides this missing information by using available
	data sources. The outcomes show that developers should
	be less concerned with supporting legacy browsers and versions
	and can be more confident about feature support because
	of an increase in use of more compatible browsers.
	However, it was also found that feature support among
	browsers is decreasing.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {524}, 
	title = {Automated modularization of GUI test cases},
	abstract = {Test cases that drive an application under test via its graphical user interface (GUI) consist of sequences of steps that perform actions on, or verify the state of, the application user interface. Such tests can be hard to maintain, especially if they are not properly modularized---that is, common steps occur in many test cases, which can make test maintenance cumbersome and expensive. Performing modularization manually can take up considerable human effort. To address this, we present an automated approach for modularizing GUI test cases. Our approach consists of multiple phases. In the first phase, it analyzes individual test cases to partition test steps into candidate subroutines, based on how user-interface elements are accessed in the steps. This phase can analyze the test cases only or also leverage execution traces of the tests, which involves a cost-accuracy tradeoff. In the second phase, the technique compares candidate subroutines across test cases, and refines them to compute the final set of subroutines. In the last phase, it creates callable subroutines, with parameterized data and control flow, and refactors the original tests to call the subroutines with context-specific data and control parameters. Our empirical results, collected using open-source applications, illustrate the effectiveness of the approach.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {527}, 
	title = {A Study on Similarity Function for Tree Structured Data},
	abstract = {Abstract
	
	We have several distance or similarity functions for trees, but their performance is not always adequate in different applications. In the base paper the Extended Sub tree (EST) function, where a new sub tree mapping is proposed. This similarity function is to compare tree structured data by defining a new set of mapping rules where sub trees are mapped rather than nodes. To reduce the time complexity as well as computational complexity of the system, efficient pruning algorithm is proposed. In the proposed system the unnecessary computation is reduced in the tree structured data by using the lossless pruning strategy. This paper provides major advancement in efficiency. This pruning strategy is ignoring the node or sub tree which has greater value than the ignoring probability. By using this technique, we can reduce the extra computation complexity.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {533}, 
	title = {??AJAX???????????},
	abstract = {????????AJAx????????????????????????????????????????????????????????????????????????????K????????????????????????????????????????????????????????????????????????????????????????????????????????AJAx???????????????????},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {535}, 
	title = {Supporting to Find Faults in Rich Internet Applications by Extracting Interaction-based State Machines},
	abstract = {Asynchronous technologies such as Ajax make Rich Internet Applications (RIAs) responsive. When implementing and maintaining RIAs, developers have difficulties in figuring out complex behavior of the applications due to nondeterministic elements such as user events. Several researches have conducted to extract state machines based on execution results of Ajax applications for understanding support and testing. However, these execution results are within a limit of execution scenarios and environments prepared by developers. In this paper, we propose a tool that statically extracts state machines from Ajax-based RIAs by focusing on interactions with RIAs. We argue that the interactions can change the states of the application. Looking at both the extracted state machines and the source code, developers can verify the correctness of certain blind spots in the execution paths. From experimental results, we concluded that our tool could help participants understand the behavior and find faults. },
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {541}, 
	title = {Leveraging existing tests in automated test generation for web applications},
	abstract = {To test web applications, developers currently write test cases in frameworks such as Selenium. On the other hand, most web test generation techniques rely on a crawler to explore the dynamic states of the application. The first approach requires much manual effort, but benefits from the domain knowledge of the developer writing the test cases. The second one is automated and systematic, but lacks the domain knowledge required to be as effective. We believe combining the two can be advantageous. In this paper, we propose to (1) mine the human knowledge present in the form of input values, event sequences, and assertions, in the human-written test suites, (2) combine that inferred knowledge with the power of automated crawling, and (3) extend the test suite for uncovered/unchecked portions of the web application under test. Our approach is implemented in a tool called Testilizer. An evaluation of our approach indicates that Testilizer (1) outperforms a random test generator, and (2) on average, can generate test suites with improvements of up to 150% in fault detection rate and up to 30% in code coverage, compared to the original test suite.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {551}, 
	title = {POGen: A Test Code Generator Based on Template Variable Coverage in Gray-Box Integration Testing for Web Applications},
	abstract = {Abstract. Web applications are complex; they consist of many subsystems
	and run on various browsers and platforms. This makes it difficult to
	conduct adequate integration testing to detect faults in the connections
	between subsystems or in the specific environments. Therefore, establishing
	an efficient integration testing method with the proper test adequacy
	criteria and tools is an important issue.
	In this paper, we propose a new test coverage called template variable
	coverage. We also propose a novel technique for generating skeleton test
	code that includes accessor methods and improves the template variable
	coverage criterion, using a tool that we developed called POGen. Our
	experiments show that template variable coverage correlates highly with
	the capability to detect faults, and that POGen can reduce testing costs},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {582}, 
	title = {A Grey-box Approach for Automated GUI-Model Generation of Mobile Applications},
	abstract = {Abstract. As the mobile platform continues to pervade all aspects of
	human activity, and mobile applications, or mobile apps for short, on
	this platform tend to be faulty just like other types of software, there is
	a growing need for automated testing techniques for mobile apps. Modelbased
	testing is a popular and important testing approach that operates
	on a model of an app�s behavior. However, such a model is often not available
	or of insufficient quality. To address this issue, we present a novel
	grey-box approach for automatically extracting a model of a given mobile
	app. In our approach, static analysis extracts the set of events supported
	by the Graphical User Interface (GUI) of the app. Then dynamic crawling
	reverse-engineers a model of the app, by systematically exercising
	these events on the running app. We also present a tool implementing
	this approach for the Android platform. Our empirical evaluation of this
	tool on several Android apps demonstrates that it can efficiently extract
	compact yet reasonably comprehensive models of high quality for such
	apps.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {584}, 
	title = {Mining behavior models from enterprise web applications},
	abstract = {Today's enterprise web applications demand very high release cycles---and consequently, frequent tests. Automating these tests typically requires a behavior model: A description of the states the application can be in, the transitions between these states, and the expected results. Furthermore one needs scripts to make the abstract actions (transitions) in the model executable. As specifying such behavior models and writing the necessary scripts manually is a hard task, a possible alternative could be to extract them from existing applications. However, mining such models can be a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present ProCrawl (PROcess CRAWLer), a generic approach to mine behavior models from (multi-user) enterprise web applications. ProCrawl observes the behavior of the application through its user interface, generates and executes tests to explore unobserved behavior. In our evaluation of three non-trivial web applications (an open-source shop system, an SAP product compliance application, and an open-source conference manager), ProCrawl produces models that precisely abstract application behavior and which can be directly used for effective model-based regression testing.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {586}, 
	title = {Web Application Model Generation through Reverse Engineering and UI Pattern Inferring},
	abstract = {Abstract:
	A great deal of effort in model-based testing is related to the creation of the model. In addition, the model itself, while a powerful tool of abstraction, can have conceptual errors, introduced by the tester. These problems can be reduced by generating those models automatically. This paper presents a dynamic reverse engineering approach that aims to extract part of the model of an existing web application through the identification of User Interface (UI) patterns. This reverse engineering approach explores automatically any web application, records information related to the interaction, analyses the gathered information, tokenizes it, and infers the existing UI patterns via syntactical analysing. After being complemented with additional information and validated, the model extracted is the input for the Pattern-Based Graphical User Interface Testing (PBGT) approach for testing existing web application under analysis.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {587}, 
	title = {A Testing Tool for Web Applications Using a Domain-Specific Modelling Language and the NuSMV Model Checker},
	abstract = {Abstract:
	Test case generation from formal models using model checking software is an established method. This paper presents a model-based testing approach for web applications based on a domain-specific language model. It is shown how the domain-specific language is transformed into the input language of the NuSMV model checker and how the resulting traces are converted into executable test scripts for various test automation tools. The presented approach has been implemented with comprehensive automation in a research tool which architecture is outlined.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {590}, 
	title = {TypeDevil: dynamic type inconsistency analysis for JavaScript},
	abstract = {Dynamic languages, such as JavaScript, give programmers the freedom to ignore types, and enable them to write concise code in short time. Despite this freedom, many programs follow implicit type rules, for example, that a function has a particular signature or that a property has a particular type. Violations of such implicit type rules often correlate with problems in the program. This paper presents TypeDevil, a mostly dynamic analysis that warns developers about inconsistent types. The key idea is to assign a set of observed types to each variable, property, and function, to merge types based in their structure, and to warn developers about variables, properties, and functions that have inconsistent types. To deal with the pervasiveness of polymorphic behavior in real-world JavaScript programs, we present a set of techniques to remove spurious warnings and to merge related warnings. Applying TypeDevil to widely used benchmark suites and real-world web applications reveals 15 problematic type inconsistencies, including correctness problems, performance problems, and dangerous coding practices.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {592}, 
	title = {Model Reverse-engineering of Mobile Applications with Exploration Strategies},
	abstract = {Abstract�This paper presents a model reverse-engineering approach
	for mobile applications that belong to the Graphical User
	Interface (GUI) application category. This approach covers the
	interfaces of an application with automatic testing to incrementally
	infer a formal model expressing the navigational paths and
	states of the application. We propose the definition of a specialised
	GUI application model which stores the discovered interfaces
	and helps limit the application exploration. Then, we present an
	algorithm based upon the Ant Colony Optimisation technique
	which offers the possibility to parallelise the exploration and
	to conceive any application exploration strategy. Finally, our
	approach is experimented on Android applications and compared
	to other tools available in the literature.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {593}, 
	title = {Enabling collaborative testing across shared software components},
	abstract = {Components of numerous software systems are developed and maintained by multiple stakeholders, and there is significant overlap and synergy in the process of testing systems with shared components. We have designed and implemented infrastructure that enables testers of different components to share their test results and artifacts so that they can collaborate in testing shared components. We also develop an example collaborative testing process that leverages our infrastructure to save effort for regression testing of systems with shared components. Our empirical study of this process shows that collaborative testing of component-based software systems can not only save significant effort by sharing test results and artifacts, but also improve test quality of individual components by utilizing synergistic data shared among component testers.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {594}, 
	title = {Mining Workflow Models from Web Applications},
	abstract = {Abstract:
	Modern business applications predominantly rely on web technology, enabling software vendors to efficiently provide them as a service, removing some of the complexity of the traditional release and update process. While this facilitates shorter, more efficient and frequent release cycles, it requires continuous testing. Having insight into application behavior through explicit models can largely support development, testing and maintenance. Model-based testing allows efficient test creation based on a description of the states the application can be in and the transitions between these states. As specifying behavior models that are precise enough to be executable by a test automation tool is a hard task, an alternative is to extract them from running applications. However, mining such models is a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present Process Crawler (ProCrawl), a tool to mine behavior models from web applications that support multi-user workflows. ProCrawl incrementally learns a model by generating program runs and observing the application behavior through the user interface. In our evaluation on several real-world web applications, ProCrawl extracted models that concisely describe the implemented workflows and can be directly used for model-based testing.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {595}, 
	title = {Model inference combining expert systems and formal models},
	abstract = {Many works relating to software engineering rely upon formal models to perform
	model-checking or automatic test case generation. Nonetheless, producing
	these models is tedious and error-prone. Model inference is a recent
	research field helping in the production of models. This approach aims at
	generating models from documentations or from execution traces (observed
	action sequences). This paper presents a new model generation method combining
	model inference with expert systems. Intuitively, an engineer is able
	to recognise the functional behaviours of an application from its traces by
	applying deduction rules. We propose a framework, simulating this way of
	deducting, with inference rules organised into layers. Each yields partial
	IOSTSs (Input Output Symbolic Transition System), which becomes more
	and more abstract and understandable. For event-driven applications, our
	proposal is also composed of a crawler, which aims at exploring the application
	by means of automatic testing. This crawler is guided in the traversal
	of the application with strategies that are implemented with inference rules
	as well.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {596}, 
	title = {Inferring models with rule-based expert systems},
	abstract = {Many works related to software engineering rely upon formal models, e.g., to perform model-checking or automatic test case generation. Nonetheless, producing such models is usually tedious and error-prone. Model inference is a research field helping in producing models by generating partial models from documentation or execution traces (observed action sequences). This paper presents a new model generation method combining model inference and expert systems. It appears that an engineer is able to recognise the functional behaviours of an application from its traces by applying deduction rules. We propose a framework, applied to Web applications, simulating this reasoning mechanism, with inference rules organised into layers. Each yields partial IOSTSs (Input Output Symbolic Transition Systems), which become more and more abstract and understandable.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {597}, 
	title = {Model Inference and Automatic Testing of Mobile Applications},
	abstract = {Abstract�We consider, in this paper, the problem of automatically
	testing Mobile applications while inferring formal
	models expressing their functional behaviours. We propose a
	framework called MCrawlT, which performs automatic testing
	through application interfaces and collects interface changes to
	incrementally infer models expressing the navigational paths
	and states of the applications under test. These models could
	be later used for comprehension aid or to carry out some
	tasks automatically, e.g., the test case generation. The main
	contributions of this paper can be summarised as follows: we
	introduce a flexible Mobile application model that allows the
	definition of state abstraction with regard to the application
	content. This definition also helps define state equivalence
	classes that segment the state space domain. Our approach
	supports different exploration strategies by applying the Ant
	Colony Optimisation technique. This feature offers the advantage
	to change the exploration strategy by another one
	as desired. The performances of MCrawlT in terms of code
	coverage, execution time, and bug detection are evaluated on 30
	Android applications and compared to other tools found in the
	literature. The results show that MCrawlT achieves significantly
	better code coverage in a given time budget.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {601}, 
	title = {Domain-Driven Model Inference Applied To Web Applications},
	abstract = {Abstract�Model inference methods are attracting increased
	attention from industrials and researchers since they can be
	used to generate models for software comprehension, for test
	case generation, or for helping devise a complete model
	(or documentation). In this context, this paper presents an
	original inference model approach which recovers models
	from Web application HTTP traces. This approach combines
	formal model inference with domain-driven expert systems. Our
	framework, whose purpose is to simulate this human behaviour,
	is composed of inference rules, translating the domain expert
	knowledge, organised into layers. Each yields partial IOSTSs
	(Input Output Symbolic Transition System), which become more
	and more abstract and intelligible.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {603}, 
	title = {Reverse Engineering and Testing Dynamic Web Applications},
	abstract = {A new generation of complex interactive dynamic web applications has emerged with the introduction
	of Web 2.0 technologies and development frameworks. The characteristics of dynamic web
	applications such as runtime DOM structure and content updates introduced new challenges in the
	understanding, maintenance and testing of this type of web applications. In this work we address
	two important challenges in the field of web application maintenance. The first challenge is that of
	modelling web application behaviour. To solve this task we develop an automatic method for reverse
	engineering the features of dynamic web applications by applying a hierarchical clustering algorithm
	based on a novel composite-tree-edits-aware distance metric between DOM tree instances of a web
	application. The proposed distance metric recognizes simple and composite structural changes in
	a DOM tree. We have evaluated our method on three real-world web applications. The evaluation
	results demonstrated that the proposed distance metric produces a number of clusters that is close to
	the actual number of features and, also, classifies DOM trees into feature clusters more accurately
	than other traditional distance metrics. The second challenge is that of systematic acceptance (and
	regression) testing at the user-interface level, which we address by developing a tool, CrawlScripter,
	for performing automated acceptance testing of JavaScript web applications. CrawlScripter allows
	to create easy-to-understand acceptance tests using the provided library of high-level instructions.
	The ability of CrawlScripter to create automated acceptance tests for different test scenarios was
	evaluated on both pedagogical and real-world dynamic web applications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {605}, 
	title = {Automated test generation for production systems with a model-based testing approach},
	abstract = {R�sum� : Ce manuscrit de th�se porte sur le probl�me du test bas� mod�le de syst�mes de production existants, tels ceux de notre partenaire industriel Michelin, l�un des trois plus grands fabricants de pneumatiques au monde. Un syst�me de production est compos� d�un ensemble de machines de production contr�l�es par un ou plusieurs logiciels au sein d�un atelier dans une usine. Malgr� les nombreux travaux dans le domaine du test bas� mod�le, l��criture de mod�les permettant de d�crire un syst�me sous test ou sa sp�cification reste un probl�me r�current, en partie � cause de la complexit� d�une telle t�che. De plus, un mod�le est utile lorsqu�il est � jour par rapport � ce qu�il d�crit, ce qui implique de le maintenir dans le temps. Pour autant, conserver une documentation � jour reste compliqu� puisqu�il faut souvent le faire manuellement. Dans notre contexte, il est important de souligner le fait qu�un syst�me de production fonctionne en continu et ne doit �tre ni arr�t� ni perturb�, ce qui limite l�usage des techniques de test classiques. Pour pallier le probl�me de l��criture de mod�les, nous proposons une approche pour construire automatiquement des mod�les depuis des s�quences d��v�nements observ�s (traces) dans un environnement de production. Pour se faire, nous utilisons les informations fournies par les donn�es �chang�es entre les �l�ments qui composent un syst�me de production. Nous adoptons une approche bo�te noire et combinons les notions de syst�me expert, inf�rence de mod�les et machine learning, afin de cr�er des mod�les comportementaux. Ces mod�les inf�r�s d�crivent des comportements complets, enregistr�s sur un syst�me analys�. Ces mod�les sont partiels, mais �galement tr�s grands (en terme de taille), ce qui les rend difficilement utilisable par la suite. Nous proposons une technique de r�duction sp�cifique � notre contexte qui conserve l��quivalence de traces entre les mod�les de base et les mod�les fortement r�duits. Gr�ce � cela, ces mod�les inf�r�s deviennent int�ressant pour la g�n�ration de documentation, la fouille de donn�es, mais �galement le test. Nous proposons une m�thode passive de test bas� mod�le pour r�pondre au probl�me du test de syst�mes de production sans interf�rer sur leur bon fonctionnement. Cette technique permet d�identifier des diff�rences entre deux syst�mes de production et r�utilise l�inf�rence de mod�les d�crite pr�c�demment. Nous introduisons deux relations d�implantation : une relation bas�e sur l�inclusion de traces, et une seconde relation plus faible propos�e, pour rem�dier au fait que les mod�les inf�r�s soient partiels. Enfin, ce manuscrit de th�se pr�sente Autofunk, un framework modulaire pour l�inf�rence de mod�les et le test de syst�mes de production qui aggr�ge les notions mentionn�es pr�c�demment. Son impl�mentation en Java a �t� appliqu�e sur diff�rentes applications et syst�mes de production chez Michelin dont les r�sultats sont donn�s dans ce manuscrit. Le prototype d�velopp� lors de la th�se a pour vocation de devenir un outil standard chez Michelin.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {609}, 
	title = {Leveraging task-based data to support functional testing of web applications},
	abstract = {Testing is paramount in order to assure the quality of a software product. Over the last years, several techniques have been proposed to leverage the testing phase as a simple and efficient step during software development. However, the features of the web environment make application testing fairly complex. The existing approaches for web application testing are usually driven to specific scenarios or application types, and few solutions are targeted for testing the functional requirements of applications. In order to tackle this problem, we propose a task-based testing approach that provides high coverage of functional requirements. Our technique consists of reassembling classical graph algorithms in order to generate all the possible paths for the execution of a task. Performed experiments indicate that our approach is effective for supporting the functional testing of web applications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {610}, 
	title = {CREATING A TESTING FRAMEWORK AND WORKFLOW FOR DEVELOPERS NEW TO WEB APPLICATION ENGINEERING},
	abstract = {Web applications are quickly replacing standalone applications for everyday tasks.
	These web applications need to be tested to ensure proper functionality and reliability.
	There have been substantial efforts to create tools that assist with the testing of web
	applications, but there is no standard set of tools or a recommended workflow to
	ensure speed of development and strength of application.
	We have used and outlined the merits of a number of existing testing tools and
	brought together the best among them to create what we believe is a fully-featured,
	easy to use, testing framework and workflow for web application development.
	We then took an existing web application, PolyXpress, and augmented its development
	process to include our workflow suggestions in order to incorporate testing at
	all levels. PolyXpress is a web application that �allows you to create location-based
	stories, build eTours, or create restaurant guides. It is the tool that will bring people
	to locations in order to entertain, educate, or provide amazing deals.�[10] After incorporating
	our testing procedures, we immediately detected previously unknown bugs
	in the software. In addition, there is now a workflow in place for future developers to
	use which will expedite their testing and development.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {612}, 
	title = {Reverse engineering of web applications},
	abstract = {Even so many years after its genesis, the Internet is still growing. Not only are
	the users increasing, so are the number of different programming languages or
	frameworks for building Web applications. However, this plethora of technologies
	makes Web applications� source code hard to comprehend and understand,
	thus deteriorating both their debugging and their maintenance costs.
	In this context, a number of proposals have been put forward to solve
	this problem. While, on one hand, there are techniques that analyze the entire
	source code of Web applications, the diversity of available implementation
	technology makes these techniques return unsatisfactory results. On the other
	hand, there are also techniques that dynamically (but blindly) explore the applications
	by running them and analyzing the results of randomly exploring
	them. In this case the results are better, but there is always the chance that
	some part of the application might be left unexplored.
	This thesis investigates if an hybrid approach combining static analysis and
	dynamic exploration of the user interface can provide better results. FREIA, a
	framework developed in the context of this thesis, is capable of analyzing Web
	applications automatically, deriving structural and behavioral interface models
	from them.
	Mesmo decorridos tantos anos desde a sua g�nese, a Internet continua a crescer.
	Este crescimento aplica-se n�o s� ao n�mero de utilizadores como tamb�m ao
	n�mero de diferentes linguagens de programa��o e frameworks utilizadas para
	a constru��o de aplica��es Web. No entanto, esta pletora de tecnologias leva
	a que o c�digo fonte das aplica��es Web seja dif�cil de compreender e analisar,
	deteriorando tanto o seu depuramento como os seus custos de manuten��o.
	Neste contexto, foram desenvolvidas algumas propostas com intuito de resolver
	este problema. N�o obstante, por um lado, existirem t�cnicas que analisam
	a totalidade do c�digo fonte das aplica��es Web, a diversidade das tecnologias
	de implementa��o existentes fazem com que estas t�cnicas gerem
	resultados insatisfat�rios. Por outro lado, existem tamb�m t�cnicas que, dinamicamente
	(apesar de cegamente), exploram as aplica��es, executando-as e
	analisando os resultados da sua explora��o aleat�ria. Neste caso, os resultados
	s�o melhores, mas corremos o risco de ter deixado alguma parte da aplica��o
	por explorar.
	Esta tese investiga se uma abordagem h�brida, combinando a an�lise est�tica
	com a explora��o din�mica da interface do utilizador consegue produzir
	melhores resultados. FREIA, uma framework desenvolvida no contexto desta
	tese � capaz de, automaticamente, analisar aplica��es Web, derivando modelos
	estruturais e comportamentais da interface das mesmas.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {615}, 
	title = {Automated Test Input Generation for Android: Are We There Yet? (E)},
	abstract = {Abstract:
	Like all software, mobile applications ("apps") must be adequately tested to gain confidence that they behave correctly. Therefore, in recent years, researchers and practitioners alike have begun to investigate ways to automate apps testing. In particular, because of Android's open source nature and its large share of the market, a great deal of research has been performed on input generation techniques for apps that run on the Android operating systems. At this point in time, there are in fact a number of such techniques in the literature, which differ in the way they generate inputs, the strategy they use to explore the behavior of the app under test, and the specific heuristics they use. To better understand the strengths and weaknesses of these existing approaches, and get general insight on ways they could be made more effective, in this paper we perform a thorough comparison of the main existing test input generation tools for Android. In our comparison, we evaluate the effectiveness of these tools, and their corresponding techniques, according to four metrics: ease of use, ability to work on multiple platforms, code coverage, and ability to detect faults. Our results provide a clear picture of the state of the art in input generation for Android apps and identify future research directions that, if suitably investigated, could lead to more effective and efficient testing tools for Android.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {617}, 
	title = {Automated Generation of Oracles for Testing User-Interaction Features of Mobile Apps},
	abstract = {Abstract:
	As the use of mobile devices becomes increasingly ubiquitous, the need for systematically testing applications (apps) that run on these devices grows more and more. However, testing mobile apps is particularly expensive and tedious, often requiring substantial manual effort. While researchers have made much progress in automated testing of mobile apps during recent years, a key problem that remains largely untracked is the classic oracle problem, i.e., to determine the correctness of test executions. This paper presents a novel approach to automatically generate test cases, that include test oracles, for mobile apps. The foundation for our approach is a comprehensive study that we conducted of real defects in mobile apps. Our key insight, from this study, is that there is a class of features that we term user-interaction features, which is implicated in a significant fraction of bugs and for which oracles can be constructed - in an application agnostic manner -- based on our common understanding of how apps behave. We present an extensible framework that supports such domain specific, yet application agnostic, test oracles, and allows generation of test sequences that leverage these oracles. Our tool embodies our approach for generating test cases that include oracles. Experimental results using 6 Android apps show the effectiveness of our tool in finding potentially serious bugs, while generating compact test suites for user-interaction features.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {619}, 
	title = {Finding HTML presentation failures using image comparison techniques},
	abstract = {Presentation failures in web applications can negatively affect an application's usability and user experience. To find such failures, testers must visually inspect the output of a web application or exhaustively specify invariants to automatically check a page's correctness. This makes finding presentation failures labor intensive and error prone. In this paper, we present a new automated approach for detecting and localizing presentation failures in web pages. To detect presentation failures, our approach uses image processing techniques to compare a web page and its oracle. Then, to localize the failures, our approach analyzes the page with respect to its visual layout and identifies the HTML elements likely to be responsible for the failure. We evaluated our approach on a set of real-world web applications and found that the approach was able to accurately detect failures and identify the faulty HTML elements.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {622}, 
	title = {Detection and Localization of HTML Presentation Failures Using Computer Vision-Based Techniques  Sign In or Purchase},
	abstract = {Abstract:
	An attractive and visually appealing appearance is important for the success of a website. Presentation failures in a site's web pages can negatively impact end users' perception of the quality of the site and the services it delivers. Debugging such failures is challenging because testers must visually inspect large web pages and analyze complex interactions among the HTML elements of a page. In this paper we propose a novel automated approach for debugging web page user interfaces. Our approach uses computer vision techniques to detect failures and can then identify HTML elements that are likely to be responsible for the failure. We evaluated our approach on a set of real-world web applications and found that the approach was able to accurately and quickly identify faulty HTML elements.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {625}, 
	title = {Testing Cross-Platform Mobile App Development Frameworks (T)},
	abstract = {Abstract:
	Mobile app developers often wish to make their apps available on a wide variety of platforms, e.g., Android, iOS, and Windows devices. Each of these platforms uses a different programming environment, each with its own language and APIs for app development. Small app development teams lack the resources and the expertise to build and maintain separate code bases of the app customized for each platform. As a result, we are beginning to see a number of cross-platform mobile app development frameworks. These frameworks allow the app developers to specify the business logic of the app once, using the language and APIs of a home platform (e.g., Windows Phone), and automatically produce versions of the app for multiple target platforms (e.g., iOS and Android). In this paper, we focus on the problem of testing cross-platform app development frameworks. Such frameworks are challenging to develop because they must correctly translate the home platform API to the (possibly disparate) target platform API while providing the same behavior. We develop a differential testing methodology to identify inconsistencies in the way that these frameworks handle the APIs of the home and target platforms. We have built a prototype testing tool, called X-Checker, and have applied it to test Xamarin, a popular framework that allows Windows Phone apps to be cross-compiled into native Android (and iOS) apps. To date, X-Checker has found 47 bugs in Xamarin, corresponding to inconsistencies in the way that Xamarin translates between the semantics of the Windows Phone and the Android APIs. We have reported these bugs to the Xamarin developers, who have already committed patches for twelve of them.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {626}, 
	title = {Cross-platform feature matching for web applications},
	abstract = {With the emergence of new computing platforms, software written for traditional platforms is being re-targeted to reach the users on these new platforms. In particular, due to the proliferation of mobile computing devices, it is common practice for companies to build mobile-specific versions of their existing web applications to provide mobile users with a better experience. Because the differences between desktop and mobile versions of a web application are not only cosmetic, but can also include substantial rewrites of key components, it is not uncommon for these different versions to provide different sets of features. Whereas some of these differences are intentional, such as the addition of location-based features on mobile devices, others are not and can negatively affect the user experience, as confirmed by numerous user reports and complaints. Unfortunately, checking and maintaining the consistency of different versions of an application by hand is not only time consuming, but also error prone. To address this problem, and help developers in this difficult task, we propose an automated technique for matching features across different versions of a multi-platform web application. We implemented our technique in a tool, called FMAP, and used it to perform a preliminary empirical evaluation on nine real-world multi-platform web applications. The results of our evaluation are promising, as FMAP was able to correctly identify missing features between desktop and mobile versions of the web applications considered, as confirmed by our analysis of user reports and software fixes for these applications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {630}, 
	title = {Root cause analysis for HTML presentation failures using search-based techniques},
	abstract = {Presentation failures in web applications can negatively impact users' perception of the application's quality and its usability. Such failures are challenging to diagnose and correct since the user interfaces of modern web applications are defined by a complex interaction between HTML tags and their visual properties defined by CSS and HTML attributes. In this paper, we introduce a novel approach for automatically identifying the root cause of presentation failures in web applications that uses image processing and search based techniques. In an experiment conducted for assessing the accuracy of our approach, we found that it was able to identify the correct root cause with 100% accuracy.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {636}, 
	title = {Bayesian Statistics in Software Engineering: Practical Guide and Case Studies},
	abstract = {Statistics comes in two main flavors: frequentist and Bayesian. For historical and technical reasons, frequentist statistics has dominated data analysis in the past; but Bayesian statistics is making a comeback at the forefront of science. In this paper, we give a practical overview of Bayesian statistics and illustrate its main advantages over frequentist statistics for the kinds of analyses that are common in empirical software engineering, where frequentist statistics still is standard. We also apply Bayesian statistics to empirical data from previous research investigating agile vs. structured development processes, the performance of programming languages, and random testing of object-oriented programs. In addition to being case studies demonstrating how Bayesian analysis can be applied in practice, they provide insights beyond the results in the original publications (which used frequentist statistics), thus showing the practical value brought by Bayesian statistics.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {642}, 
	title = {ReDeCheck: an automatic layout failure checking tool for responsively designed web pages},
	abstract = {Since people frequently access websites with a wide variety of devices (e.g., mobile phones, laptops, and desktops), developers need frameworks and tools for creating layouts that are useful at many viewport widths. While responsive web design (RWD) principles and frameworks facilitate the development of such sites, there is a lack of tools supporting the detection of failures in their layout. Since the quality assurance process for responsively designed websites is often manual, time-consuming, and error-prone, this paper presents ReDeCheck, an automated layout checking tool that alerts developers to both potential unintended regressions in responsive layout and common types of layout failure. In addition to summarizing ReDeCheck�s benefits, this paper explores two different usage scenarios for this tool that is publicly available on GitHub.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {645}, 
	title = {The rise of Chrome},
	abstract = {Since Chrome�s initial release in 2008 it has grown in market share, and now controls roughly half of the desktop browsers market. In contrast with Internet Explorer, the previous dominant browser, this was not achieved by marketing practices such as bundling the browser with a pre-loaded operating system. This raises the question of how Chrome achieved this remarkable feat, while other browsers such as Firefox and Opera were left behind. We show that both the performance of Chrome and its conformance with relevant standards are typically better than those of the two main contending browsers, Internet Explorer and Firefox. In addition, based on a survey of the importance of 25 major features, Chrome product managers seem to have made somewhat better decisions in selecting where to put effort. Thus the rise of Chrome is consistent with technical superiority over the competition.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {646}, 
	title = {Declarative layout constraints for testing web applications},
	abstract = {Abstract
	The paper focuses on bugs in web applications that can be detected by analyzing the contents and layout of page elements inside a browser's window. Based on an empirical analysis of 35 real-world web sites and applications (such as Facebook, Dropbox, and Moodle), it provides a survey and classification of more than 90 instances of layout-based bugs. It then introduces Cornipickle, an automated testing tool that provides a declarative language to express desirable properties of a web application as a set of human-readable assertions on the page's HTML and CSS data. Such properties can be verified on-the-fly as a user interacts with an application.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {648}, 
	title = { Refactoring and migration of cascading style sheets: towards optimization and improved maintainability},
	abstract = {Cascading Style Sheets is the standard styling language, and is extensively used for defining the presentation of web, mobile and desktop applications. Despite its popularity, the language's design shortcomings have made CSS development and maintenance challenging. This thesis aims at developing techniques for safely transforming CSS code (through refactoring, or migration to a preprocessor language), with the goal of optimization and improved maintainability.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {649}, 
	title = {An automatic layout faults detection technique in responsive web pages considering JavaScript defined dynamic layouts},
	abstract = {Abstract:
	Revolutionary changes in technology has brought a lots of different types of devices towards the Internet enabled services. It has become challenging to keep web sites functional and user friendly in a wide range of devices. Responsive Web Design (RWD) steps up to facilitate the necessary support to overcome those challenges with complex cascading style sheet (CSS) and JavaScript defined layouts. Fluidly modified layout that controls appearance of Responsive Web Pages (RWP) is subjected to erroneous when faulty CSS and JavaScript modifies the actual look of the web page. Though several methods have already been proposed to detect layout faults in web pages, they are error-prone and time consuming in nature. Thus an automatic approach considering both CSS and JavaScript defined dynamic layout fault detection technique have been proposed in this paper. Simulation result demonstrates that proposed technique outperforms the existing methods in case of time requirement, memory requirement and fault detection rate.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {651}, 
	title = {Searching for behavioural bugs with stateful test oracles in web crawlers},
	abstract = {Web applications pervade all aspects of human activity today. Therefore the content of the web has become extremely important. According to the great number of applications present nowadays, as a consequence, the manifestation of a bug has become very common. Testing modern web applications, so called "Web 2.0" applications has become more difficult due to their "stateful" nature, so the development of an approach capable of testing these applications in order to detect these bugs has become a necessity. The paper presents an automated approach for testing these dynamic web applications, where a combination of dynamic crawling and back-end testing is used to automatically detect behavioural bugs.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {665}, 
	title = {Unsupervised Outlier Detection in Software Engineering},
	abstract = {The increasing complexity of software systems has lead to increased demands
	on the tools and methods used when developing software systems.
	To determine if a tool or method is more efficient or accurate than others
	empirical studies are used. The data used in empirical studies might be
	affected by outliers i.e. data points that deviates significantly from the
	rest of the data set. Hence, the statistical analysis might be distorted by
	these outliers as well.
	This study investigates if outliers are present within Empirical Software
	Engineering (ESE) studies using unsupervised methods for detection.
	It also tries to assess if the statistical analyses performed in ESE studies
	are affected by outliers by removing them and performing a re-analysis.
	The subjects used in this study comes from a narrow literature review of
	recently published papers within Software Engineering (SE). While collecting
	the samples needed for this study the current state of practise
	regarding data availability and analysis reproducibility is investigated.
	This study�s results shows that outliers can be found in ESE studies
	and it also identifies issues regarding data availability within the same
	field. Finally, this study presents guidelines for how to improve the way
	outlier detection is presented within ESE studies as well as guidelines for
	publishing data.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {668}, 
	title = {Automated Recommender System for Web Based Applications},
	abstract = {Abstract:
	Online shopping new way of business in present days based on the previous surfing and purchasing products are recommended
	to the users. The existing method of recommending the product has to undergo several processes or functionalities and these
	processes or functionalities are manually tested for the accuracy. The manual testing method requires lot of time and money and
	other resources. To overcome the problem this paper proposes a Automation Testing for the recommender system, with Feature
	Vector Algorithm and perform a automation on each modules of the Feature Vector algorithm and also checks the CrossBrowser
	compatibility across the browser and also collecting the online reviews from by using Web Crawling Technique.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {669}, 
	title = {Class coverage GUI testing for Android applications},
	abstract = {Abstract:
	Mobile devices such as smartphones and tablets have become an integral part of a person's life. These portable devices opened up a new software market for mobile application development resulting in various applications from healthcare, banking till entertainment. Therefore, there is a need for mobile applications to be reliable and maintainable. In this paper we introduce an equivalent class based technique for testing the graphical user interface of Android applications. This technique is a specification based approach, in which test cases are generated based on the functionalities and the graphical user interface specification. For each possible user interface event a set of test cases are generated using equivalence class partitioning approach. Once the test cases are generated for the given application, the app is executed based on the generated test cases and results are compared with the other testing techniques. From the obtained results we can infer that our approach detects more bugs than other previous work. In addition, this approach helps in the generation of test cases at an early in the app development life cycle.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {333}, 
	title = {WebSee: A Tool for Debugging HTML Presentation Failures},
	abstract = {Abstract:
	Presentation failures in a website can negatively impact end users' perception of the quality of the website, the services it delivers, and the branding a company is trying to achieve. Presentation failures can occur easily in modern web applications because of the highly complex and dynamic nature of the HTML, CSS, and JavaScript that define a web page's visual appearance. Debugging such failures manually is time consuming and error-prone, and existing techniques do not provide an automated debugging solution. In this paper, we present our tool, WebSee, that provides a fully automated debugging solution for presentation failures in web applications. When run on real-world web applications, WebSee was able to accurately and quickly identify faulty HTML elements.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {334}, 
	title = {Using Visual Symptoms for Debugging Presentation Failures in Web Applications},
	abstract = {Abstract:
	Presentation failures in a website can undermine its success by giving users a negative perception of the trustworthiness of the site and the quality of the services it delivers. Unfortunately, existing techniques for debugging presentation failures do not provide developers with automated and broadly applicable solutions for finding the site's faulty HTML elements and CSS properties. To address this limitation, we propose a novel automated approach for debugging web sites that is based on image processing and probabilistic techniques. Our approach first builds a model that links observable changes in the web site's appearance to faulty elements and styling properties. Then using this model, our approach predicts the elements and styling properties most likely to cause the observed failure for the page under test and reports these to the developer. In evaluation, our approach was more accurate and faster than prior techniques for identifying faulty elements in a website.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {335}, 
	title = {Detecting and Localizing Internationalization Presentation Failures in Web Applications},
	abstract = {Abstract:
	Web applications can be easily made available to an international audience by leveraging frameworks and tools for automatic translation and localization. However, these automated changes can distort the appearance of web applications since it is challenging for developers to design their websites to accommodate the expansion and contraction of text after it is translated to another language. Existing web testing techniques do not support developers in checking for these types of problems and manually checking every page in every language can be a labor intensive and error prone task. To address this problem, we introduce an automated technique for detecting when a web page's appearance has been distorted due to internationalization efforts and identifying the HTML elements or text responsible for the observed problem. In evaluation, our approach was able to detect internationalization problems in a set of 54 web applications with high precision and recall and was able to accurately identify the underlying elements in the web pages that led to the observed problem.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {336}, 
	title = {A Study of Causes and Consequences of Client-Side JavaScript Bugs},
	abstract = {Abstract:
	Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, JavaScript is known to be error-prone. While prior studies have demonstrated the prevalence of JavaScript faults, no attempts have been made to determine their causes and consequences. The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. We perform an empirical study of 502 bug reports from 19 bug repositories. The bug reports are thoroughly examined to classify and extract information about each bug' cause (the error) and consequence (the failure and impact). Our results show that the majority (68 percent) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80 percent of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components. These results indicate that JavaScript programmers and testers need tools that can help them reason about the DOM. Additionally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript.
	},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {339}, 
	title = {Automated repair of layout cross browser issues using search-based techniques},
	abstract = {A consistent cross-browser user experience is crucial for the success of a website. Layout Cross Browser Issues (XBIs) can severely undermine a website�s success by causing web pages to render incorrectly in certain browsers, thereby negatively impacting users� impression of the quality and services that the web page delivers. Existing Cross Browser Testing (XBT) techniques can only detect XBIs in websites. Repairing them is, hitherto, a manual task that is labor intensive and requires significant expertise. Addressing this concern, our paper proposes a technique for automatically repairing layout XBIs in websites using guided search-based techniques. Our empirical evaluation showed that our approach was able to successfully fix 86% of layout XBIs reported for 15 different web pages studied, thereby improving their cross-browser consistency.
	},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {343}, 
	title = {Web??????????????????},
	abstract = {??????Web???????????,?????????????????,???????????Web?????????????????????????????????????,????????????????????????????,????????????????Web???????????????????,???????Web???????,?????????????????????????????????,??????,??????????????????????????????????,?????????????????,????????????????????????????????Web??????????
	},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {346}, 
	title = {Automatic Detection of Potential Layout Faults Following Changes to Responsive Web Pages (N)},
	abstract = {Abstract:
	Due to the exponential increase in the number ofmobile devices being used to access the World Wide Web, it iscrucial that Web sites are functional and user-friendly across awide range of Web-enabled devices. This necessity has resulted in the introduction of responsive Web design (RWD), which usescomplex cascading style sheets (CSS) to fluidly modify a Web site's appearance depending on the viewport width of the device in use. Although existing tools may support the testing of responsive Web sites, they are time consuming and error-prone to use because theyrequire manual screenshot inspection at specified viewport widths. Addressing these concerns, this paper presents a method thatcan automatically detect potential layout faults in responsively designed Web sites. To experimentally evaluate this approach, weimplemented it as a tool, called ReDeCheck, and applied itto 5 real-world web sites that vary in both their approach toresponsive design and their complexity. The experiments revealthat ReDeCheck finds 91% of the inserted layout faults.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {347}, 
	title = {Using Semantic Similarity in Crawling-Based Web Application Testing},
	abstract = {Abstract:
	To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, their broad use is limited by the required manual configurations for input value selection, GUI state comparison and clickable detection. In existing crawlers, the configurations are usually string-matching based rules looking for tags or attributes of DOM elements, and often application-specific. Moreover, in input topic identification, it can be difficult to determine which rule suggests a better match when several rules match an input field to more than one topic. This paper presents a natural-language approach based on semantic similarity to address the above issues. The proposed approach represents DOM elements as vectors in a vector space formed by the words used in the elements. The topics of encountered input fields during crawling can then be inferred by their similarities with ones in a labeled corpus. Semantic similarity can also be applied to suggest if a GUI state is newly discovered and a DOM element is clickable under an unsupervised learning paradigm. We evaluated the proposed approach in input topic identification with 100 real-world forms and GUI state comparison with real data from industry. Our evaluation shows that the proposed approach has comparable or better performance to the conventional techniques. Experiments in input topic identification also show that the accuracy of the rule-based approach can be improved by up to 22% when integrated with our approach.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {348}, 
	title = {A Review on Web Application Testing and its Current Research Directions},
	abstract = {Abstract
	Testing is an important part of every software development process on which companies devote considerable time and effort. The burgeoning web applications and their proliferating economic significance in the society made the area of web application testing an area of acute importance. The web applications generally tend to take faster and quicker release cycles making their testing very challenging. The main issues in testing are cost efficiency and bug detection efficiency. Coverage-based testing is the process of ensuring exercise of specific program elements. Coverage measurement helps determine the �thoroughness� of testing achieved. An avalanche of tools, techniques, frameworks came into existence to ascertain the quality of web applications. A comparative study of some of the prominent tools, techniques and models for web application testing is presented. This work highlights the current research directions of some of the web application testing techniques.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {349}, 
	title = {Using Semantic Similarity for Input Topic Identification in Crawling-based Web Application Testing},
	abstract = {To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, in existing crawlers, rules for identifying the topics of input text fields, such as login ids, passwords, emails, dates and phone numbers, have to be manually configured. Moreover, the rules for one application are very often not suitable for another. In addition, when several rules conflict and match an input text field to more than one topics, it can be difficult to determine which rule suggests a better match. This paper presents a natural-language approach to automatically identify the topics of encountered input fields during crawling by semantically comparing their similarities with the input fields in labeled corpus. In our evaluation with 100 real-world forms, the proposed approach demonstrated comparable performance to the rule-based one. Our experiments also show that the accuracy of the rule-based approach can be improved by up to 19% when integrated with our approach.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {351}, 
	title = {Visualization of automated test results obtained by the TESTAR tool},
	abstract = {Abstract. Bigger and more complex software systems demand quality
	practices that are seldom carried out in real industry. A common
	practice is to provide a post-release maintenance service of products to
	correct defects reported by the end user. In previous work we presented
	TESTAR, a technology-agnostic tool for automated testing of applications
	from their GUI. Here we introduce state-transition graph models
	derived from TESTAR test results as a tool for visualisation of what has
	been tested, to which extent and which software defects were found. We
	discuss how such models enable to perform quality assessment of software
	products by inspecting and debugging the system behaviour from
	the GUI perspective. This constitutes a step forward in aid of software
	developers and testers, since the User Interface is commonly the means
	end-users encounter potential software defects.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {352}, 
	title = {Understanding a prospective approach to designing malicious social bots},
	abstract = {The security implications of social bots are evident in consideration of the fact that data sharing and propagation functionality are well integrated with social media sites. Existing social bots primarily use Really Simple Syndication and OSN (online social network) application program interface to communicate with OSN servers. Researchers have profiled their behaviors well and have proposed various mechanisms to defend against them. We predict that a web test automation rootkit (WTAR) is a prospective approach for designing malicious social bots. In this paper, we first present the principles of designing WTAR-based social bots. Second, we implement three WTAR-based bot prototypes on Facebook, Twitter, and Weibo. Third, we validate this new threat by analyzing behaviors of the prototypes in a lab environment and on the Internet, and analyzing reports from widely-used antivirus software. Our analyses show that WTAR-based social bots have the following features: (i) they do not connect to OSN directly, and therefore produce few network flows; (ii) they can log in to OSNs easily and perform a variety of social activities; (iii) they can mimic the behaviors of a human user on an OSN. Finally, we propose several possible mechanisms in order to defend against WTAR-based social bots.
	},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {354}, 
	title = {Chapter Four - Recent Advances in Automatic Black-Box Testing},
	abstract = {Abstract
	
	Research in black-box testing has produced impressive results in the past 40 years, addressing many aspects of the problem that span from integration with the development process, to test case generation and execution. In the past few years, the research in this area has focused mostly on the automation of black-box approaches to improve applicability and scalability. This chapter surveys the recent advances in automatic black-box testing, covering contributions from 2010 to 2014, presenting the main research results and discussing the research trends.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {359}, 
	title = {Automated Testing of Event-Driven Applications},
	abstract = {Abstract
	Event-driven applications, such as, web applications and Android mobile ap-
	plications, may be tested by selecting an interesting input (i.e. a sequence of
	events), and deciding if a failure occurs when the selected input is applied to
	the event-driven application under test. Automated testing promises to re-
	duce the workload for developers by automatically selecting interesting inputs
	and detect failures. However, it is non-trivial to conduct automated testing
	of event-driven applications because of, for example, infinite input spaces and
	the absence of specifications of correct application behavior.
	In this PhD dissertation, we identify a number of specific challenges when
	conducting automated testing of event-driven applications, and we present
	novel techniques for solving these challenges.
	First, we present an algorithm for stateless model-checking of event-driven
	applications with partial-order reduction, and we show how this algorithm
	may be used to systematically test web applications for timing related fail-
	ures.  Next, we present an algorithm for generating inputs to event-driven
	applications in a targeted manner, combining existing techniques using UI
	models and concolic testing in a novel way. Finally, we show how server inter-
	face descriptions can be used to simplify the process of automated testing of
	web applications that depend on client-server communication, and we present
	a learning algorithm for inferring such server interface descriptions from con-
	crete observations.
	We implement tools for web applications and Android mobile applications
	using the above algorithms and techniques, and we experimentally evaluate
	the effectiveness of the proposed solutions on real-world applications. Based
	on our experiments, we conclude that our proposed solutions are useful when
	automatically testing event-driven applications, and that our proposed solu-
	tions pushes the state-of-the-art within this area.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {360}, 
	title = {A SURVEY OF BUSINESS INTILLIGENCE USING DATA MINING, WEB MINING AND TEXT MINING},
	abstract = {Information and technology has taken new changes in the field of communication. This new world is the world of digital with the large amount of data is available on the internet. Lot of peoples is accessing the social sites like facebook, government sites also having the data in huge variety. Banks now maintain all their data related to customer and employees on internet. We required finding new ways and technologies through which we can search the closer observations on this huge data and generate knowledge, i.e. why mining technologies comes in  to existence. Mining techniques  basically
	used
	some
	automated
	tools
	to
	achieve
	business
	intelligence.
	Using
	these
	tools,
	we
	will
	able
	In
	this
	paper,
	we
	will
	talk
	about
	mining
	techniques,
	their
	implementations
	and
	we
	will
	also
	talk
	about
	how
	these
	mining
	techniques
	will
	be
	useful
	to
	achieve
	Business
	Intelligence.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {361}, 
	title = {QVMMA: A Short Term and Long Term Layer 3 DDoS Detector and Mitigator},
	abstract = {QVMMA: A Short Term and Long Term Layer 3 DDoS Detector and Mitigator
	{tag}                                                                           {/tag}  
	IJCA Proceedings on International Conference
	on Communication, Computing and Virtualization        
	� 2016 by IJCA Journal        
	ICCCV 2016 - Number 1        
	Year of Publication: 2016        
	Authors:                              
	Sonia Laskar
	Dhirendra Mishra
	{bibtex}icccv20163.bib{/bibtex} 
	Abstract          
	Distributed Denial of Service (DDoS) attacks continue to harm servers using intense wars
	against popular ecommerce and content websites.  The short term and long term types of
	popular DDoS attacks can be detected, prevented and mitigated using the proposed novel
	Qualified Vector Match and Merge Algorithm (QVMMA) in real time.  14 feature components are
	used to generate an attack signature in real time and stored in dynamically updated DDoS
	Captured Attack Pattern (DCAP)30database.  It is effective in detecting new and old attacks. 
	Persistent DDoS attacks cause financial damage or reputation loss by loss of the
	company&apos;s valuable clients.  The server&apos;s availability is heavily compromised. 
	1 / 5
	QVMMA: A Short Term and Long Term Layer 3 DDoS Detector and Mitigator
	Popular websites Github and BBC UK faced DDoS attacks in 2015.  Long term DDoS attack
	directed on Github continued for over 118 hours34,35.  Short term DDoS attack experienced by
	BBC36 website caused its patchy response.  The main crux of the problem is the absence of a
	way to differentiate between attack records and legitimate records while the attack is occurring
	in real time.  Several methods1-31,37-42,43 are listed in brief in the paper.  Post mortem
	solutions are not applicable in real time.  Available real time solutions are slow.  QVMMA is an
	ideal faster real time solution to prevent DDoS attacks using Statistical Feature Vector
	Generation.  Matlab is used for DDoS real time simulation where the topologies (bus, star,
	abilene network) are created using OMNET++33.  QVMMA generates and uses Statistical
	Feature Vector for Attack Signature Generation, Matching and Identification only for qualifier
	satisfied records.  The web server&apos;s log files used as input to QVMMA are according to
	W3C log format standard34.  Experimentation is completed with exhaustive 336 cases.  Four
	networks are tested with 5, 8, 10, 13 nodes.  Performance evaluation of QVMMA concludes
	EER is 11. 8% when threshold is 1. 6.  Using model of FAR and FAR, the trendline provides
	threshold at 1 with EER at 10%.  Abilene network achieves best result.  As the number of
	attackers, nodes and intermediate routers increase, detection time increases.  If threshold is
	increased, the accuracy reduces.  If the number of nodes increases, accuracy increases.  Thus
	it is concluded that QVMMA can be used for effective layer 3 DDoS Prevention and Mitigation in
	real time based on results generated in Matlab simulation.  Extended results are provided.  A
	model is provided in this paper to predict the detection time for any number of attackers.  Other
	models are provided based on data collected through experimentation to formulate a relation
	between detection time, accuracy, Actual Attack Traffic Passed Rate (A_ATPR) with respect to
	the number of attackers.  The corresponding correlation coefficient and regression coefficient
	are calculated to identify and conclude the strong relationships.  This paper focuses on results
	and discussion on studying the effects and trend observed based on increasing the number of
	attackers during a DDoS attack.   Thus QVMMA is fast enough to be used in real time to detect
	and mitigate short term or long term layer 3 Denial of Service(DoS) and more complex DDoS
	attacks. },
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {362}, 
	title = {00 A Survey of Dynamic Analysis and Test Generation for JavaScript},
	abstract = {JavaScript has become one of the most prevalent programming languages. Unfortunately, some of the unique
	properties that contribute to this popularity also make JavaScript programs prone to errors and difficult for
	program analyses to reason about. These properties include the highly dynamic nature of the language, a set
	of unusual language features, a lack of encapsulation mechanisms, and the �no crash� philosophy. This paper
	surveys dynamic program analysis and test generation techniques for JavaScript targeted at improving the
	correctness, reliability, performance, security, and privacy of JavaScript-based software.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {363}, 
	title = {Metamorphic Testing for Cybersecurity},
	abstract = { Abstract:
	Metamorphic testing (MT) can enhance security testing by providing an alternative to using a test oracle, which is often unavailable or impractical. The authors report how MT detected previously unknown bugs in real-world critical applications such as code obfuscators, giving evidence that software testing requires diverse perspectives to achieve greater cybersecurity.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {365}, 
	title = {Automatizaci�n de pruebas de compatibilidad web en un entorno de desarrollo continuo de software},
	abstract = {
	Resumo
	
	El desarrollo de software continuo, es un enfoque en el cual los equipos mantienen la producci�n de software en ciclos cortos de tiempo, asegurando que el producto pueda ser lanzado de manera fiable en cualquier momento. Hoy en d�a, este enfoque est� siendo cada vez m�s utilizado en las organizaciones, especialmente en las que desarrollan aplicaciones en entorno web o m�vil. Sin embargo, al lanzar versiones del producto con mayor frecuencia, emergen m�s defectos en el mismo. Esto se debe, principalmente, a que el tiempo para realizar los ciclos de pruebas son muy cortos. Uno de los desaf�os que existe actualmente es la aceleraci�n de las pruebas sobre la interfaz de usuario, entre ellas las de compatibilidad web. En este trabajo se presenta una t�cnica utilizada en una gran empresa de desarrollo de software, para automatizar las pruebas de compatibilidad de web, mediante la automatizaci�n de comparaci�n de im�genes al hacer pruebas cruzadas entre distintos navegadores. Los resultados indican que la t�cnica propuesta se adapta a los requerimientos de los procesos de desarrollo continuo, aumentando el rendimiento y velocidad de este tipo de pruebas.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {366}, 
	title = {Comparison of Different Techniques of Web GUI-based Testing with the Representative Tools Selenium and EyeSel},
	abstract = {Abstract [en]
	
	Context. Software testing is becoming more and more important in software development life-cycle especially for web testing. Selenium is one of the most widely used property-based Graph-User-Interface(GUI) web testing tools. Nevertheless, it also has some limitations. For instance, Selenium cannot test the web components in some specific plugins or HTML5 videos frame. But it is important for testers to verify the functionality of plugins or videos on the websites. Recently, the theory of the image recognition-based GUI testing is introduced which can locate and interact with the components to be tested on the websites by image recognition. There are only a few papers do research on comparing property-based GUI web testing and image recognition-based GUI testing. Hence, we formulated our research objectives based on this main gap.
	
	Objectives. We want to compare these two different techniques with EyeSel which is the tool represents the image recognition-based GUI testing and Selenium which is the tool represents the property-based GUI testing. We will evaluate and compare the strengths and drawbacks of these two tools by formulating specific JUnit testing scripts. Besides, we will analyze the comparative result and then evaluate if EyeSel can solve some of the limitations associated with Selenium. Therefore, we can conclude the benefits and drawbacks of property-based GUI web testing and image recognition-based GUI testing.  
	
	Methods. We conduct an experiment to develop test cases based on websites� components both by Selenium and EyeSel. The experiment is conducted in an educational environment and we select 50 diverse websites as the subjects of the experiment. The test scripts are written in JAVA and ran by Eclipse.  The experiment data is collected for comparing and analyzing these two tools.
	
	Results. We use quantitative analysis and qualitative analysis to analyze our results. First of all, we use quantitative analysis to evaluate the effectiveness and efficiency of two GUI web testing tools. The effectiveness is measured by the number of components that can be tested by these two tools while the efficiency is measured by the measurements of test cases� development time and execution time. The results are as follows (1) EyeSel can test more number of components in web testing than Selenium (2) Testers need more time to develop test cases by Selenium than by EyeSel (3) Selenium executes the test cases faster than EyeSel. (4) �Results (1)� indicates the effectiveness of EyeSel is better than Selenium while �Results (2)(3)� indicate the efficiency of EyeSel is better than Selenium. Secondly, we use qualitative analysis to evaluate four quality characteristics (learnability, robustness, portability, functionality) of two GUI web testing tools. The results show that portability and functionality of Selenium are better than EyeSel while the learnability of EyeSel is better than Selenium. And both of them have good robustness in web testing.
	
	Conclusions. After analyzing the results of comparison between Selenium and EyeSel, we conclude that (1) Image recognition-based GUI testing is more effectiveness than property-based GUI web testing (2) Image recognition-based GUI testing is more efficiency than property-based GUI web testing (3) The portability and functionality of property-based GUI web testing is better than Image recognition-based GUI testing (4) The learnability of image recognition-based GUI testing is better than property-based GUI web testing. (5) Both of them are good at different aspects of robustness},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {367}, 
	title = {VISOR: A fast image processing pipeline with scaling and translation invariance for test oracle automation of visual output systems},
	abstract = {Abstract
	
	Test oracles differentiate between the correct and incorrect system behavior. Hence, test oracle automation is essential to achieve overall test automation. Otherwise, testers have to manually check the system behavior for all test cases. A common test oracle automation approach for testing systems with visual output is based on exact matching between a snapshot of the observed output and a previously taken reference image. However, images can be subject to scaling and translation variations. These variations lead to a high number of false positives, where an error is reported due to a mismatch between the compared images although an error does not exist. To address this problem, we introduce an automated test oracle, named VISOR, that employs a fast image processing pipeline. This pipeline includes a series of image filters that align the compared images and remove noise to eliminate differences caused by scaling and translation. We evaluated our approach in the context of an industrial case study for regression testing of Digital TVs. Results show that VISOR can avoid 90% of false positive cases after training the system for 4 h. Following this one-time training, VISOR can compare thousands of image pairs within seconds on a laptop computer.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {675}, 
	title = {Web Application Model Generation through Reverse Engineering and UI Pattern Inferring  Sign In or Purchase},
	abstract = {Abstract:
	A great deal of effort in model-based testing is related to the creation of the model. In addition, the model itself, while a powerful tool of abstraction, can have conceptual errors, introduced by the tester. These problems can be reduced by generating those models automatically. This paper presents a dynamic reverse engineering approach that aims to extract part of the model of an existing web application through the identification of User Interface (UI) patterns. This reverse engineering approach explores automatically any web application, records information related to the interaction, analyses the gathered information, tokenizes it, and infers the existing UI patterns via syntactical analysing. After being complemented with additional information and validated, the model extracted is the input for the Pattern-Based Graphical User Interface Testing (PBGT) approach for testing existing web application under analysis.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {677}, 
	title = {Search-based security testing of web applications},
	abstract = {SQL injections are still the most exploited web application vulnerabilities. We present a technique to automatically detect such vulnerabilities through targeted test generation. Our approach uses search-based testing to systematically evolve inputs to maximize their potential to expose vulnerabilities. Starting from an entry URL, our BIOFUZZ prototype systematically crawls a web application and generates inputs whose effects on the SQL interaction are assessed at the interface between Web server and database. By evolving those inputs whose resulting SQL interactions show best potential, BIOFUZZ exposes vulnerabilities on real-world Web applications within minutes. As a black-box approach, BIOFUZZ requires neither analysis nor instrumentation of server code; however, it even outperforms state-of-the-art white-box vulnerability scanners.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {679}, 
	title = {Testing Web Applications Through Layout Constraints},
	abstract = {Abstract:
	The paper focuses on bugs in web applications that can be detected by analyzing the contents and layout of page elements inside a browser's window. Based on an empirical analysis of 35 real-world web sites and applications (such as Facebook, Dropbox, and Moodle), it provides a survey and classification of more than 90 instances of layout-based bugs. It then introduces Cornipickle, an automated testing tool that provides a declarative language to express desirable properties of a web application as a set of human-readable assertions on the page's HTML and CSS data. Such properties can be verified on-the-fly as a user interacts with an application.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {680}, 
	title = { Automated exploration and analysis of ajax web applications with WebMole},
	abstract = {WebMole is a browser-based tool that automatically and exhaustively explores all pages inside a web application. Contrarily to classical web crawlers, which only explore pages accessible through regular anchors, WebMole can find its way through Ajax applications that use JavaScript-triggered links, and handles state changes that do not involve a page reload. User-defined functions called oracles can be used to bound the range of pages explored by WebMole to specific parts of an application, as well as to evaluate Boolean test conditions on all visited pages. Overall, WebMole can prove a more flexible alternative to automated testing suites such as Selenium WebDriver.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {681}, 
	title = {Exhaustive Exploration of Ajax Web Applications with Selective Jumping},
	abstract = {Abstract:
	Exploring modern web applications is a difficult task with the presence of client-side JavaScript code, as a crawler cannot jump or backtrack arbitrarily inside applications that maintain a state. In this paper, we present Web Mole, an automated crawler that implements a formal framework for web exploration that generalizes existing approaches. Web Mole uses an algorithm that explores an application without the need for arbitrary backtracking, it intercepts HTTP requests called from client-side code, and uses that information to perform selectively jump to pages while preserving the client-server state relationship. Comparisons with existing crawlers on various classes of graphs show that this strategy incurs a lower exploration cost.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {682}, 
	title = {A Reference Framework for the Automated Exploration of Web Applications},
	abstract = {Abstract:
	Web crawling is the process of exhaustively exploring the contents of a web site or application through automated means. While the results of such a crawling can be put through numerous uses ranging from a simple backup to comprehensive testing and analysis, features of modern-day applications prevent crawlers from properly exploring applications. We provide an in-depth analysis of 15 such features, and report on their presence in a study of 16 real-world web sites. Based on that study, we develop a configurable web application where the presence of each such feature can be turned on or off, aimed as a test bench where existing crawlers can be compared in a uniform way. Our results, which are the first exhaustive comparison of available crawlers, indicates areas where future work should be aimed.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {683}, 
	title = {Co-authorship Networks in Additive Manufacturing Studies Based on Social Network Analysis},
	abstract = {In the last 30 years there has been growing interest in additive manufacturing technology. Coauthorship
	of published papers plays such an important role in scientific development. The
	collaborative research social network on the field of additive manufacturing is paid particular
	attention in this paper. A framework based on open source software is proposed to automate the
	data collection and social network analysis. The characteristics of the overall network structure is
	analyzed and visualized. The development process of AM collaboration community mode is
	discussed. So as to provide inspiration for collaborative research and further development of
	additive manufacturing technology. 
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {684}, 
	title = {ALEX: Mixed-Mode Learning of Web Applications at Ease},
	abstract = {In this paper, we present ALEX, a web application that enables non-programmers to fully automatically infer models of web applications via active automata learning. It guides the user in setting up dedicated learning scenarios, and invites her to experiment with the available options in order to infer models at adequate levels of abstraction. In the course of this process, characteristics that go beyond a mere �site map� can be revealed, such as hidden states that are often either specifically designed or indicate errors in the application logic. Characteristic for ALEX is its support for mixed-mode learning: REST and web services can be executed simultaneously in one learning experiment, which is ideal when trying to compare back-end and front-end functionality of a web application. ALEX has been evaluated in a comparative study with 140 undergraduate students, which impressively highlighted its potential to make formal methods like active automata learning more accessible to a non-expert crowd.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {686}, 
	title = {Extraction of User Behavior Profiles for Software Modernization},
	abstract = {In the field of software modernization static and dynamic analysis of the system is a core
	part of the modernization process. The static analysis focuses on the extraction of structural
	and architectural information by searching the source code for dependencies. The dynamic
	analysis tracks method calls at runtime and enables statements about the real usage of
	the software in a productive environment. Often the monitoring is executed from the
	system�s point of view and not from the user behavior perspective focusing on architectural
	information. The idea of user behavior profile extraction was picked up by research groups
	dealing with automatic generation of test cases. Based on user sessions, a user behavior
	model is created. The derived test cases are augmented with workload characteristics for
	simulating realistic load during the test execution.
	This thesis we combine the concept of user behavior profiles and software modernization.
	We introduce a session extraction tool and a behavior model extraction tool based on the
	TeeTime framework. The Pipe-and-Filter architecture provides a good performance and
	reusability. The session extraction tool processes custom records created with the Kieker
	framework in order to create log files of user sessions. The session log files are analyzed
	with the behavior model extraction tool. It supports several visualizations, calculates think
	time statistics and is highly customizable for analyzing single processes.
	We instrument the b+m bAV-Manager, an session-based, workflow-oriented administration
	software for customer and calculation data of insurers, developed by the b+m
	Informatik AG. The implementation of instrumentation components is realized with interceptors
	of the Spring framework. In an experiment we monitor the web application and
	record the user behavior with the session extraction tool. Based on the session logs we
	analyze the screenflow and the workflow with the behavior extraction tool. The results
	allow us, to make quantitative and qualitative statements about the user behavior in comparison
	with the application model. We derive several suggestions regarding the imminent
	modernization of the b+m bAV-Manager, that improve the matching of defined screen- and
	workflows and the extracted user behavior profiles.
	},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {689}, 
	title = {Chapter Four - Advances in Web Application Testing, 2010�2014},
	abstract = {As web applications increase in popularity, complexity, and size, approaches and tools to automate testing the correctness of web applications must continually evolve. In this chapter, we provide a broad background on web applications and the challenges in testing these distributed, dynamic applications made up of heterogeneous components. We then focus on the recent advances in web application testing that were published between 2010 and 2014, including work on test-case generation, oracles, testing evaluation, and regression testing. Through this targeted survey, we identify trends in web application testing and open problems that still need to be addressed.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {690}, 
	title = {Automatic Test Case Generation for Modern Web Applications Using Population-Based Automatic Fuzzy Neural Network},
	abstract = {Abstract�Automatic test case generation is an approach to decrease cost and time in software testing. Although there
	have been lots of proposed methods for automatic test case generation of web applications, there still exists some
	challenges which needs more researches. The most important problem in this area is the lack of a complete descriptive
	model which indicates the whole behaviors of web application as guidance for the generation of test cases with high
	software coverage. In this paper, test cases are generated automatically to test web applications using a machine
	learning method. The proposed method called RTCGW (Rule-based Test Case Generator for Web Applications)
	generates test cases based on a set of fuzzy rules that try to indicate the whole software behaviors to reach to a high
	level of software coverage. For this purpose a novel machine learning approach based on fuzzy neural networks is
	proposed to extract fuzzy rules from a set of data and then used to generate a set of fuzzy rules representing software
	behaviors. The fuzzy rule set is then used to generate software test cases and the generated test cases are optimized
	using an optimization algorithm based on combination of genetic and simulated annealing algorithms. Two
	benchmark problems are tested using the optimized test cases. The results show a high level of coverage and
	performance for the proposed method in comparison with other methods. },
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {692}, 
	title = {Composing and Delivering Heterogeneous Web Testing Software as a Composite Web Testing Service},
	abstract = {Abstract:
	Load testing and cross-browser testing are ones of the web testing types particularly reliant on the support of cloud computing platforms for the implementation of TaaS. The main challenge involved in the composition of heterogeneous web application testing tools is the incompatibility of their inputs and outputs. However, the need to manually configure the tools greatly undermines the convenience and applicability of their applications. This paper proposes a system for the composition and delivery of heterogeneous web testing tools with the following contributions: (1) four adapters to automatically bridge the gap between the inputs and outputs of six state-of-the-art testing tools, (2) a composite web testing service with the adapters, and (3) two adapters to enable delivering the composite service via emails. Experiment results demonstrate the effectiveness of the proposed system in reducing the effort required for load testing and cross-browser testing by comparison with a conventional method.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {1091}, 
	title = {Using Multi-Locators to Increase the Robustness of Web Test Cases},
	abstract = {Abstract:
	The main reason for the fragility of web test cases is the inability of web element locators to work correctly when the web page DOM evolves. Web elements locators are used in web test cases to identify all the GUI objects to operate upon and eventually to retrieve web page content that is compared against some oracle in order to decide whether the test case has passed or not. Hence, web element locators play an extremely important role in web testing and when a web element locator gets broken developers have to spend substantial time and effort to repair it. While algorithms exist to produce robust web element locators to be used in web test scripts, no algorithm is perfect and different algorithms are exposed to different fragilities when the software evolves. Based on such observation, we propose a new type of locator, named multi-locator, which selects the best locator among a candidate set of locators produced by different algorithms. Such selection is based on a voting procedure that assigns different voting weights to different locator generation algorithms. Experimental results obtained on six web applications, for which a subsequent release was available, show that the multi-locator is more robust than the single locators (about -30% of broken locators w.r.t. the most robust kind of single locator) and that the execution overhead required by the multiple queries done with different locators is negligible (2-3% at most).},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1092}, 
	title = {Why creating web page objects manually if it can be done automatically?},
	abstract = {Page Object is a design pattern aimed at making web test scripts more readable, robust and maintainable. The effort to manually create the page objects needed for a web application may be substantial and unfortunately existing tools do not help web developers in such task.
	
	In this paper we present Apogen, a tool for the automatic generation of page objects for web applications. Our tool automatically derives a testing model by reverse engineering the target web application and uses a combination of dynamic and static analysis to generate Java page objects for the popular Selenium WebDriver framework. Our preliminary evaluation shows that it is possible to use around 3/4 of the automatic page object methods as they are, while the remaining 1/4 need only minor modifications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1093}, 
	title = {Chapter Five - Approaches and Tools for Automated End-to-End Web Testing},
	abstract = {Abstract
	The importance of test automation in web engineering comes from the widespread use of web applications and the associated demand for code quality. Test automation is considered crucial for delivering the quality levels expected by users, since it can save a lot of time in testing and it helps developers to release web applications with fewer defects. The main advantage of test automation comes from fast, unattended execution of a set of tests after some changes have been made to a web application. Moreover, modern web applications adopt a multitier architecture where the implementation is scattered across different layers and run on different machines. For this reason, end-to-end testing techniques are required to test the overall behavior of web applications.
	
	In the last years, several approaches have been proposed for automated end-to-end web testing and the choice among them depends on a number of factors, including the tools used for web testing and the costs associated with their adoption. They can be classified using two main criteria: the first concerns how test cases are developed (ie, Capture-Replay and Programmable approaches), while, the second concerns how test cases localize the web elements to interact with (ie, Coordinates-based, DOM-based, and Visual approaches), that is what kind of locators are used for selecting the target GUI components.
	For developers and project managers it is not easy to select the most suitable automated end-to-end web testing approach for their needs among the existing ones. This chapter provides a comprehensive overview of the automated end-to-end web testing approaches and summarizes the findings of a long term research project aimed at empirically investigating their strengths and weaknesses.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1094}, 
	title = {Robula+: an algorithm for generating robust XPath locators for web testing},
	abstract = {Abstract
	
	Automated test scripts are used with success in many web development projects, so as to automatically verify key functionalities of the web application under test, reveal possible regressions and run a large number of tests in short time. However, the adoption of automated web testing brings advantages but also novel problems, among which the test code fragility problem. During the evolution of the web application, existing test code may easily break and testers have to correct it. In the context of automated DOM-based web testing, one of the major costs for evolving the test code is the manual effort necessary to repair broken web page element locators � lines of source code identifying the web elements (e.g. form fields and buttons) to interact with.
	
	In this work, we present Robula+, a novel algorithm able to generate robust XPath-based locators � locators that are likely to work correctly on new releases of the web application. We compared Robula+ with several state of the practice/art XPath locator generator tools/algorithms. Results show that XPath locators produced by Robula+ are by far the most robust. Indeed, Robula+ reduces the locators' fragility on average by 90% w.r.t. absolute locators and by 63% w.r.t. Selenium IDE locators. },
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1095}, 
	title = {Why do Record/Replay Tests of Web Applications Break?},
	abstract = {Abstract:
	Software engineers often use record/replay tools to enable the automated testing of web applications. Tests created in this manner can then be used to regression test new versions of the web applications as they evolve. Web application tests recorded by record/replay tools, however, can be quite brittle, they can easily break as applications change. For this reason, researchers have begun to seek approaches for automatically repairing record/replay tests. To date, however, there have been no comprehensive attempts to characterize the causes of breakagesin record/replay tests for web applications. In this work, wepresent a taxonomy classifying the ways in which record/replay tests for web applications break, based on an analysis of 453 versions of popular web applications for which 1065 individual test breakages were recognized. The resulting taxonomy can help direct researchers in their attempts to repair such tests. It can also help practitioners by suggesting best practices when creating tests or modifying programs, and can help researchers with other tasks such as test robustness analysis and IDE design.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1098}, 
	title = {Ringer: web automation by demonstration},
	abstract = {With increasing amounts of data available on the web and a diverse range of users interested in programmatically accessing that data, web automation must become easier. Automation helps users complete many tedious interactions, such as scraping data, completing forms, or transferring data between websites. However, writing web automation scripts typically requires an expert programmer because the writer must be able to reverse engineer the target webpage. We have built a record and replay tool, Ringer, that makes web automation accessible to non-coders. Ringer takes a user demonstration as input and creates a script that interacts with the page as a user would. This approach makes Ringer scripts more robust to webpage changes because user-facing interfaces remain relatively stable compared to the underlying webpage implementations. We evaluated our approach on benchmarks recorded on real webpages and found that it replayed 4x more benchmarks than a state-of-the-art replay tool.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1100}, 
	title = {WATERFALL: an incremental approach for repairing record-replay tests of web applications},
	abstract = {Software engineers use record/replay tools to capture use case scenarios that can serve as regression tests for web applications. Such tests, however, can be brittle in the face of code changes. Thus, researchers have sought automated approaches for repairing broken record/replay tests. To date, such approaches have operated by directly analyzing differences between the releases of web applications. Often, however, intermediate versions or commits exist between releases, and these represent finer-grained sequences of changes by which new releases evolve. In this paper, we present WATERFALL, an incremental test repair approach that applies test repair techniques iteratively across a sequence of fine-grained versions of a web application. The results of an empirical study on seven web applications show that our approach is substantially more effective than a coarse-grained approach (209% overall), while maintaining an acceptable level of overhead.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1101}, 
	title = {End-User Record and Replay for the Web},
	abstract = {The usefulness of today�s websites is limited by their form and ease of access. Even though
	the web contains an ever-expanding wealth of information, much of it exists in a form that is
	not directly useful. How can end-users access the web in a way that meets their needs?
	We present record and replay (R+R) as a way to bridge the gap between a website�s functionality
	and the end-user�s goal. R+R leverages an interface the user knows and is stable �
	that is, the webpage � in order to automate repetitive tasks. A R+R system observes a user
	interacting with a website and produces a script which, when executed, repeats the original
	interaction. End-users can use R+R to automate a sequence of actions and programmers can
	use these recordings as an API to execute more complicated tasks. Unfortunately, as websites
	become more complex, R+R becomes increasingly difficult.
	The challenge with modern websites is that a single demonstration of the interaction has
	limited information, making scripts fragile to changes in the website. For past R+R systems,
	this was less of an issue because of the static nature of websites. But as the web becomes more
	dynamic, it becomes difficult to produce a robust script that mimics the interactivity of the user
	and can adapt to changes on the page.
	To solve this problem, we developed Ringer, a R+R system for the web. Ringer is built on
	three key abstractions � actions, triggers, and elements. Ringer takes a user demonstration
	as input and synthesize a script that interacts with the page as a user would. To make Ringer
	scripts robust, we develop novel methods for web R+R. In particular, Ringer uses the following
	features:
	� Inferring triggers automatically which synchronize the script with the state of the webpage
	� Monitoring the replay execution to ensure actions faithfully mimic the user
	� Identifying elements on the replay-time page using a similarity metric
	To evaluate our work, we run Ringer on a suite of real-world benchmarks by replaying
	interactions on Alexa-ranked websites. We compare Ringer against a current state-of-the-art
	replay tool and find that Ringer is able to replay all 29 benchmark interactions, compared
	to only 5 benchmarks for the previous approach. Additionally, our benchmarks show that a
	replayer needs to synchronize with the state of a webpage in order to replay correctly, motivating
	Ringer�s use of triggers. We show that our trigger inference algorithm can synthesize sufficient
	synchronization, while also having the added benefit of speeding up the replay execution.
	Finally, we show that R+R is useful as a building block for end-user applications by building
	two such tools using Ringer. One allows end-users to scrape structured data from a website
	simply through demonstration. The other allows end-users to aggregate real-time data from
	various websites in the form of live tiles, by specifying the data they want on a website through
	demonstration.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1102}, 
	title = {Testing Web Applications: A Survey},
	abstract = {Web applications are widely used. The massive use of web
	applications imposes the need for testing them. Testing web
	applications is a challenging process given that it needs to
	account for the dynamic, asynchronous and interactive nature
	of web applications. Various strategies exist for testing
	web applications such as capture-replay and programmable
	web testing. However, test suites created in this manner
	are brittle and easily break when changes are applied to the
	web application under test. Furthermore, web applications
	continuously evolve and new versions of web applications are
	constantly released in order to fix bugs, respond to changing
	requirements, modify layouts, etc. The continuous evolution
	of web applications might lead to test suite obsoleteness. In
	this scenario, the test suite that was created for the first
	version of the web application would become outdated and
	would require repair. In this paper, we present a survey relative
	to testing web applications. We selected eight papers
	that discuss topics related to testing web applications. The
	topics that are discussed in this paper are: Test repair, test
	breakage prevention, test maintenance, capture-replay testing
	versus programmable web testing and faults within web
	applications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1103}, 
	title = {PerfGuard: binary-centric application performance monitoring in production environments},
	abstract = {Diagnosis of performance problems is an essential part of software development and maintenance. This is in particular a challenging problem to be solved in the production environment where only program binaries are available with limited or zero knowledge of the source code. This problem is compounded by the integration with a significant number of third-party software in most large-scale applications. Existing approaches either require source code to embed manually constructed logic to identify performance problems or support a limited scope of applications with prior manual analysis. This paper proposes an automated approach to analyze application binaries and instrument the binary code transparently to inject and apply performance assertions on application transactions. Our evaluation with a set of large-scale application binaries without access to source code discovered 10 publicly known real world performance bugs automatically and shows that PerfGuard introduces very low overhead (less than 3% on Apache and MySQL server) to production systems.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1104}, 
	title = {Accelerating Test Automation through a Domain Specific Language},
	abstract = {Abstract:
	Test automation involves the automatic execution of test scripts instead of being manually run. This significantly reduces the amount of manual effort needed and thus is of great interest to the software testing industry. There are two key problems in the existing tools & methods for test automation - a) Creating an automation test script is essentially a code development task, which most testers are not trained on, and b) the automation test script is seldom readable, making the task of maintenance an effort intensive process. We present the Accelerating Test Automation Platform (ATAP) which is aimed at making test automation accessible to non-programmers. ATAP allows the creation of an automation test script through a domain specific language based on English. The English-like test scripts are automatically converted to machine executable code using Selenium WebDriver. ATAP's English-like test script makes it easy for non-programmers to author. The functional flow of an ATAP script is easy to understand as well thus making maintenance simpler (you can understand the flow of the test script when you revisit it many months later). ATAP has been built around the Eclipse ecosystem and has been used in a real-life testing project. We present the details of the implementation of ATAP and the results from its usage in practice.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1106}, 
	title = {Processing automation scripts of software},
	abstract = {Processing automation scripts used for testing pages includes running the automation scripts using a processor, searching for an element on the page according to locating information in an instruction of the automation scripts, collecting element-related information of the element in response to finding of the element on the page according to the locating information, and associating the collected element-related information of the element with the instruction of the automation scripts. The element-related information associated with the instruction is saved.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1109}, 
	title = {Regression testing of web applications using Record/Replay tools},
	abstract = {Software engineers often use record/replay tools to enable the automated testing of web applications. Tests created in this man- ner can then be used to regression test new versions of the web applications as they evolve. Web application tests recorded by record/replay tools, however, can be quite brittle; they can easily break as applications change. For this reason, researchers have be- gun to seek approaches for automatically repairing record/replay tests. This research investigates different aspects in relation to test- ing web applications using record/replay tools. The areas that we are interested in include taxonomizing the causes behind breakages and developing automated techniques to repair breakages, creating prevention techniques to stop the occurrence of breakages and de- veloping automated frameworks for root cause analysis. Finally, we intend to evaluate all of these activities via controlled studies involving software engineers and real web application tests.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1110}, 
	title = {Comparative Study of Cross Browser Compatibility as Design Issue in Various Websites},
	abstract = {Abstract  -  In this current era of information technology websites are very important means of communication. Lot of efforts is required by different institutions / organizations to portray complete information on beautifully designed websites.  Websites act as an online agent through which a user can get his work done without physically visiting the organizations.  Website design is given with a very critical look by the designer so that it can provide users with all the facilities of the concerned institutions / organizations online. To make websites behavior similar in all the different browsers employed by the different categories of the users, the responsibility of the designer and the concerned institutions / organizations increases manifold. In this research paper author developed an online tool using .NET Framework using C# to study cross browser compatibility as Design issue in various categories of the websites like Job portals, Government, educational,  Commercial and Social networking. The automated tool developed by author function on the basis of the different standards prescribed in W3C guidelines document UAAG 2.0 [7] and act like a parser and renders the complete code of the website and produces result on basis of the behavior of the websites in five most popular and widely used Browsers like parameters like Internet Explorer[7,8,9], Chrome, Safari, Fire fox. Each Browser is tested on the basis of the five parameters which are included in the parser are Blinking, Active X control, Website Resolution; image Formats, HTML Tag errors. The results  obtained after testing five different categories of websites shows that educational and social networking sites shows least compatibility in multiple browsers where as job portals, commercial and government websites shows 100% compliance to the website design standards recommended by W3C w.r.t browser compatibility of different websites on different browsing platform.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1111}, 
	title = {Issues on Developing a Location Aware Game for Mobile Browsers},
	abstract = {Despite of recent development of mobile browsers and web technologies most location based games intended to be played with a phone are delivered as installable programs. This is unfortunate as in many cases applications which can be used directly from a web page without installation are much more accessible to users. In this paper we present common problematics building location based applications using only web technologies. The problematics are explored via building an example application and presented together with solution options.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1112}, 
	title = {Virtuaalinen tietokanta sovellusalustana},
	abstract = {Abstract:
	
	T�ss� ty�ss� tarkastellaan Movenium Oy:n toteuttamaa virtuaalista tietokantaratkaisua nimelt� Collector. Ty�n alussa esitell��n tarvittava teoria tietokannoista ja SaaS-palveluista. Kirjallisuusosuudessa verrataan virtuaalista tietokantaratkaisua eri tietokantoihin, kuten perinteiseen relaatiomalliin sek� NoSql-tietokantoihin.
	
	
	Tutkimuksen aiheena on Collectorin kuormankestokapasiteetti ja sen riitt�vyys Movenium Oy:n tarpeisiin tulevaisuudessa. Tutkimus on toteutettu ohjelmalla, joka kasvattaa tietokantaa satunnaisdatalla ja suorittaa hakuja. Hakujen suoritusaikojen perusteella voidaan p��tell� tietokantaratkaisun kuormankest�vyys.
	This bachelor�s thesis is about virtual database solution called Collector. First section describes the needed theory about databases and SaaS. After theory part, Collector is being evaluated against different kind of database solutions. Both relational databases and NoSql-databases are covered.
	
	
	Main focus of the research is to review Collector's performance and capacity. This is done by a script which writes data to database and executes search queries to determine how long does the searches take.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1113}, 
	title = {Web Interface for natural language processing engines},
	abstract = {This thesis presents a graphical user interface that simplifies the use of natural language processing engines. Until now the use of the implemented part-of-speech task and other natural language processing tasks were possible only through command line interface. This restriction of the current interface was the reason, because of which the users were required to have both programming and scripting knowledge and experience. In addition to that, the preparation of the input data, used to feed the engine, and output data from the tasks had to be handled manually.
	The problem was solved by developing a web-based application and integrating OpenNLP engine and its part-of-speech tagging task. Hence, the chosen solution provides portability and accessibility from various locations. The user interface simplifies the working process for users with little or no technical knowledge and experience. Moreover, the application facilitates and guides the users through the task�s process flow. It also allows them to automatically preprocess their data to a format required as an input for the engine. After that, the users can simply follow the stages for the rest of the task and use the engine. Moreover, the application saves the data at the end of every stage of the task, which does not enroll the user to execute the whole task at once. Also the data used as an input and output from every stage of the task is stored automatically on the server, which provides reusability. And last, the application has modular structure, which provides the ability to extend the amount of tasks and engines according to the needs of the users. 
	The application provides simple interface, automated process and file handling. However the speed and accessibility of the application depends on the connection and the load on the server. The current software can be expanded with additional engines and tasks, but only if they support the current file and system structure.
	In the future some parts of the user interface and the back-end structure could be improved, as well as some more complexed tasks and different engines could be implemented.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1114}, 
	title = {System for computationally quantifying similarities between images},
	abstract = {Systems and methods provide for quantifying the similarity between images that appear similar. Given a set of images, one image is selected as the base image, to which the remaining images are compared. One or more portions of the base image are selected for comparison and the color composition of these areas is calculated. Then, the color compositions are examined to quantify the similarity or difference between the images, which is assigned a score reflective of the quantitative similarity or difference. The results are displayed. These systems and methods allow, e.g, a website owner to check whether web pages have come through imperfectly across different browsers; the analysis identifies not just blatant errors, but even single-pixel shifts.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1115}, 
	title = {Method and system for webpage regression testing},
	abstract = {A regression testing system comprises an automatic test tool configured to capture a first web screen shot and a second web screen shot of a webpage, where the webpage has undergone an update or edit. The regression testing system also comprises a visual comparator configured to identify similar areas in the first web screen shot and the second web screen shot. The visual comparator receives, and compares characteristics of, the web screen shots. Furthermore, the regression testing system generates a report with marked different characteristics between the first and second web screen shots. The regression testing system identifies similar areas in the first and second web screen shots shot even if the similar areas are at different locations within the web screen shots. The comparison performed by the visual comparator includes performing a pixel comparison combined with a marking algorithm to group differences in smaller, related but separate areas.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1118}, 
	title = {Identifying equivalent javascript events},
	abstract = {Identifying equivalent JavaScript events includes receiving source code containing two JavaScript events for equivalency analysis, extracting an HTML element containing an event from each JavaScript event and analyzing the extracted HTML elements. Responsive to a determination that the HTML elements are of a same type according to equivalency criteria B, and responsive to a determination that the HTML elements have a same number of attributes according to equivalency criteria C, a determination is made whether JavaScript function calls of each JavaScript event are similar according to equivalency criteria A. Responsive to a determination that the JavaScript function calls are similar according to equivalency criteria A, and responsive to a determination that the other attributes of the HTML elements satisfy equivalency criteria D, the JavaScript events are identified as equivalent.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1119}, 
	title = {Detecting influence caused by changing the source code of an application from which a document object model tree and cascading style sheet may be extracted},
	abstract = {One embodiment is a computer-implemented method for detecting an influence caused by changing a source code of an application from which a document object model (DOM) tree and cascading style sheets (CSSs) are extracted. The method includes saving one or more input operations of a user of the application, a DOM tree, and a CSS for each of one or more times that an instruction is received to check a screen state. After the source code is changed, the one or more input operations are emulated in an operation order, for each of the one or more times. A DOM tree and CSS are acquired for each of the one or more times. The saved DOM tree and CSS are compared with the acquired DOM tree and CSS for each of the one or more times. A result of the comparison is output.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1121}, 
	title = {Indexing and annotating a usability test recording},
	abstract = {A method for indexing a user interface test recording includes capturing, during a session, a recording of a plurality of interactions with the user interface, capturing, during the session, an event stream including a plurality of user interface events, synchronizing, in time, the plurality of interactions and the plurality of user interface events, identifying a point of interest in the event stream, wherein the point of interest is correlated to a time in the recording by the synchronization, and annotating the recording at a time correlated to when the point of interest occurred.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1122}, 
	title = {Facilitating debugging of web applications through recording reduction: A family of empirical studies},
	abstract = {Abstract
	Recording the sequence of events that lead to a failure of a web application can be an effective aid for debugging. Users can send a recording that results in a failure to a web application�s developer. The developer can then replay the recording, reproduce the failure, and find the fault(s) that cause it. Developers can do the same thing when faced with faults encountered in web applications in-house. A recording of an event sequence, however, may include many events that are not related to a failure, and this may render debugging more difficult. To address this problem, we have adapted Delta Debugging to function on recordings of web applications, in a manner that lets it identify and discard portions of those recordings that do not influence the occurrence of a failure, The resulting recording reduction technique can enable developers to localize faults based on reduced recordings instead of larger unreduced recordings, potentially reducing the amount of time and effort required to locate faults. We present the results of four empirical studies of our approach, in which we apply it to recordings created by Selenium IDE. In our first study we applied our technique to 30 faulty web applications obtained from developer forums, and showed that our technique could achieve significant reductions in recording size and replay time on these applications. In our second study we explored whether programmers could benefit from the use of reduced recordings when attempting to locate faults, and showed that our technique did increase their efficiency and effectiveness. In our third study we explored the scalability of our approach by applying it to substantially larger, more complex applications, and found that the approach worked even better on these larger applications than on the first set of smaller ones studied. In our fourth study we considered whether programmers working with two of these larger applications, who had more direct experience with the applications and the use of recordings and debugging could benefit from our technique. We found that the technique improved their efficiency and effectiveness, and the degree of improvement was even larger than that observed in our second study. Overall, these results suggest that recording reduction may be useful as means for helping programmers debug web applications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1123}, 
	title = {Facilitating debugging of web applications through recording reduction},
	abstract = {Abstract
	Recording the sequence of events that lead to a failure of a web application can be an effective aid for debugging. Users can send a recording that results in a failure to a web application�s developer. The developer can then replay the recording, reproduce the failure, and find the fault(s) that cause it. Developers can do the same thing when faced with faults encountered in web applications in-house. A recording of an event sequence, however, may include many events that are not related to a failure, and this may render debugging more difficult. To address this problem, we have adapted Delta Debugging to function on recordings of web applications, in a manner that lets it identify and discard portions of those recordings that do not influence the occurrence of a failure, The resulting recording reduction technique can enable developers to localize faults based on reduced recordings instead of larger unreduced recordings, potentially reducing the amount of time and effort required to locate faults. We present the results of four empirical studies of our approach, in which we apply it to recordings created by Selenium IDE. In our first study we applied our technique to 30 faulty web applications obtained from developer forums, and showed that our technique could achieve significant reductions in recording size and replay time on these applications. In our second study we explored whether programmers could benefit from the use of reduced recordings when attempting to locate faults, and showed that our technique did increase their efficiency and effectiveness. In our third study we explored the scalability of our approach by applying it to substantially larger, more complex applications, and found that the approach worked even better on these larger applications than on the first set of smaller ones studied. In our fourth study we considered whether programmers working with two of these larger applications, who had more direct experience with the applications and the use of recordings and debugging could benefit from our technique. We found that the technique improved their efficiency and effectiveness, and the degree of improvement was even larger than that observed in our second study. Overall, these results suggest that recording reduction may be useful as means for helping programmers debug web applications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1124}, 
	title = {AppCheck: A Crowdsourced Testing Service for Android Applications},
	abstract = {Abstract:
	It is well known that the fragmentation of Android ecosystem has caused severe compatibility issues. Therefore, for Android apps, cross-platform testing (the apps must be tested on a multitude of devices and operating system versions) is particularly important to assure their quality. Although lots of cross-platform testing techniques have been proposed, there are still some limitations: 1) it is time-consuming and error-prone to encode platform-agnostic tests manually, 2) test scripts generated by existing record/replay techniques are brittle and will break when replayed on different platforms, 3) Developers, and even test vendors have not equipped some special Android devices. As a result, apps have not been tested sufficiently, leading to many compatibility issues after releasing. To address these limitations, this paper proposes AppCheck, a crowdsourced testing service for Android apps. To generate tests that will explore different behavior of the app automatically, AppCheck crowdsources event trace collection over the Internet, and various touch events will be captured when real users interact with the app. The collected event traces are then transformed into platform-agnostic test scripts, and directly replayed on the devices of real users. During the replay, various data (e.g., screenshots and layout information) will be extracted to identify compatibility issues. Our empirical evaluation shows that AppCheck is effective and improves the state of the art},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1125}, 
	title = {DEVELOPMENT OF AGNOSTIC MOBILE APPLICATIONS WITH CROSS-PLATFORM CLOUD COMPUTING PLATFORMS},
	abstract = {Smart phones became part and parcel of our life, where mobility provides a freedom of not being
	bounded by time and space. In addition, number of smartphones produced each year is skyrocketing.
	However, this also created discrepancies or fragmentation among devices and OSes, which in turn
	made an exceeding hard for developers to deliver hundreds of similar featured applications with
	various versions for the market consumption.
	This thesis is an attempt to investigate whether cloud based mobile development platforms can mitigate
	and eventually eliminate fragmentation challenges. During this research, we have selected and analyzed
	the most popular cloud based development platforms and tested integrated cloud features.
	This research showed that cloud based mobile development platforms may able to reduce mobile
	fragmentation and enable to utilize single codebase to deliver a mobile application for different
	platforms.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {1128}, 
	title = {Real time web development testing and reporting system},
	abstract = {Elements of the geometry of the image of a webpage as rendered on at least one target browser are compared with elements of a baseline geometry of the webpage to determine the differences between elements of the baseline geometry of the webpage and elements of the respective geometries of the image of the webpage as rendered on the at least one target browser. The elements of the image may be determined by a software tool for determining elements of a document geometry, such as a DOM geometry service. Code such as JavaScript may be injected into the webpage for use in determining the elements of the geometry of the image of the webpage. A list of issues that web developers face may be generated and the above differences between respective elements may allow arriving at a solution for at least some of the issues in order to provide testing of webpage information in real time.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1129}, 
	title = {Testing framework for dynamic web pages},
	abstract = {A system and method for bucket testing includes: retrieving a user's information from a user in a bucket testing group when a request to serve a web page is received; determining from the user's information if the user has an active bucket identifier associated with the user's account in a persistent store; retrieving bucket parameters from the user's information when it is determined that the user has an active bucket identifier; determining if the bucket parameters are within a range assigned to the bucket testing group; passing the bucket parameters to the server; and loading configuration and files associated with the active bucket identifier.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1140}, 
	title = {Estimating similarity of rich internet pages using visual information},
	abstract = {Abstract
	Traditional text-based web page similarity measures fail to handle rich-information-embedded modern web pages. Current approaches regard web pages as either DOM trees or images. However, the former only focuses on the web page structure, while the latter ignores the inner connections among different web page features. Therefore, they are not suitable for modern web pages. Hence, the idea of a block tree is introduced, which contains both structural and visual information of web pages. A visual similarity metric is proposed as the edit distance between two block trees. Finally, an experiment is undertaken, by cross-comparing 500 web pages, illustrating that the model appears to be highly accurate, empirically demonstrating that the metric is highly promising.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1141}, 
	title = {Maintenance of automated test suites in industry: An empirical study on Visual GUI Testing},
	abstract = {Abstract
	Context: Verification and validation (V&V) activities make up 20�50% of the total development costs of a software system in practice. Test automation is proposed to lower these V&V costs but available research only provides limited empirical data from industrial practice about the maintenance costs of automated tests and what factors affect these costs. In particular, these costs and factors are unknown for automated GUI-based testing.
	Objective: This paper addresses this lack of knowledge through analysis of the costs and factors associated with the maintenance of automated GUI-based tests in industrial practice.
	Method: An empirical study at two companies, Siemens and Saab, is reported where interviews about, and empirical work with, Visual GUI Testing is performed to acquire data about the technique�s maintenance costs and feasibility.
	Results: 13 factors are observed that affect maintenance, e.g. tester knowledge/experience and test case complexity. Further, statistical analysis shows that developing new test scripts is costlier than maintenance but also that frequent maintenance is less costly than infrequent, big bang maintenance. In addition a cost model, based on previous work, is presented that estimates the time to positive return on investment (ROI) of test automation compared to manual testing.
	Conclusions: It is concluded that test automation can lower overall software development costs of a project while also having positive effects on software quality. However, maintenance costs can still be considerable and the less time a company currently spends on manual testing, the more time is required before positive, economic, ROI is reached after automation.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1142}, 
	title = {TESTAR - from academic protoype towards an industry-ready tool for automated testing at the User Interface level},
	abstract = {Abstract. Testing applications with a Graphical User Interface (GUI)
	is an important, though challenging and time consuming task. The state
	of the art in the industry are still capture and replay tools, which may
	simplify the recording and execution of input sequences, but do not support
	the tester in finding fault-sensitive test cases and leads to a huge
	overhead on maintenance of the test cases when the GUI changes. While
	search-based test case generation strategies are well researched for various
	areas of testing, relatively little work has been done on applying
	these techniques to an entire GUI of an application. In this paper we
	present the tool TESTAR, an automated search-based approach to test
	applications at the GUI level whose objective is to solve part of the
	maintenance problem by automatically generating test cases based on a
	structure that is automatically derived from the GUI.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1143}, 
	title = {Closing Gaps between Capture and Replay: Model-based GUI Testing},
	abstract = {Abstract. Testing software as a black box can be time consuming and errorprone.
	Operating and monitoring the graphical user interface is a generic method
	to test such systems. This work deals with convenient and systematic testing of
	GUI software systems. It presents a new approach to model-based GUI testing
	by combining the strengths of four well-researched areas combined: (1) the intuitive
	capture&replay method, (2) widget trees for modeling the GUI, (3) state
	charts and (4) the classification tree method. The approach is implemented as
	a prototype and is currently under validation on a real GUI. The presented approach
	includes the whole test cycle, from scanning the GUI and model-based
	test specification to the automatic execution of tests.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1144}, 
	title = {Automated Localisation Testing in Industry with Test},
	abstract = {Abstract
	Test  ??  is a testing tool that automatically and dynamically generates, executes and verifies test sequences based on a tree model that is derived from the software User Interface through assistive technologies. Test  ??  is an academic prototype that we continuously try to transfer to companies to get feedback about its applicability. In this paper we report on one of these short experiences of using Test  ??  in industry at the Valencian company Indenova. We applied the tool to check the localisation quality of a secure web platform that encapsulates a set of applications as services.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1145}, 
	title = {Development and maintenance efforts testing graphical user interfaces: a comparison},
	abstract = {For testing of graphical user interfaces many tools exists. The aim of this work is a statement regarding the advantages and disadvantages of various testing tools with regard to their use in the economic context to be taken. It is compared, inter alia, whether there are differences in the generations of test tools in terms of finding defects and which tool has the lowest development and maintenance costs. Results show that with QF-Test test suites can be created the quickest while EggPlant has the shortest maintenance time. TestComplete performs worse in both disciplines. For test robustness, no clear picture can be drawn. The selection of a test tool is typically done once in a project at the beginning and should be considered carefully.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1146}, 
	title = {Automated Testing of Web Applications with TESTAR},
	abstract = {Abstract
	The TESTAR tool was originally conceived to perform automated testing of desktop applications via their Graphical User Interface (GUI). Starting from the premise that source code is not available, TESTAR automatically selects actions based only on information derived from the GUI and in this way generates test sequences on the fly. In this work we extend its use to web applications and carry out experiments using the Odoo open source management software as the testing object. We also introduce novel metrics to evaluate the performance of the testing with TESTAR, which are valid even when access to the source code is not available and testing is only possible via the GUI. We compare results obtained for two types of action selection mechanisms, based on random choice and   QQ -learning with different parameter settings. Statistical analysis shows the superiority of the latter provided an adequate choice of parameters; furthermore, the results point to interesting areas for improvement.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1147}, 
	title = {Combining profiling and monitoring to analyze test coverage and identify performance problems},
	abstract = {The use of profilers is a common approach for locating bottlenecks in software performance.
	
	Existing profilers typically generalize memory consumption and CPU usage. This work is dedicated to profiling-based identification of performance problems for specific moments of program execution. By combining conventional profiling with monitoring of user actions (e.g. mouse and keyboard inputs), a more fine-grained analysis of program behavior is possible. The calculation of coverage levels for GUI tests will also be available. The current state of this work describes a proposed solution. Realization of a prototype implementing the approach is currently ongoing.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1148}, 
	title = {Another experience with Test? in industry: automated localisation testing},
	abstract = {Abstract. Test?
	is a testing tool that automatically and dynamically
	generates, executes and verifies test sequences based on a tree model that
	is derived from the User Interface through the Accessibility API. Test?
	is
	an academic prototype that we continuously try to transfer to companies
	to get feedback about its applicability. In this paper we report on one
	of these short experiences of using Test?
	in industry at the Valencian
	company Indenova.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1149}, 
	title = {TESTAR para testing IoT},
	abstract = {As the number of devices connected to the Internet is increasing, the so-called
	Internet of Things (IoT) is becoming a reality. It even has the required potential to
	change both the way we live and the way we work.
	In order to take advantage of the benefits that the IoT can bring us, ensuring the quality
	of massively interconnected devices becomes a pressing necessity. A means of satisfying
	this need would be automated testing of IoT devices. However, this presents many
	difficulties such as the lack of standards and limitations in battery and memory.
	In this work we start from an automated testing tool at the user interface level that has
	already been successfully applied in several industrial cases. Maintaining its philosophy
	and approach, a new tool is developed which is applicable to the IoT environment. The
	tool is evaluated by testing a smart home and the results are presented.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1150}, 
	title = {Reconocimiento de widgets autom�tico para aplicaciones Java/Swing en TESTAR},
	abstract = {The Software Testing and Quality (STAQ) group of the PROS research center at the
	Polytechnic University of Valencia (UPV) has developed a tool, called TESTAR
	(www.testar.org) for automated testing at the user interface level (UI) . TESTAR generates and
	executes test cases automatically based on a tree model automatically derived from the UI of the
	application under test. This tree is built using the Accessibility API of the operating system that
	helps to recognize all graphical UI elements (widgets). The tool is not capture / replay nor uses
	image recognition. Companies that have deployed the tool are very positive and see it as a
	paradigm shift for testing. They believe that TESTAR has the potential to solve many problems
	with existing tools.
	In this project the aim is to extend and implement the recognizability of widgets (graphical
	elements of the User Interface) of the TESTAR tool for Java applications in Microsoft Windows
	operating systems. TESTAR has a limitation regarding the recognition of widgets when the Java
	technology Swing is used (it runs smoothly for AWT and SWT).
	TESTAR is based on accessibility technologies that expose widgets of the software
	application under test. The "lightweight" character of Swing makes that some Swing elements
	are not correctly identified by accessibility technologies . To support the application
	accesbilidad for Swing there is a bridge called Java Access Bridge exposes the Java
	Accessibility API in a dynamic link library (DLL) for Windows:
	http://www.oracle.com/technetwork/articles/javase/index-jsp-136191.html
	Therefore, the work of the project will be:
	� Study the Java Access Bridge bridge to facilitate recognition of widgets automatically in
	Java / Swing applications.
	� Implement a plug-in for TESTAR to enrich the tool with recognition of Swing widgets, in
	addition to the current support for AWT and SWT technologies.
	� Assess the capacity of TESTAR in Java / Swing with two case studies with industrial
	applications. (Currently EVERIS and Clearone are companies that have shown interest in
	having this capacity available in TESTAR).
	� Document the results},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1151}, 
	title = {Harnessing Automated Test Case Generators for GUI Testing in Industry},
	abstract = {Abstract:
	Modern graphical user interfaces (GUIs) are highly dynamic and support multi-touch interactions and screen gestures besides conventional inputs via mouse and keyboard. Hence, the flexibility of modern GUIs enables countless usage scenarios and combinations including all kind of interactions. From the viewpoint of testing, this flexibility results in a combinatorial explosion of possible interaction sequences. It dramatically raises the required time and effort involved in GUI testing, which brings manual exploration as well as conventional regression testing approaches to its limits. Automated test generation (ATG) has been proposed as a solution to reduce the effort for manually designing test cases and to speed-up test execution cycles. In this paper we describe how we successfully harnessed a state-of-the-art ATG tool (Randoop) developed for code-based API testing to generate GUI test cases. The key is an adapter that transforms API calls to GUI events. The approach is the result of a research transfer project with the goal to apply ATG for testing of human machine interfaces used to control industrial machinery. In this project the ATG tool was used to generate unit test cases for custom GUI controls and system tests for exploring navigation scenarios. It helped to increase the test coverage and was able reveal new defects in the implementation of the GUI controls as well as in the GUI application.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1152}, 
	title = {Automated Testing at the User Interface level},
	abstract = {Graphical User Interfaces (GUIs) represent the main connection
	point between a software�s components and its end
	users and can be found in almost all modern applications.
	This makes them attractive for testers, since testing at the GUI
	level means testing from the user�s perspective and is thus
	the ultimate way of verifying a program�s correct behaviour.
	Current GUIs can account for 45-60% of the entire source
	code [1] and are often large and complex. To be effective, UI
	testing should be automated.
	A substantial part of the current state-of-the art for automating
	UI testing is still based on the Capture and Replay
	(CR) technique [2]. CR requires significant human intervention
	to record interactions (i.e. clicks, keystrokes, drag/drop
	operations) that are used as regression tests for new product
	releases. A known concern of CR is that it creates a critical
	maintenance problem because the test cases easily break when
	the UI evolves, which happens often. A more advanced technique,
	Visual testing [3], takes advantage of image processing
	algorithms to simulate step by step human interactions. Though
	visual approaches simplify the work of testers, they are slow,
	imprecise (prone to false positives with wrong UI element
	identification, and false negatives with missed UI elements),
	and also rely on the GUI stability.
	We present a completely different approach to automated
	GUI testing called TESTAR1
	(Test Automation at the user
	inteRface level). TESTAR automatically and dynamically generates
	test sequences based on a tree model (automatically
	derived from the UI through the Accessibility API). No test
	cases are recorded and the tree model is inferred for every
	state, this implies that tests will run even when the GUI
	changes. This reduces the maintenance problem that threatens
	the techniques mentioned earlier.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1153}, 
	title = {Hybrid monkey testing: enhancing automated GUI tests with random test generation},
	abstract = {Many software projects maintain automated GUI tests that are repeatedly executed for regression testing. Every test run executes exactly the same fixed sequence of steps confirming that the currently tested version shows precisely the same behavior as the last version. The confirmatory approach implemented by these tests limits their ability to find new defects. We therefore propose to combine existing automated regression tests with random test generation. Random test generation creates a rich variety of test steps that interact with the system under test in new, unexpected ways. Enhancing existing test cases with random test steps allows revealing new, hidden defects with little extra effort. In this paper we describe our implementation of a hybrid approach that enhances existing GUI test cases with additional, randomly generated interactions. We conducted an experiment using a mature, widely-used open source application. On average the added random interactions increased the number of visited application windows per test by 23.6% and code coverage by 12.9%. Running the enhanced tests revealed three new defects.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1154}, 
	title = {Adapting automated test generation to GUI testing of industry applications},
	abstract = {Abstract
	Context
	
	Automated test generation promises to improve the effectiveness of software testing and to reduce the involved manual effort. While automated test generation has been successfully applied for code-level API testing, it has not found widespread adoption in practice for testing of graphical user interfaces. Tools for test generation do not support GUI testing out-of-the-box but require dedicated extensions.
	
	Objective
	
	This paper explores the applicability of automated test generation for testing GUIs of industry applications. We propose a test adapter approach to bridge the gap between automated test generation tools and industry applications.
	
	Method
	
	A multiple case study was conducted in which automated test generation with test adapters has been applied at the unit, integration, and system test level in three industry projects from two different companies.
	
	Results
	
	Automated test generation via test adapters could be applied at all test levels. It has led to an increase of coverage as well as the detection of new defects that were not found by preceding testing activities in the projects. While test adapters can easily be implemented at the unit test level, their complexity and the corresponding effort for providing adapter implementations rises at higher test levels.
	
	Conclusion
	
	Test adapters can be used for applying automated test generation for testing GUIs of industry applications. They bridge the gap between automated test generation tools and industry applications. The development of test adapters requires no tool-specific knowledge and can be performed by members of the development team.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1155}, 
	title = {Searching for the Best Test},
	abstract = {Abstract:
	Random testing has been controversial throughout the history. In the early 70s opinions about random testing were divided: Girard and Rault (1973) call it a valuable test case generation scheme [11]. This is confirmed by Thayer, Lipow and Nelson (1978) in their book on software reliability [21] they say it is the necessary final step in the testing activities. However, Glenford Myers (1979) in his seminal work on the art of Software Testing [18] denominates random testing as probably the poorest testing method.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1156}, 
	title = {Overview of the ICST International Software Testing Contest},
	abstract = {Abstract:
	In the software testing contest, practitioners and researcher's are invited to test their test approaches against similar approaches to evaluate pros and cons and which is perceivably the best. The 2017 iteration of the contest focused on Graphical User Interface-driven testing, which was evaluated on the testing tool TESTONA. The winner of the competition was announced at the closing ceremony of the international conference on software testing (ICST), 2017.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {1158}, 
	title = {Efficient means to test server generated applications on mobile device},
	abstract = {Systems and methods are provided to test changes for a mobile app built by web-based tooling directly on a physical mobile device. A first application can be loaded on a mobile device. The first application can receive metadata of a second application. The first application can execute the second application using the metadata. Access to local resources can be intercepted and redirected to the server for processing. Additionally, changes made to the second application using the web-based tooling can be pushed to the first application using a persistent channel allowing the changes to be immediately tested.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1159}, 
	title = {Software testing system and method},
	abstract = {A computer implemented system and method for testing code for implementation in web browsers, implements test class code defining test cases for testing operations on web applications implementable by different web browser types, and implements handler class code comprising code specific to each web browser and defining strategies to be used by test cases. The handler class code implements a handler interface and the test class code uses the handler interface to identify the methods to be used for test cases. A non-transient storage medium stores code for a handler template supporting different web browser types for use in a test environment, the code comprising code to identify browser specific strategies for implementation in the test environment; code to implement a strategy support interface for use by a handler factory to select a handler instance; and code to implement a handler interface for use by a test case to identify methods for testing web browser functions.},
	duplicado = {false},
	inserir = {false}
}

%--	IEEE Xplore
@Article{
	id = {1591}, 
	title = {Detection and Localization of HTML Presentation Failures Using Computer Vision-Based Techniques},
	abstract = {Abstract:
	An attractive and visually appealing appearance is important for the success of a website. Presentation failures in a site's web pages can negatively impact end users' perception of the quality of the site and the services it delivers. Debugging such failures is challenging because testers must visually inspect large web pages and analyze complex interactions among the HTML elements of a page. In this paper we propose a novel automated approach for debugging web page user interfaces. Our approach uses computer vision techniques to detect failures and can then identify HTML elements that are likely to be responsible for the failure. We evaluated our approach on a set of real-world web applications and found that the approach was able to accurately and quickly identify faulty HTML elements.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1592}, 
	title = {Detecting and Localizing Visual Inconsistencies in Web Applications},
	abstract = {Abstract:
	Failures in the presentation layer of a web application can negatively impact its usability and end users' perception of the application's quality. The problem of verifying the consistency of a web application's user interface across its different pages is one of the many challenges that software development teams face in testing the presentation layer. In this paper we propose a novel automated approach to detect and localize visual inconsistencies in web applications. To detect visual inconsistencies, our approach uses computer vision techniques to compare a test web page with its reference. Then to localize, our approach analyzes the structure and style of the underlying HTML elements to find the faulty elements responsible for the observed inconsistencies.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1594}, 
	title = {Using Visual Symptoms for Debugging Presentation Failures in Web Applications},
	abstract = {Abstract:
	Presentation failures in a website can undermine its success by giving users a negative perception of the trustworthiness of the site and the quality of the services it delivers. Unfortunately, existing techniques for debugging presentation failures do not provide developers with automated and broadly applicable solutions for finding the site's faulty HTML elements and CSS properties. To address this limitation, we propose a novel automated approach for debugging web sites that is based on image processing and probabilistic techniques. Our approach first builds a model that links observable changes in the web site's appearance to faulty elements and styling properties. Then using this model, our approach predicts the elements and styling properties most likely to cause the observed failure for the page under test and reports these to the developer. In evaluation, our approach was more accurate and faster than prior techniques for identifying faulty elements in a website.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1595}, 
	title = {Automated analysis of CSS rules to support style maintenance},
	abstract = {Abstract:
	CSS is a widely used language for describing the presentation semantics of HTML elements on the web. The language has a number of characteristics, such as inheritance and cascading order, which makes maintaining CSS code a challenging task for web developers. As a result, it is common for unused rules to be accumulated over time. Despite these challenges, CSS analysis has not received much attention from the research community. We propose an automated technique to support styling code maintenance, which (1) analyzes the runtime relationship between the CSS rules and DOM elements of a given web application (2) detects unmatched and ineffective selectors, overridden declaration properties, and undefined class values. Our technique, implemented in an open source tool called Cilla, has a high precision and recall rate. The results of our case study, conducted on fifteen open source and industrial web-based systems, show an average of 60% unused CSS selectors in deployed applications, which points to the ubiquity of the problem.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1597}, 
	title = {A Comprehensive Client-Side Behavior Model for Diagnosing Attacks in Ajax Applications},
	abstract = {Abstract:
	Behavior models of applications are widely used for diagnosing security incidents in complex web-based systems. However, Ajax techniques that enable better web experiences also make it fairly challenging to model Ajax application behaviors in the complex browser environment. In Ajax applications, server-side states are no longer synchronous with the views to end users at the client side. Therefore, to model the behaviors of Ajax applications, it is indispensable to incorporate client-side application states into the behavior models, as being explored by prior work. Unfortunately, how to leverage behavior models to perform security diagnosis in Ajax applications has yet been thoroughly examined. Existing models extracted from Ajax application behaviors are insufficient in a security context. In this paper, we propose a new behavior model for diagnosing attacks in Ajax applications, which abstracts both client-side state transitions as well as their communications to external servers. Our model articulates different states with the browser events or user actions that trigger state transitions. With a prototype implementation, we demonstrate that the proposed model is effective in attack diagnosis for real-world Ajax applications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1598}, 
	title = {CSSDev: refactoring duplication in cascading style sheets},
	abstract = {Abstract:
	Cascading Style Sheets (CSS) is a widely-used language for defining the presentation of structured documents and user interfaces. Despite its popularity, CSS still lacks adequate tool support for everyday maintenance tasks, such as debugging and refactoring. In this paper, we present CSSDEV, a tool suite for analyzing CSS code to detect refactoring opportunities.(https://youtu.be/lu3oITi1XrQ).},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1600}, 
	title = {Detecting and Localizing Internationalization Presentation Failures in Web Applications},
	abstract = {Abstract:
	Web applications can be easily made available to an international audience by leveraging frameworks and tools for automatic translation and localization. However, these automated changes can distort the appearance of web applications since it is challenging for developers to design their websites to accommodate the expansion and contraction of text after it is translated to another language. Existing web testing techniques do not support developers in checking for these types of problems and manually checking every page in every language can be a labor intensive and error prone task. To address this problem, we introduce an automated technique for detecting when a web page's appearance has been distorted due to internationalization efforts and identifying the HTML elements or text responsible for the observed problem. In evaluation, our approach was able to detect internationalization problems in a set of 54 web applications with high precision and recall and was able to accurately identify the underlying elements in the web pages that led to the observed problem.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1601}, 
	title = {Software Analysis for the Web: Achievements and Prospects},
	abstract = {Abstract:
	The web has had a significant impact on our lives. A technology that was initially created for sharing documents across the network has evolved into a strong medium for developing and distributing software applications. In this paper, we first provide a concise overview of the evolution of the web itself. We then focus on some of the main industrial and research achievements in software analysis and testing techniques geared toward web apps, in the past two decades. We discuss static, dynamic, and hybrid analyses approaches, software testing and test adequacy techniques, as well as techniques that help developers write, analyze and maintain their code. Finally, we present some of the current and future challenges and research opportunities ahead in this field.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1604}, 
	title = {Detecting latent cross-platform API violations},
	abstract = {Abstract:
	Many APIs enable cross-platform system development by abstracting over the details of a platform, allowing application developers to write one implementation that will run on a wide variety of platforms. Unfortunately, subtle differences in the behavior of the underlying platforms make cross-platform behavior difficult to achieve. As a result, applications using these APIs can be plagued by bugs difficult to observe before deployment. These portability bugs can be particularly difficult to diagnose and fix because they arise from the API implementation, the operating system, or hardware, rather than application code. This paper describes CheckAPI, a technique for detecting violations of cross-platform portability. CheckAPI compares an application's interactions with the API implementation to its interactions with a partial specification-based API implementation, and does so efficiently enough to be used in real production systems and at runtime. CheckAPI finds latent errors that escape pre-release testing. This paper discusses the subtleties of different kinds of API calls and strategies for effectively producing the partial implementations. Validating CheckAPI on JavaScript, the Seattle project's Repy VM, and POSIX detects dozens of violations that are confirmed bugs in widely-used software.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {1606}, 
	title = {A Survey of Dynamic Analysis and Test Generation for JavaScript},
	abstract = {JavaScript has become one of the most prevalent programming languages. Unfortunately, some of the unique properties that contribute to this popularity also make JavaScript programs prone to errors and difficult for program analyses to reason about. These properties include the highly dynamic nature of the language, a set of unusual language features, a lack of encapsulation mechanisms, and the �no crash� philosophy. This article surveys dynamic program analysis and test generation techniques for JavaScript targeted at improving the correctness, reliability, performance, security, and privacy of JavaScript-based software.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1609}, 
	title = {The first decade of GUI ripping: Extensions, applications, and broader impacts},
	abstract = {Abstract:
	This paper provides a retrospective examination of GUI Ripping - reverse engineering a workflow model of the graphical user interface of a software application - born a decade ago out of recognition of the severe need for improving the then largely manual state-of-the-practice of functional GUI testing. In these last 10 years, GUI ripping has turned out to be an enabler for much research, both within our group at Maryland and other groups. Researchers have found new and unique applications of GUI ripping, ranging from measuring human performance to re-engineering legacy user interfaces. GUI ripping has also enabled large-scale experimentation involving millions of test cases, thereby helping to understand the nature of GUI faults and characteristics of test cases to detect them. It has resulted in large multi-institutional Government-sponsored research projects on test automation and benchmarking. GUI ripping tools have been ported to many platforms, including Java AWT and Swing, iOS, Android, UNO, Microsoft Windows, and web. In essence, the technology has transformed the way researchers and practitioners think about the nature of GUI testing, no longer considered a manual activity; rather, thanks largely to GUI Ripping, automation has become the primary focus of current GUI testing techniques.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1611}, 
	title = {Automated Modularization of GUI Test Cases},
	abstract = {Abstract:
	Test cases that drive an application under test via its graphical user interface (GUI) consist of sequences of steps that perform actions on, or verify the state of, the application user interface. Such tests can be hard to maintain, especially if they are not properly modularized - that is, common steps occur in many test cases, which can make test maintenance cumbersome and expensive. Performing modularization manually can take up considerable human effort. To address this, we present an automated approach for modularizing GUI test cases. Our approach consists of multiple phases. In the first phase, it analyzes individual test cases to partition test steps into candidate subroutines, based on how user-interface elements are accessed in the steps. This phase can analyze the test cases only or also leverage execution traces of the tests, which involves a cost-accuracy tradeoff. In the second phase, the technique compares candidate subroutines across test cases, and refines them to compute the final set of subroutines. In the last phase, it creates callable subroutines, with parameterized data and control flow, and refactors the original tests to call the subroutines with context-specific data and control parameters. Our empirical results, collected using open-source applications, illustrate the effectiveness of the approach.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1614}, 
	title = {Automated Acceptance Testing of JavaScript Web Applications},
	abstract = {Abstract:
	Acceptance testing is an important part of software development and it is performed to ensure that a system delivers its required functionalities. Today, most modern interactive web applications are designed using Web 2.0 technologies, many among them relying on JavaScript. JavaScript enables the development of client-side functionality through the dynamic modification of the web-page's content and structure without calls to the server. This implies that server-side testing frameworks will necessarily fail to test the complete application behaviors. In this paper we present a method for automated acceptance testing of JavaScript web applications to ensure that required functionalities have been implemented. Using an intuitive, human-readable scripting language our method allows users to describe user stories in high level declarative test scripts and to then execute these test scripts on a web application using an automated website crawler. We also describe a case study that evaluates our approach in terms of capabilities to translate user stories in automated acceptance test scripts.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1616}, 
	title = {Invariant-Based Automatic Testing of Modern Web Applications},
	abstract = {Abstract:
	Ajax-based Web 2.0 applications rely on stateful asynchronous client/server communication, and client-side runtime manipulation of the DOM tree. This not only makes them fundamentally different from traditional web applications, but also more error-prone and harder to test. We propose a method for testing Ajax applications automatically, based on a crawler to infer a state-flow graph for all (client-side) user interface states. We identify Ajax-specific faults that can occur in such states (related to, e.g., DOM validity, error messages, discoverability, back-button compatibility) as well as DOM-tree invariants that can serve as oracles to detect such faults. Our approach, called Atusa, is implemented in a tool offering generic invariant checking components, a plugin-mechanism to add application-specific state validators, and generation of a test suite covering the paths obtained during crawling. We describe three case studies, consisting of six subjects, evaluating the type of invariants that can be obtained for Ajax applications as well as the fault revealing capabilities, scalability, required manual effort, and level of automation of our testing approach.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1617}, 
	title = {Reverse Engineering iOS Mobile Applications},
	abstract = {Abstract:
	As a result of the ubiquity and popularity of smart phones, the number of third party mobile applications is explosively growing. With the increasing demands of users for new dependable applications, novel software engineering techniques and tools geared towards the mobile platform are required to support developers in their program comprehension and analysis tasks. In this paper, we propose a reverse engineering technique that automatically (1) hooks into, dynamically runs, and analyzes a given iOS mobile application, (2) exercises its user interface to cover the interaction state space and extracts information about the runtime behaviour, and (3) generates a state model of the given application, capturing the user interface states and transitions between them. Our technique is implemented in a tool called iCrawler. To evaluate our technique, we have conducted a case study using six open-source iPhone applications. The results indicate that iCrawler is capable of automatically detecting the unique states and generating a correct model of a given mobile application.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1618}, 
	title = {Visual testing of Graphical User Interfaces: An exploratory study towards systematic definitions and approaches},
	abstract = {Abstract:
	Graphical User Interface (GUI) testing literature emphasizes testing a system's functionality through its GUI, rather than testing visual aspects of the GUI itself. In this paper we introduce the notion of visual testing as a subset of GUI testing. To explore visual testing, we have conducted a study of defects in four open source systems. We found that visual defects represent between 16% and 33% of reported defects in those systems. Two categories of visual defects are identified with six subcategories within each of them. Other findings are also reported that are aimed at motivating the importance and the need for systematically conducting visual testing among researchers and practitioners.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1619}, 
	title = {Extracting Interaction-Based Stateful Behavior in Rich Internet Applications},
	abstract = {Abstract:
	Although asynchronous technologies such as Ajax make Rich Internet Applications (RIAs) responsive, they can result in unexpected behavior due to nondeterministic client-side processing and asynchronous communication. One difficulty in understanding such erroneous behavior lies in the unpredictable contexts of the running system. Dynamic behavior analysis techniques do not help to verify the correctness of certain "blind spots" in the execution path. In this work, we present a static approach for extracting all possible state transitions described in source code from the RIAs. Our approach is based on the assumption that user, server and self interactions with the RIAs can change the states of the application. Our method consists of three steps: (i) annotating interactions and extracting their controls in source code (ii) abstracting a call graph to extract relationships among the interactions and (iii) refining the relationships with interaction controls By extracting the state machines of test scenarios of the correct and wrong behavior, it can help developers to pinpoint the statements in the source code that lead to the erroneous behavior. Our approach has been evaluated against a few experimental cases and we conclude that it can extract comprehensible state machines in a reasonable time.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1620}, 
	title = {Real Challenges in Mobile App Development},
	abstract = {Abstract:
	Context: Mobile app development is a relatively new phenomenon that is increasing rapidly due to the ubiquity and popularity of smartphones among end-users. Objective: The goal of our study is to gain an understanding of the main challenges developers face in practice when they build apps for different mobile devices. Method: We conducted a qualitative study, following a Grounded Theory approach, in which we interviewed 12 senior mobile developers from 9 different companies, followed by a semi-structured survey, with 188 respondents from the mobile development community. Results: The outcome is an overview of the current challenges faced by mobile developers in practice, such as developing apps across multiple platforms, lack of robust monitoring, analysis, and testing tools, and emulators that are slow or miss many features of mobile devices. Conclusion: Based on our findings of the current practices and challenges, we highlight areas that require more attention from the research and development community.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1621}, 
	title = {Extended Subtree: A New Similarity Function for Tree Structured Data},
	abstract = {Abstract:
	Although several distance or similarity functions for trees have been introduced, their performance is not always satisfactory in many applications, ranging from document clustering to natural language processing. This research proposes a new similarity function for trees, namely Extended Subtree (EST), where a new subtree mapping is proposed. EST generalizes the edit base distances by providing new rules for subtree mapping. Further, the new approach seeks to resolve the problems and limitations of previous approaches. Extensive evaluation frameworks are developed to evaluate the performance of the new approach against previous proposals. Clustering and classification case studies utilizing three real-world and one synthetic labeled data sets are performed to provide an unbiased evaluation where different distance functions are investigated. The experimental results demonstrate the superior performance of the proposed distance function. In addition, an empirical runtime analysis demonstrates that the new approach is one of the best tree distance functions in terms of runtime efficiency.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1623}, 
	title = {Automated verification of pattern-based interaction invariants in Ajax applications},
	abstract = {Abstract:
	When developing asynchronous JavaScript and XML (Ajax) applications, developers implement Ajax design patterns for increasing the usability of the applications. However, unpredictable contexts of running applications might conceal faults that will break the design patterns, which decreases usability. We propose a support tool called JSVerifier that auto-matically verifies interaction invariants; the applications handle their interactions in invariant occurrence and order. We also present a selective set of interaction invariants derived from Ajax design patterns, as input. If the application behavior breaks the design patterns, JSVerifier automatically outputs faulty execution paths for debugging. The results of our case studies show that JSVerifier can verify the interaction invariants in a feasible amount of time, and we conclude that it can help developers increase the usability of Ajax applications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1624}, 
	title = {Adaptive Random Testing for Image Comparison in Regression Web Testing},
	abstract = {Abstract:
	Web applications have become the most popular type of software in the past decade, attracting the attention of both the academia and the industry. In parallel with their popularity, the complexity of aesthetics and functionality of web applications have also increased significantly, creating a big challenge for maintenance and cross-browser compliance testing. Since such testing and verification activities require visual analysis, web application testing has not been sufficiently automated. In this paper, we propose a novel pairwise image comparison approach suitable for web application testing where the location of layout faults needs to be detected efficiently while insignificant variations being neglected. This technique is developed based on the characteristics of fault patterns of browser layouts. An empirical study conducted with the industry partner shows our approach is more effective and efficient than existing methods in this area.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1630}, 
	title = {Using Semantic Similarity in Crawling-Based Web Application Testing},
	abstract = {Abstract:
	To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, their broad use is limited by the required manual configurations for input value selection, GUI state comparison and clickable detection. In existing crawlers, the configurations are usually string-matching based rules looking for tags or attributes of DOM elements, and often application-specific. Moreover, in input topic identification, it can be difficult to determine which rule suggests a better match when several rules match an input field to more than one topic. This paper presents a natural-language approach based on semantic similarity to address the above issues. The proposed approach represents DOM elements as vectors in a vector space formed by the words used in the elements. The topics of encountered input fields during crawling can then be inferred by their similarities with ones in a labeled corpus. Semantic similarity can also be applied to suggest if a GUI state is newly discovered and a DOM element is clickable under an unsupervised learning paradigm. We evaluated the proposed approach in input topic identification with 100 real-world forms and GUI state comparison with real data from industry. Our evaluation shows that the proposed approach has comparable or better performance to the conventional techniques. Experiments in input topic identification also show that the accuracy of the rule-based approach can be improved by up to 22% when integrated with our approach.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1641}, 
	title = {Automated Generation of Oracles for Testing User-Interaction Features of Mobile Apps},
	abstract = {Abstract:
	As the use of mobile devices becomes increasingly ubiquitous, the need for systematically testing applications (apps) that run on these devices grows more and more. However, testing mobile apps is particularly expensive and tedious, often requiring substantial manual effort. While researchers have made much progress in automated testing of mobile apps during recent years, a key problem that remains largely untracked is the classic oracle problem, i.e., to determine the correctness of test executions. This paper presents a novel approach to automatically generate test cases, that include test oracles, for mobile apps. The foundation for our approach is a comprehensive study that we conducted of real defects in mobile apps. Our key insight, from this study, is that there is a class of features that we term user-interaction features, which is implicated in a significant fraction of bugs and for which oracles can be constructed - in an application agnostic manner -- based on our common understanding of how apps behave. We present an extensible framework that supports such domain specific, yet application agnostic, test oracles, and allows generation of test sequences that leverage these oracles. Our tool embodies our approach for generating test cases that include oracles. Experimental results using 6 Android apps show the effectiveness of our tool in finding potentially serious bugs, while generating compact test suites for user-interaction features.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1643}, 
	title = {Automatic Detection of Potential Layout Faults Following Changes to Responsive Web Pages (N)},
	abstract = {Abstract:
	Due to the exponential increase in the number ofmobile devices being used to access the World Wide Web, it iscrucial that Web sites are functional and user-friendly across awide range of Web-enabled devices. This necessity has resulted in the introduction of responsive Web design (RWD), which usescomplex cascading style sheets (CSS) to fluidly modify a Web site's appearance depending on the viewport width of the device in use. Although existing tools may support the testing of responsive Web sites, they are time consuming and error-prone to use because theyrequire manual screenshot inspection at specified viewport widths. Addressing these concerns, this paper presents a method thatcan automatically detect potential layout faults in responsively designed Web sites. To experimentally evaluate this approach, weimplemented it as a tool, called ReDeCheck, and applied itto 5 real-world web sites that vary in both their approach toresponsive design and their complexity. The experiments revealthat ReDeCheck finds 91% of the inserted layout faults.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1645}, 
	title = {AppCheck: A Crowdsourced Testing Service for Android Applications},
	abstract = {Abstract:
	It is well known that the fragmentation of Android ecosystem has caused severe compatibility issues. Therefore, for Android apps, cross-platform testing (the apps must be tested on a multitude of devices and operating system versions) is particularly important to assure their quality. Although lots of cross-platform testing techniques have been proposed, there are still some limitations: 1) it is time-consuming and error-prone to encode platform-agnostic tests manually, 2) test scripts generated by existing record/replay techniques are brittle and will break when replayed on different platforms, 3) Developers, and even test vendors have not equipped some special Android devices. As a result, apps have not been tested sufficiently, leading to many compatibility issues after releasing. To address these limitations, this paper proposes AppCheck, a crowdsourced testing service for Android apps. To generate tests that will explore different behavior of the app automatically, AppCheck crowdsources event trace collection over the Internet, and various touch events will be captured when real users interact with the app. The collected event traces are then transformed into platform-agnostic test scripts, and directly replayed on the devices of real users. During the replay, various data (e.g., screenshots and layout information) will be extracted to identify compatibility issues. Our empirical evaluation shows that AppCheck is effective and improves the state of the art.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1646}, 
	title = {Searching for Behavioural Bugs with Stateful Test Oracles in Web Crawlers},
	abstract = {Abstract:
	Web applications pervade all aspects of human activity today. Therefore the content of the web has become extremely important. According to the great number of applications present nowadays, as a consequence, the manifestation of a bug has become very common. Testing modern web applications, so called "Web 2.0" applications has become more difficult due to their "stateful" nature, so the development of an approach capable of testing these applications in order to detect these bugs has become a necessity. The paper presents an automated approach for testing these dynamic web applications, where a combination of dynamic crawling and back-end testing is used to automatically detect behavioural bugs.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1647}, 
	title = {Class coverage GUI testing for Android applications},
	abstract = {Abstract:
	Mobile devices such as smartphones and tablets have become an integral part of a person's life. These portable devices opened up a new software market for mobile application development resulting in various applications from healthcare, banking till entertainment. Therefore, there is a need for mobile applications to be reliable and maintainable. In this paper we introduce an equivalent class based technique for testing the graphical user interface of Android applications. This technique is a specification based approach, in which test cases are generated based on the functionalities and the graphical user interface specification. For each possible user interface event a set of test cases are generated using equivalence class partitioning approach. Once the test cases are generated for the given application, the app is executed based on the generated test cases and results are compared with the other testing techniques. From the obtained results we can infer that our approach detects more bugs than other previous work. In addition, this approach helps in the generation of test cases at an early in the app development life cycle.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1651}, 
	title = {An automatic layout faults detection technique in responsive web pages considering JavaScript defined dynamic layouts},
	abstract = {Abstract:
	Revolutionary changes in technology has brought a lots of different types of devices towards the Internet enabled services. It has become challenging to keep web sites functional and user friendly in a wide range of devices. Responsive Web Design (RWD) steps up to facilitate the necessary support to overcome those challenges with complex cascading style sheet (CSS) and JavaScript defined layouts. Fluidly modified layout that controls appearance of Responsive Web Pages (RWP) is subjected to erroneous when faulty CSS and JavaScript modifies the actual look of the web page. Though several methods have already been proposed to detect layout faults in web pages, they are error-prone and time consuming in nature. Thus an automatic approach considering both CSS and JavaScript defined dynamic layout fault detection technique have been proposed in this paper. Simulation result demonstrates that proposed technique outperforms the existing methods in case of time requirement, memory requirement and fault detection rate.},
	duplicado = {false},
	inserir = {false}
}

@Article{
	id = {1724}, 
	title = {Facilitating debugging of web applications through recording reduction},
	abstract = {Abstract
	Recording the sequence of events that lead to a failure of a web application can be an effective aid for debugging. Users can send a recording that results in a failure to a web application�s developer. The developer can then replay the recording, reproduce the failure, and find the fault(s) that cause it. Developers can do the same thing when faced with faults encountered in web applications in-house. A recording of an event sequence, however, may include many events that are not related to a failure, and this may render debugging more difficult. To address this problem, we have adapted Delta Debugging to function on recordings of web applications, in a manner that lets it identify and discard portions of those recordings that do not influence the occurrence of a failure, The resulting recording reduction technique can enable developers to localize faults based on reduced recordings instead of larger unreduced recordings, potentially reducing the amount of time and effort required to locate faults. We present the results of four empirical studies of our approach, in which we apply it to recordings created by Selenium IDE. In our first study we applied our technique to 30 faulty web applications obtained from developer forums, and showed that our technique could achieve significant reductions in recording size and replay time on these applications. In our second study we explored whether programmers could benefit from the use of reduced recordings when attempting to locate faults, and showed that our technique did increase their efficiency and effectiveness. In our third study we explored the scalability of our approach by applying it to substantially larger, more complex applications, and found that the approach worked even better on these larger applications than on the first set of smaller ones studied. In our fourth study we considered whether programmers working with two of these larger applications, who had more direct experience with the applications and the use of recordings and debugging could benefit from our technique. We found that the technique improved their efficiency and effectiveness, and the degree of improvement was even larger than that observed in our second study. Overall, these results suggest that recording reduction may be useful as means for helping programmers debug web applications.},
	duplicado = {false},
	inserir = {false}
}

%-- OUTRAS BASES
@Article{
	id = {1915}, 
	title = {Leveraging task-based data to support functional testing of web applications},
	abstract = {Testing is paramount in order to assure the quality of a software product. Over the last years, several techniques have been proposed to leverage the testing phase as a simple and efficient step during software development. However, the features of the web environment make application testing fairly complex. The existing approaches for web application testing are usually driven to specific scenarios or application types, and few solutions are targeted for testing the functional requirements of applications. In order to tackle this problem, we propose a task-based testing approach that provides high coverage of functional requirements. Our technique consists of reassembling classical graph algorithms in order to generate all the possible paths for the execution of a task. Performed experiments indicate that our approach is effective for supporting the functional testing of web applications.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1917}, 
	title = {Mining behavior models from enterprise web applications},
	abstract = {Today's enterprise web applications demand very high release cycles---and consequently, frequent tests. Automating these tests typically requires a behavior model: A description of the states the application can be in, the transitions between these states, and the expected results. Furthermore one needs scripts to make the abstract actions (transitions) in the model executable. As specifying such behavior models and writing the necessary scripts manually is a hard task, a possible alternative could be to extract them from existing applications. However, mining such models can be a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present ProCrawl (PROcess CRAWLer), a generic approach to mine behavior models from (multi-user) enterprise web applications. ProCrawl observes the behavior of the application through its user interface, generates and executes tests to explore unobserved behavior. In our evaluation of three non-trivial web applications (an open-source shop system, an SAP product compliance application, and an open-source conference manager), ProCrawl produces models that precisely abstract application behavior and which can be directly used for effective model-based regression testing.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1918}, 
	title = {Inferring models with rule-based expert systems},
	abstract = {Many works related to software engineering rely upon formal models, e.g., to perform model-checking or automatic test case generation. Nonetheless, producing such models is usually tedious and error-prone. Model inference is a research field helping in producing models by generating partial models from documentation or execution traces (observed action sequences). This paper presents a new model generation method combining model inference and expert systems. It appears that an engineer is able to recognise the functional behaviours of an application from its traces by applying deduction rules. We propose a framework, applied to Web applications, simulating this reasoning mechanism, with inference rules organised into layers. Each yields partial IOSTSs (Input Output Symbolic Transition Systems), which become more and more abstract and understandable.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1919}, 
	title = {Software engineering for the web: the state of the practice},
	abstract = {Today�s web applications increasingly rely on client-side code execution. HTML is not just created on the server, but manipulated extensively within the browser through JavaScript code. In this paper, we seek to understand the software engineering implications of this. We look at deviations from many known best practices in such areas of performance, accessibility, and correct structuring of HTML documents. Furthermore, we assess to what extent such deviations manifest themselves through client-side code manipulation only. To answer these questions, we conducted a large scale experiment, involving automated client-enabled crawling of over 4000 web applications, resulting in over 100,000,000 pages analyzed, and close to 1,000,000 unique client-side user interface states. Our findings show that the majority of sites contain a substantial number of problems, making sites unnecessarily slow, inaccessible for the visually impaired, and with layout that is unpredictable due to errors in the dynamically modified DOM trees.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1920}, 
	title = {X-PERT: accurate identification of cross-browser issues in web applications},
	abstract = {Due to the increasing popularity of web applications, and the number of browsers and platforms on which such applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious concern for organizations that develop web-based software. Most of the techniques for XBI detection developed to date are either manual, and thus costly and error-prone, or partial and imprecise, and thus prone to generating both false positives and false negatives. To address these limitations of existing techniques, we developed X-PERT, a new automated, precise, and comprehensive approach for XBI detection. X-PERT combines several new and existing differencing techniques and is based on our findings from an extensive study of XBIs in real-world web applications. The key strength of our approach is that it handles each aspects of a web application using the differencing technique that is best suited to accurately detect XBIs related to that aspect. Our empirical evaluation shows that X-PERT is effective in detecting real-world XBIs, improves on the state of the art, and can provide useful support to developers for the diagnosis and (eventually) elimination of XBIs.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1921}, 
	title = {Enabling collaborative testing across shared software components},
	abstract = {Components of numerous software systems are developed and maintained by multiple stakeholders, and there is significant overlap and synergy in the process of testing systems with shared components. We have designed and implemented infrastructure that enables testers of different components to share their test results and artifacts so that they can collaborate in testing shared components. We also develop an example collaborative testing process that leverages our infrastructure to save effort for regression testing of systems with shared components. Our empirical study of this process shows that collaborative testing of component-based software systems can not only save significant effort by sharing test results and artifacts, but also improve test quality of individual components by utilizing synergistic data shared among component testers.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1922}, 
	title = {TypeDevil: dynamic type inconsistency analysis for JavaScript},
	abstract = {Dynamic languages, such as JavaScript, give programmers the freedom to ignore types, and enable them to write concise code in short time. Despite this freedom, many programs follow implicit type rules, for example, that a function has a particular signature or that a property has a particular type. Violations of such implicit type rules often correlate with problems in the program. This paper presents TypeDevil, a mostly dynamic analysis that warns developers about inconsistent types. The key idea is to assign a set of observed types to each variable, property, and function, to merge types based in their structure, and to warn developers about variables, properties, and functions that have inconsistent types. To deal with the pervasiveness of polymorphic behavior in real-world JavaScript programs, we present a set of techniques to remove spurious warnings and to merge related warnings. Applying TypeDevil to widely used benchmark suites and real-world web applications reveals 15 problematic type inconsistencies, including correctness problems, performance problems, and dangerous coding practices.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1924}, 
	title = {DLint: dynamically checking bad coding practices in JavaScript},
	abstract = {JavaScript has become one of the most popular programming languages, yet it is known for its suboptimal design. To effectively use JavaScript despite its design flaws, developers try to follow informal code quality rules that help avoid correctness, maintainability, performance, and security problems. Lightweight static analyses, implemented in "lint-like" tools, are widely used to find violations of these rules, but are of limited use because of the language's dynamic nature. This paper presents DLint, a dynamic analysis approach to check code quality rules in JavaScript. DLint consists of a generic framework and an extensible set of checkers that each addresses a particular rule. We formally describe and implement 28 checkers that address problems missed by state-of-the-art static approaches. Applying the approach in a comprehensive empirical study on over 200 popular web sites shows that static and dynamic checking complement each other. On average per web site, DLint detects 49 problems that are missed statically, including visible bugs on the web sites of IKEA, Hilton, eBay, and CNBC.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1925}, 
	title = {A grey-box approach for automated GUI-model generation of mobile applications},
	abstract = {As the mobile platform continues to pervade all aspects of human activity, and mobile applications, or mobile apps for short, on this platform tend to be faulty just like other types of software, there is a growing need for automated testing techniques for mobile apps. Modelbased testing is a popular and important testing approach that operates on a model of an app's behavior. However, such a model is often not available or of insufficient quality. To address this issue, we present a novel grey-box approach for automatically extracting a model of a given mobile app. In our approach, static analysis extracts the set of events supported by the Graphical User Interface (GUI) of the app. Then dynamic crawling reverse-engineers a model of the app, by systematically exercising these events on the running app. We also present a tool implementing this approach for the Android platform. Our empirical evaluation of this tool on several Android apps demonstrates that it can efficiently extract compact yet reasonably comprehensive models of high quality for such apps.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1928}, 
	title = {A Survey of Dynamic Analysis and Test Generation for JavaScript},
	abstract = {JavaScript has become one of the most prevalent programming languages. Unfortunately, some of the unique properties that contribute to this popularity also make JavaScript programs prone to errors and difficult for program analyses to reason about. These properties include the highly dynamic nature of the language, a set of unusual language features, a lack of encapsulation mechanisms, and the �no crash� philosophy. This article surveys dynamic program analysis and test generation techniques for JavaScript targeted at improving the correctness, reliability, performance, security, and privacy of JavaScript-based software.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1931}, 
	title = {ALEX: Mixed-Mode Learning of Web Applications at Ease},
	abstract = {Abstract
	In this paper, we present ALEX, a web application that enables non-programmers to fully automatically infer models of web applications via active automata learning. It guides the user in setting up dedicated learning scenarios, and invites her to experiment with the available options in order to infer models at adequate levels of abstraction. In the course of this process, characteristics that go beyond a mere �site map� can be revealed, such as hidden states that are often either specifically designed or indicate errors in the application logic. Characteristic for ALEX is its support for mixed-mode learning: REST and web services can be executed simultaneously in one learning experiment, which is ideal when trying to compare back-end and front-end functionality of a web application. ALEX has been evaluated in a comparative study with 140 undergraduate students, which impressively highlighted its potential to make formal methods like active automata learning more accessible to a non-expert crowd.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1932}, 
	title = {Traceability Types for Mastering Change in Collaborative Software Quality Management},
	abstract = {Abstract
	Software is constantly evolving and to successfully comprehend and manage this evolutionary change is a challenging task which requires traceability support. In this paper we propose a novel approach to traceability as a cornerstone for successful impact analysis and change management, in the context of collaborative software quality management. We first motivate the crucial role of traceability within lifecycle management of the new generation of distributed fragmented software services. Based on the model-based collaborative software quality management framework of Living Models, we then categorize software quality management services and identify novel types of traceability. This is followed by an overview and classification of sample software quality management services from literature, enabled by the interrelation with the identified types of traceability. From this classification we derive the need for further research on traceability in collaborative software quality management.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1933}, 
	title = {Web Service Test Evolution},
	abstract = {Abstract
	In order to remain useful test scripts must evolve parallel to the test objects they are intended to test. In the approach described here the test objects are web services whose test script is derived from the web service interface definition. The test script structure is automatically generated from the WSDL structure with tags and attributes, however, the content, i.e. the test data has to be inserted by hand. From this script service requests are automatically generated and service responses automatically validated. As with other generated software artifacts, once the structure of the interface or the logic of the targeted service is changed, the content of the test script is no longer valid. It has to be altered and/or enhanced to fit the new interface structure and/or the altered service logic. In this paper the author proposes a semi-automated approach to solving this test maintenance problem and explains how it has been implemented in a web service testing tool by employing data reverse engineering techniques. The author also report on his experience with the approach when maintaining a test in the field.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1934}, 
	title = {Worlds Apart: Industrial and Academic Focus Areas in Software Testing},
	abstract = {Abstract:
	To determine how industry and academia approach software testing, researchers compared the titles of presentations from selected conferences in each of the two communities. The results shed light on the root cause of low industry-academia collaboration and led to suggestions on how to improve this situation.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1935}, 
	title = {A Comparative Study of Software Testing Techniques},
	abstract = {Abstract
	Nowadays, software systems have become an essential element in our daily life. To ensure the quality and operation of software, testing activities have become primordial in the software development life cycle (SDLC). Indeed, software bugs can potentially cause dramatic consequences if the product is released to the end user without testing. The software testing role is to verify that the actual result and the expected result are consistent and ensure that the system is delivered without bugs. Many techniques, approaches and tools have been proposed to help check that the system is defect free. In this paper, we highlight two software testing techniques considered among the most used techniques to perform software tests, and then we perform a comparative study of these techniques, the approaches that supports studied techniques, and the tools used for each technique. We have selected the first technique based on the 2014 survey [62] that heighted the motivations for using the Model-based-testing, and by analyzing the survey results we have found that some MBT limits are benefits in Risk based testing, the second technique in our study.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1937}, 
	title = {Integrating a Lightweight Risk Assessment Approach into an Industrial Development Process},
	abstract = {Abstract
	Risk assessment is dependent on its application domain. Risk values consist of probability and impact factors, but there is no fixed, unique guideline for the determination of these two factors. For a precise risk-value calculation, an adequate collection of factors is crucial. In this paper, we show the evolution from the first phase until the application of a risk assessment approach in the area of an international insurance company. In such a risk-aware field we have to systematically determine relevant factors and their severity. The final results are melted into a calculation tool that is embedded in the companies development process and used for decision support system. This paper shows the results and observations for the whole implementation process achieved via action research.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1938}, 
	title = {Risk orientation in software testing processes of small and medium enterprises: an exploratory and comparative study},
	abstract = {Abstract
	Risk orientation in testing is an important means to balance quality, time-to-market, and cost of software. Especially for small and medium enterprises (SME) under high competitive and economic pressure, risk orientation can help to focus testing activities on critical areas of a software product. Although several risk-based approaches to testing are available, the topic has so far not been investigated in the context of SME, where risks are often associated with business critical issues. This article fills the gap and explores the state of risk orientation in the testing processes of SME. Furthermore, it compares the state of risk-based testing in SME to the situation in large enterprises. The article is based on a multiple case study conducted with five SME. A previous study on risk-based testing in large enterprises is used as reference for investigating the differences between risk orientation in SME and large enterprises. The findings of our study show that a strong business focus, the use of informal risk concepts, as well as the application of risk knowledge to reduce testing cost and time are key differences of risk-based testing in SME compared to large enterprises.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1939}, 
	title = {Using Defect Taxonomies for Testing Requirements},
	abstract = {Abstract:
	Systematic defect management based on bug-tracking systems such as Bugzilla is well established and has been successfully used in many software organizations. Defect management weights the failures observed during test execution according to their severity and forms the basis for effective defect taxonomies. In practice, most defect taxonomies are used only for the a posteriori allocation of testing resources to prioritize failures for debugging. Thus, these taxonomies' full potential to control and improve all the steps of testing has remained unexploited. This is especially the case for testing a system's user requirements. System-level defect taxonomies can improve the design of requirements-based tests, the tracing of defects to requirements, the quality assessment of requirements, and the control of the relevant defect management. So, we developed requirements-based testing with defect taxonomies (RTDT). This approach is aligned with the standard test process and uses defect taxonomies to support all phases of testing requirements. To illustrate this approach and its benefits, we use an example project (which we call Project A) from a public health insurance institution.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1940}, 
	title = {Manual test case derivation from UML activity diagrams and state machines: A controlled experiment},
	abstract = {Abstract
	Context
	
	It is a difficult and challenging task to fully automatize model-based testing because this demands complete and unambiguous system models as input. Therefore, in practice, test cases, especially on the system level, are still derived manually from behavioral models like UML activity diagrams or state machines. But this kind of manual test case derivation is error-prone and knowing these errors makes it possible to provide guidelines to reduce them.
	
	Objective
	
	The objective of the study presented in this paper therefore is to examine which errors are possible and actually made when manually deriving test cases from UML activity diagrams or state machines and whether there are differences between these diagram types.
	
	Method
	
	We investigate the errors made when deriving test cases manually in a controlled student experiment. The experiment was performed and internally replicated with overall 84 participants divided into three groups at two institutions.
	
	Results
	
	As a result of our experiment, we provide a taxonomy of errors made and their frequencies. In addition, our experiment provides evidence that activity diagrams have a higher perceived comprehensibility but also a higher error-proneness than state machines with regard to manual test case derivation. This information helps to develop guidelines for manual test case derivation from UML activity diagrams and state machines.
	
	Conclusion
	
	Most errors observed were due to missing test steps, conditions or results, or content was written into the wrong field. As activity diagrams have a higher perceived comprehensibility, but also more error-prone than state machines, both diagram types are useful for manual test case derivation. Their application depends on the context and should be complemented with clear rules on how to derive test cases.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1941}, 
	title = {New Perspectives on Software Quality [Guest editors' introduction]},
	abstract = {Abstract:
	This special issue, owing to its fundamental software quality focus, comprises a collection of diverse articles that address the challenges and directions for software quality research. The Web extra at http://youtu.be/T7V4RSr1KEE is an audio interview in which Davide Falessi speaks with guest editors Annie Kuntzmann-Combelles, Michael Felderer, and Ruth Breu about methods for improving software quality management, testing, and security on intelligent and interconnected devices.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1942}, 
	title = {A taxonomy of risk-based testing},
	abstract = {Abstract
	Software testing has often to be done under severe pressure due to limited resources and a challenging time schedule facing the demand to assure the fulfillment of the software requirements. In addition, testing should unveil those software defects that harm the mission-critical functions of the software. Risk-based testing uses risk (re-)assessments to steer all phases of the test process to optimize testing efforts and limit risks of the software-based system. Due to its importance and high practical relevance, several risk-based testing approaches were proposed in academia and industry. This paper presents a taxonomy of risk-based testing providing a framework to understand, categorize, assess, and compare risk-based testing approaches to support their selection and tailoring for specific purposes. The taxonomy is aligned with the consideration of risks in all phases of the test process and consists of the top-level classes risk drivers, risk assessment, and risk-based test process. The taxonomy of risk-based testing has been developed by analyzing the work presented in available publications on risk-based testing. Afterwards, it has been applied to the work on risk-based testing presented in this special section of the International Journal on Software Tools for Technology Transfer.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1943}, 
	title = {RUP Alignment and Coverage Analysis of CMMI ML2 Process Areas for the Context of Software Projects Execution},
	abstract = {Abstract
	The simultaneous adoption of CMMI and RUP allows the definition of �what to do� (with the support of CMMI) and �how to do� (with the support of RUP) in the context of executing software development projects. In this paper, our main contribution relates to the alignment of CMMI ML2 with RUP, in the context of executing software projects and the analysis of RUP coverage. We present the alignment for CMMI ML2 process areas, incorporating priority mechanisms. The adopted case study allows the analysis of the way RUP supports CMMI ML2 process areas taking into account the proposed alignment and the theoretical coverage analyzed. For particular process areas, RUP can be considered a good approach for CMMI ML2 implementation.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1944}, 
	title = {A multiple case study on risk-based testing in industry},
	abstract = {Abstract
	In many development projects, testing has to be conducted under severe pressure due to limited resources and a challenging time schedule. Risk-based testing, which utilizes identified risks of the system for testing purposes, has a high potential to improve testing as it helps to optimize the allocation of resources and provides decision support for management. But for many organizations, the integration of a risk-based approach into established testing activities is a challenging task, and there are several options to do so. In this article, we analyze how risk is defined, assessed, and applied to support and improve testing activities in projects, products, and processes. We investigate these questions empirically by a multiple case study of currently applied risk-based testing activities in industry. The case study is based on three cases from different backgrounds, i.e., a test project in context of the extension of a large Web-based information system, product testing of a measurement and diagnostic equipment for the electrical power industry, as well as a test process of a system integrator of telecommunication solutions. By analyzing and comparing these different industrial cases, we draw conclusions on the state of risk-based testing and discuss possible improvements.},
	duplicado = {false},
	inserir = {false}
}
@Article{
	id = {1946}, 
	title = {A Demonstration Case on Steps and Rules for the Transition from Process-Level to Software Logical Architectures in Enterprise Models},
	abstract = {Abstract
	At the analysis phase of an enterprise information system development, the alignment between the process level requirements (information systems) with the product level requirements (software system) may not be properly achieved. Modeling the processes for the enterprise�s business is often insufficient for implementation teams, and implementation requirements are often misaligned with business and stakeholder needs. In this paper, we demonstrate, though a real industrial case, how transition steps and rules are used to assure that process- and product-level requirements are aligned, within an approach that supports the creation of the intended requirements. The input for the transition steps is an information system logical architecture, and the output is a product-level (software) use case model.},
	duplicado = {false},
	inserir = {false}
}